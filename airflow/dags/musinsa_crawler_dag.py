from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.trigger_dagrun import TriggerDagRunOperator
from datetime import datetime, timedelta
import os
import time
import requests
import json
import boto3
from botocore.exceptions import ClientError
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import requests
from bs4 import BeautifulSoup


# WebDriver Manager ì„í¬íŠ¸
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
import pendulum

# í•œêµ­ ì‹œê°„ëŒ€ ì„¤ì •
KST = pendulum.timezone('Asia/Seoul')



class S3Uploader:
    """S3ì— ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        # Airflow í™˜ê²½ë³€ìˆ˜ì—ì„œ AWS í‚¤ ê°€ì ¸ì˜¤ê¸°
        self.aws_access_key_id = os.environ.get('AWS_ACCESS_KEY_ID')
        self.aws_secret_access_key = os.environ.get('AWS_SECRET_ACCESS_KEY')
        self.aws_region = os.environ.get('AWS_REGION', 'ap-northeast-2')
        self.bucket_name = os.environ.get('S3_BUCKET_NAME', 'ivle-malle')
        
        # í™˜ê²½ë³€ìˆ˜ ë””ë²„ê¹… (ë¯¼ê°í•œ ì •ë³´ëŠ” ë§ˆìŠ¤í‚¹)
        print(f"ğŸ” í™˜ê²½ë³€ìˆ˜ í™•ì¸:")
        print(f"   AWS_ACCESS_KEY_ID: {'ì„¤ì •ë¨' if self.aws_access_key_id else 'ì„¤ì •ë˜ì§€ ì•ŠìŒ'}")
        print(f"   AWS_SECRET_ACCESS_KEY: {'ì„¤ì •ë¨' if self.aws_secret_access_key else 'ì„¤ì •ë˜ì§€ ì•ŠìŒ'}")
        print(f"   AWS_REGION: {self.aws_region}")
        print(f"   S3_BUCKET_NAME: {self.bucket_name}")
        
        # í™˜ê²½ë³€ìˆ˜ê°€ ì—†ìœ¼ë©´ ì—ëŸ¬ ë°œìƒ
        if not self.aws_access_key_id or not self.aws_secret_access_key:
            print("âŒ AWS í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
            print("   Airflow UI â†’ Admin â†’ Variablesì—ì„œ ë‹¤ìŒ ë³€ìˆ˜ë“¤ì„ ì„¤ì •í•´ì£¼ì„¸ìš”:")
            print("   - AWS_ACCESS_KEY_ID")
            print("   - AWS_SECRET_ACCESS_KEY")
            print("   - AWS_REGION (ì„ íƒì‚¬í•­)")
            print("   - S3_BUCKET_NAME (ì„ íƒì‚¬í•­)")
            self.s3_client = None
            return
        
        try:
            self.s3_client = boto3.client(
                's3',
                aws_access_key_id=self.aws_access_key_id,
                aws_secret_access_key=self.aws_secret_access_key,
                region_name=self.aws_region
            )
            print(f"âœ… S3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ: {self.bucket_name}")
        except Exception as e:
            print(f"âŒ S3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.s3_client = None
    
    def upload_single_html(self, html_file, date_str, category):
        """ë‹¨ì¼ HTML íŒŒì¼ì„ S3ì— ì—…ë¡œë“œ"""
        try:
            # íŒŒì¼ ê²½ë¡œ í™•ì¸
            if not os.path.exists(html_file):
                return f"âŒ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {html_file}"
            
            # S3 í‚¤ ìƒì„±: musinsa_htmls/{date}/{category}/{filename}
            filename = os.path.basename(html_file)
            s3_key = f"musinsa_htmls/{date_str}/{category}/{filename}"
            
            print(f"ğŸ”„ ì—…ë¡œë“œ ì¤‘: {html_file} â†’ s3://{self.bucket_name}/{s3_key}")
            
            self.s3_client.upload_file(
                html_file,
                self.bucket_name,
                s3_key,
                ExtraArgs={'ContentType': 'text/html'}
            )
            return f"âœ… {filename} ì—…ë¡œë“œ ì™„ë£Œ"
        except Exception as e:
            return f"âŒ {filename} ì—…ë¡œë“œ ì‹¤íŒ¨: {e}"
    
    def upload_html_files_parallel(self, html_files, category, date_str):
        """HTML íŒŒì¼ë“¤ì„ ë³‘ë ¬ë¡œ S3ì— ì—…ë¡œë“œí•©ë‹ˆë‹¤."""
        from concurrent.futures import ThreadPoolExecutor, as_completed
        
        if not self.s3_client:
            print("âŒ S3 í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False
        
        print(f"ğŸ”„ {category} HTML íŒŒì¼ {len(html_files)}ê°œ S3 ì—…ë¡œë“œ ì‹œì‘...")
        print(f"ğŸ“ íŒŒì¼ ëª©ë¡: {html_files}")
        
        # ë³‘ë ¬ ì—…ë¡œë“œ (10ê°œì”© ë™ì‹œ ì²˜ë¦¬)
        with ThreadPoolExecutor(max_workers=10) as executor:
            futures = []
            for html_file in html_files:
                future = executor.submit(self.upload_single_html, html_file, date_str, category)
                futures.append(future)
            
            # ëª¨ë“  ì—…ë¡œë“œ ì™„ë£Œ ëŒ€ê¸°
            success_count = 0
            for future in as_completed(futures):
                try:
                    result = future.result()
                    print(result)
                    if "âœ…" in result:
                        success_count += 1
                except Exception as e:
                    print(f"âŒ ì—…ë¡œë“œ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        
        print(f"ğŸ“Š S3 ì—…ë¡œë“œ ì™„ë£Œ: {success_count}/{len(html_files)} ì„±ê³µ")
        return success_count == len(html_files)

# Airflow Variablesë¥¼ í™˜ê²½ë³€ìˆ˜ë¡œ ë³€í™˜
from airflow.models import Variable

try:
    # Airflow Variablesì—ì„œ ê°’ ê°€ì ¸ì˜¤ê¸°
    aws_access_key = Variable.get("AWS_ACCESS_KEY_ID", default_var=None)
    aws_secret_key = Variable.get("AWS_SECRET_ACCESS_KEY", default_var=None)
    aws_region = Variable.get("AWS_REGION", default_var="ap-northeast-2")
    s3_bucket = Variable.get("S3_BUCKET_NAME", default_var="ivle-malle")
    
    if aws_access_key and aws_secret_key:
        # Variablesë¥¼ í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì •
        os.environ['AWS_ACCESS_KEY_ID'] = aws_access_key
        os.environ['AWS_SECRET_ACCESS_KEY'] = aws_secret_key
        os.environ['AWS_REGION'] = aws_region
        os.environ['S3_BUCKET_NAME'] = s3_bucket
        print("ğŸ”§ Airflow Variablesë¥¼ í™˜ê²½ë³€ìˆ˜ë¡œ ë³€í™˜ ì™„ë£Œ")
    else:
        print("âŒ Airflow Variablesê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
        print("   Airflow UI â†’ Admin â†’ Variablesì—ì„œ ë‹¤ìŒ ë³€ìˆ˜ë“¤ì„ ì„¤ì •í•´ì£¼ì„¸ìš”:")
        print("   - AWS_ACCESS_KEY_ID")
        print("   - AWS_SECRET_ACCESS_KEY")
        print("   - AWS_REGION (ì„ íƒì‚¬í•­)")
        print("   - S3_BUCKET_NAME (ì„ íƒì‚¬í•­)")
        
except Exception as e:
    print(f"âŒ Airflow Variables ë¡œë“œ ì‹¤íŒ¨: {e}")
    print("   Airflow UI â†’ Admin â†’ Variablesì—ì„œ ë‹¤ìŒ ë³€ìˆ˜ë“¤ì„ ì„¤ì •í•´ì£¼ì„¸ìš”:")
    print("   - AWS_ACCESS_KEY_ID")
    print("   - AWS_SECRET_ACCESS_KEY")
    print("   - AWS_REGION (ì„ íƒì‚¬í•­)")
    print("   - S3_BUCKET_NAME (ì„ íƒì‚¬í•­)")

# S3 ì—…ë¡œë” ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
s3_uploader = S3Uploader()

def get_driver():
    """
    Selenium WebDriverë¥¼ ì´ˆê¸°í™”í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤.
    WebDriver Managerë¥¼ ì‚¬ìš©í•˜ì—¬ ChromeDriver ê²½ë¡œë¥¼ ìë™ìœ¼ë¡œ ê´€ë¦¬í•©ë‹ˆë‹¤.
    """
    chrome_options = Options()
    chrome_options.add_argument("--headless=new")  # ìƒˆë¡œìš´ í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ ì‚¬ìš©
    chrome_options.add_argument("--no-sandbox")  # ìƒŒë“œë°•ìŠ¤ ë¹„í™œì„±í™” (Docker/CI í™˜ê²½ì—ì„œ í•„ìš”)
    chrome_options.add_argument("--disable-dev-shm-usage") # /dev/shm ì‚¬ìš© ë¹„í™œì„±í™” (Docker/CI í™˜ê²½ì—ì„œ í•„ìš”)
    chrome_options.add_argument("user-agent=Mozilla/5.0") # User-Agent ì„¤ì •

    # WebDriver Managerë¥¼ ì‚¬ìš©í•˜ì—¬ ChromeDriver ê²½ë¡œë¥¼ ìë™ìœ¼ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤.
    service = Service(ChromeDriverManager().install())
    
    return webdriver.Chrome(service=service, options=chrome_options)

def is_valid_product(product_id):
    """
    ì£¼ì–´ì§„ product_idê°€ ìœ íš¨í•œ ë¬´ì‹ ì‚¬ ìƒí’ˆ í˜ì´ì§€ì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    HTTP ìš”ì²­ì„ ì‚¬ìš©í•˜ì—¬ 404 ì‘ë‹µì´ ì•„ë‹Œì§€ í™•ì¸í•©ë‹ˆë‹¤.
    """
    url = f"https://www.musinsa.com/products/{product_id}"
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        response = requests.get(url, headers=headers, timeout=3)
        return response.status_code != 404
    except requests.exceptions.RequestException as e:
        print(f"ğŸš¨ is_valid_product ìš”ì²­ ì˜¤ë¥˜ ({product_id}): {e}")
        return False
    except Exception as e:
        print(f"ğŸš¨ is_valid_product ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ({product_id}): {e}")
        return False

def get_product_category_fast(product_id):
    """
    HTTP ìš”ì²­ìœ¼ë¡œ ë¹ ë¥´ê²Œ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
    Seleniumë³´ë‹¤ í›¨ì”¬ ë¹ ë¥´ê²Œ ì¹´í…Œê³ ë¦¬ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    url = f"https://www.musinsa.com/products/{product_id}"
    headers = {"User-Agent": "Mozilla/5.0"}
    
    try:
        response = requests.get(url, headers=headers, timeout=5)
        if response.status_code != 200:
            return None

        soup = BeautifulSoup(response.text, "html.parser")

        # 1. breadcrumb ìŠ¤íƒ€ì¼ ì¹´í…Œê³ ë¦¬ íƒìƒ‰ (ì—…ë°ì´íŠ¸ëœ ì„ íƒì)
        breadcrumb = soup.select("div.sc-1prswe3-1.sFSiu a")
        if breadcrumb:
            categories = [a.get_text(strip=True) for a in breadcrumb]
            for cat in categories:
                if cat in ["ìƒì˜", "ë°”ì§€", "ì•„ìš°í„°", "ì‹ ë°œ", "ê°€ë°©", "ì•¡ì„¸ì„œë¦¬"]:
                    return cat

        # 2. fallback: ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ê²€ìƒ‰
        html_text = response.text
        category_keywords = ["ìƒì˜", "ë°”ì§€", "ì•„ìš°í„°", "ì‹ ë°œ", "ê°€ë°©", "ì•¡ì„¸ì„œë¦¬"]
        for keyword in category_keywords:
            if keyword in html_text:
                return keyword

        return None

    except Exception as e:
        return None

    except requests.exceptions.RequestException as e:
        print(f"ğŸš¨ get_product_category_fast ìš”ì²­ ì˜¤ë¥˜ ({product_id}): {e}")
        return None
    except Exception as e:
        print(f"ğŸš¨ get_product_category_fast ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ({product_id}): {e}")
        return None

def load_last_product_ids():
    """ë§ˆì§€ë§‰ìœ¼ë¡œ í¬ë¡¤ë§í•œ ìƒí’ˆ IDë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    config_file = "/home/tjddml/musinsa_htmls/last_product_id.json"
    default_id = 5253942
    
    try:
        if os.path.exists(config_file):
            with open(config_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° ê°€ì¥ ìµœê·¼ ë‚ ì§œì˜ ê°’ ë°˜í™˜
            if isinstance(data, dict) and data:
                latest_date = max(data.keys())
                return data[latest_date]
            
            # ê¸°ì¡´ í˜•íƒœ(ìˆ«ì)ì¸ ê²½ìš°
            elif isinstance(data, (int, float)):
                return int(data)
            
        return default_id
        
    except Exception as e:
        print(f"âš ï¸ ì„¤ì • íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {e}, ê¸°ë³¸ê°’ ì‚¬ìš©")
        return default_id

def save_last_product_id(last_id):
    """ë§ˆì§€ë§‰ìœ¼ë¡œ í¬ë¡¤ë§í•œ ìƒí’ˆ IDë¥¼ ë‚ ì§œë³„ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    config_file = "/home/tjddml/musinsa_htmls/last_product_id.json"
    today = datetime.now().strftime('%Y-%m-%d')
    
    try:
        # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
        existing_data = {}
        if os.path.exists(config_file):
            with open(config_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if isinstance(data, dict):
                existing_data = data
        
        # ì˜¤ëŠ˜ ë‚ ì§œë¡œ ì €ì¥
        existing_data[today] = last_id
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(os.path.dirname(config_file), exist_ok=True)
        
        # íŒŒì¼ ì €ì¥
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(existing_data, f, ensure_ascii=False, indent=2)
        
    except Exception as e:
        print(f"âŒ ì„¤ì • íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {e}")

def save_html_by_date(category, product_id, html_content):
    """ìƒí’ˆ HTMLì„ ë‚ ì§œë³„ ë””ë ‰í† ë¦¬ì— ì €ì¥í•©ë‹ˆë‹¤."""
    today = datetime.now().strftime('%Y-%m-%d')
    date_dir = f"/home/tjddml/musinsa_htmls/{today}/{category}"
    
    # ë‚ ì§œë³„ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(date_dir, exist_ok=True)
    
    html_file = f"{date_dir}/{product_id}.html"
    
    try:
        with open(html_file, "w", encoding="utf-8") as f:
            f.write(html_content)
        print(f"ğŸ’¾ HTML ì €ì¥ ì™„ë£Œ: {html_file}")
        return html_file
    except Exception as e:
        print(f"âŒ HTML ì €ì¥ ì˜¤ë¥˜ ({product_id}): {e}")
        return None

def click_all_tabs(driver):
    """
    ëª¨ë“  íƒ­ì„ í´ë¦­í•˜ì—¬ ë™ì  ì½˜í…ì¸ ë¥¼ ë¡œë”©í•©ë‹ˆë‹¤.
    ê¸°ì¡´ ì½”ë“œì˜ íƒ­ í´ë¦­ ë¡œì§ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    try:
        # íƒ­ ë²„íŠ¼ë“¤ ì°¾ê¸°
        tab_buttons = WebDriverWait(driver, 5).until(
            EC.presence_of_all_elements_located((By.CSS_SELECTOR, "[data-mds='AccordionTrigger'] button"))
        )
        
        # ê° íƒ­ í´ë¦­
        for tab in tab_buttons:
            try:
                driver.execute_script("arguments[0].scrollIntoView(true);", tab)
                driver.execute_script("arguments[0].click();", tab)
                time.sleep(1.2)
            except Exception as e:
                print(f"  âš ï¸ íƒ­ í´ë¦­ ì‹¤íŒ¨(ë¬´ì‹œ): {e}")
                continue
                
    except Exception as e:
        print(f"â— íƒ­ ë²„íŠ¼ íƒìƒ‰ ì‹¤íŒ¨: {e}")

def crawl_products():
    """
    ë¬´ì‹ ì‚¬ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ìƒí’ˆ HTMLì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
    í•˜ë‚˜ì˜ ìŠ¤ë ˆë“œì—ì„œ ì¹´í…Œê³ ë¦¬ë¥¼ í™•ì¸í•˜ì—¬ ìƒì˜/í•˜ì˜ ì¤‘ í•˜ë‚˜ë§Œ 1ê°œ ìˆ˜ì§‘í•˜ë©´ ì¢…ë£Œí•©ë‹ˆë‹¤.
    HTMLì€ ë‚ ì§œë³„ ë””ë ‰í† ë¦¬ì— ì €ì¥ë˜ê³  S3ì—ë„ ì—…ë¡œë“œë©ë‹ˆë‹¤.
    """
    # ë§ˆì§€ë§‰ ìƒí’ˆ ID ë¡œë“œ
    last_id = load_last_product_ids()
    print(f"ğŸ“‹ ì‹œì‘ ìƒí’ˆ ID: {last_id}")
    print(f"ğŸ“Š ì €ì¥ ëª¨ë“œ: ë‚ ì§œë³„ HTML ì €ì¥ + S3 ì—…ë¡œë“œ")
    print(f"ğŸ¯ í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ìƒì˜ ë˜ëŠ” í•˜ì˜ 1ê°œë§Œ ìˆ˜ì§‘ í›„ ì¢…ë£Œ")
    
    # í¬ë¡¤ë§ ê²°ê³¼ ì €ì¥
    crawl_results = {
        'top': {'count': 0, 'last_id': last_id, 'files': []},
        'bottom': {'count': 0, 'last_id': last_id, 'files': []}
    }
    
    target_count = 1  # í…ŒìŠ¤íŠ¸ìš©: 1ê°œë§Œ ìˆ˜ì§‘
    
    # ì‹œì‘ ID ì„¤ì •
    product_id = last_id
    
    driver = None
    try:
        driver = get_driver()
        print(f"ğŸš€ í…ŒìŠ¤íŠ¸ìš© ë‹¨ì¼ ìŠ¤ë ˆë“œ í¬ë¡¤ë§ ì‹œì‘... (ì‹œì‘ ID: {product_id})")
        
        # ìƒì˜ë‚˜ í•˜ì˜ ì¤‘ í•˜ë‚˜ë¼ë„ ìˆ˜ì§‘ë˜ë©´ ì¢…ë£Œ
        while crawl_results['top']['count'] < target_count and crawl_results['bottom']['count'] < target_count:
            # ìœ íš¨í•œ ìƒí’ˆ IDì¸ì§€ í™•ì¸
            if not is_valid_product(product_id):
                print(f"âŒ {product_id} - ìœ íš¨í•˜ì§€ ì•Šì€ ìƒí’ˆ (404)")
                product_id += 1
                continue
            
            # Seleniumìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ í™•ì¸
            try:
                url = f"https://www.musinsa.com/products/{product_id}"
                driver.get(url)
                time.sleep(1.5)
                
                # ì—…ë°ì´íŠ¸ëœ CSS ì„ íƒì ì‚¬ìš©
                category_elem = driver.find_element(
                    By.CSS_SELECTOR,
                    "#root > div.sc-3weaze-0.kfGyOQ > div.sc-1puoja0-0.dSJyEH > div > div.sc-1prswe3-1.sFSiu.text-body_13px_reg.text-gray-600.font-pretendard > span:nth-child(1) > a"
                )
                category = category_elem.text.strip()
                
            except Exception as e:
                print(f"ğŸš¨ Selenium ì¹´í…Œê³ ë¦¬ í™•ì¸ ì˜¤ë¥˜ ({product_id}): {e}")
                product_id += 1
                continue
            
            # ì¹´í…Œê³ ë¦¬ë³„ ì²˜ë¦¬
            if category == "ìƒì˜":
                try:
                    print(f"ğŸ” {product_id} - {category} ìƒí’ˆ ê°ì§€")
                    
                    # âœ… íƒ­ í´ë¦­ (ëª¨ë“  ì •ë³´ ë¡œë”©)
                    click_all_tabs(driver)
                    
                    # HTML ì €ì¥ (ë‚ ì§œë³„ ë””ë ‰í† ë¦¬)
                    html_file = save_html_by_date('top', product_id, driver.page_source)
                    if html_file:
                        crawl_results['top']['files'].append(html_file)
                    
                    crawl_results['top']['count'] += 1
                    crawl_results['top']['last_id'] = product_id
                    
                    print(f"âœ… top ì €ì¥ ì™„ë£Œ ({crawl_results['top']['count']}/{target_count}): {product_id}")
                    
                    # ëª©í‘œ ë‹¬ì„± ì‹œ ì•Œë¦¼
                    if crawl_results['top']['count'] >= target_count:
                        print(f"ğŸ¯ top ëª©í‘œ ë‹¬ì„±! ({target_count}ê°œ)")
                        print(f"ğŸš€ í…ŒìŠ¤íŠ¸ ì™„ë£Œ! ìƒì˜ 1ê°œ ìˆ˜ì§‘ë¨ - í¬ë¡¤ë§ ì¢…ë£Œ")
                        break
                    else:
                        print(f"ğŸ“ˆ top ì¶”ê°€ ìˆ˜ì§‘ ì¤‘... ({crawl_results['top']['count']}/{target_count})")
                        
                except Exception as e:
                    print(f"ğŸš¨ Selenium í¬ë¡¤ë§ ì˜¤ë¥˜ ({product_id}): {e}")
                    
            elif category == "ë°”ì§€":
                try:
                    print(f"ğŸ” {product_id} - {category} ìƒí’ˆ ê°ì§€")
                    
                    # âœ… íƒ­ í´ë¦­ (ëª¨ë“  ì •ë³´ ë¡œë”©)
                    click_all_tabs(driver)
                    
                    # HTML ì €ì¥ (ë‚ ì§œë³„ ë””ë ‰í† ë¦¬)
                    html_file = save_html_by_date('bottom', product_id, driver.page_source)
                    if html_file:
                        crawl_results['bottom']['files'].append(html_file)
                    
                    crawl_results['bottom']['count'] += 1
                    crawl_results['bottom']['last_id'] = product_id
                    
                    print(f"âœ… bottom ì €ì¥ ì™„ë£Œ ({crawl_results['bottom']['count']}/{target_count}): {product_id}")
                    
                    # ëª©í‘œ ë‹¬ì„± ì‹œ ì•Œë¦¼
                    if crawl_results['bottom']['count'] >= target_count:
                        print(f"ğŸ¯ bottom ëª©í‘œ ë‹¬ì„±! ({target_count}ê°œ)")
                        print(f"ğŸš€ í…ŒìŠ¤íŠ¸ ì™„ë£Œ! í•˜ì˜ 1ê°œ ìˆ˜ì§‘ë¨ - í¬ë¡¤ë§ ì¢…ë£Œ")
                        break
                    else:
                        print(f"ğŸ“ˆ bottom ì¶”ê°€ ìˆ˜ì§‘ ì¤‘... ({crawl_results['bottom']['count']}/{target_count})")
                        
                except Exception as e:
                    print(f"ğŸš¨ Selenium í¬ë¡¤ë§ ì˜¤ë¥˜ ({product_id}): {e}")
                    
            else:
                # ì¹´í…Œê³ ë¦¬ê°€ ë‹¤ë¥¸ ê²½ìš°
                print(f"â„¹ï¸ ì¹´í…Œê³ ë¦¬ ë¶ˆì¼ì¹˜ ({product_id}): {category}")
            
            product_id += 1
            
            # ë¬´í•œ ë£¨í”„ ë°©ì§€ (ìµœëŒ€ 1000ê°œ ì‹œë„)
            if product_id > last_id + 1000:
                print(f"âš ï¸ ìµœëŒ€ ì‹œë„ íšŸìˆ˜ ì´ˆê³¼")
                break
            
            # í…ŒìŠ¤íŠ¸ ëª¨ë“œì—ì„œëŠ” í•˜ë‚˜ë¼ë„ ìˆ˜ì§‘ë˜ë©´ ì¢…ë£Œë˜ë¯€ë¡œ ì´ ì¡°ê±´ì€ ì œê±°
            # (ìœ„ì—ì„œ ê°ê° breakë¡œ ì²˜ë¦¬ë¨)
    
    finally:
        if driver:
            driver.quit()
    
    # S3 ì—…ë¡œë“œ
    today = datetime.now().strftime('%Y-%m-%d')
    print(f"\nğŸ”„ S3 ì—…ë¡œë“œ ì‹œì‘...")
    
    # ìƒì˜ HTML S3 ì—…ë¡œë“œ
    if crawl_results['top']['files']:
        upload_success = s3_uploader.upload_html_files_parallel(crawl_results['top']['files'], 'top', today)
        if upload_success:
            print(f"âœ… ìƒì˜ HTML S3 ì—…ë¡œë“œ ì™„ë£Œ: {len(crawl_results['top']['files'])}ê°œ")
        else:
            print(f"âŒ ìƒì˜ HTML S3 ì—…ë¡œë“œ ì‹¤íŒ¨")
    
    # í•˜ì˜ HTML S3 ì—…ë¡œë“œ
    if crawl_results['bottom']['files']:
        upload_success = s3_uploader.upload_html_files_parallel(crawl_results['bottom']['files'], 'bottom', today)
        if upload_success:
            print(f"âœ… í•˜ì˜ HTML S3 ì—…ë¡œë“œ ì™„ë£Œ: {len(crawl_results['bottom']['files'])}ê°œ")
        else:
            print(f"âŒ í•˜ì˜ HTML S3 ì—…ë¡œë“œ ì‹¤íŒ¨")
    
    # ë§ˆì§€ë§‰ ìƒí’ˆ ID ì €ì¥ (ê°€ì¥ í° ID + 1ë¡œ ì €ì¥í•˜ì—¬ ì¤‘ë³µ ë°©ì§€)
    final_last_id = max(crawl_results['top']['last_id'], crawl_results['bottom']['last_id']) + 1
    save_last_product_id(final_last_id)
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ í¬ë¡¤ë§ ì™„ë£Œ ê²°ê³¼:")
    print(f"   ìƒì˜: {crawl_results['top']['count']}ê°œ (ë§ˆì§€ë§‰ ID: {crawl_results['top']['last_id']})")
    print(f"   í•˜ì˜: {crawl_results['bottom']['count']}ê°œ (ë§ˆì§€ë§‰ ID: {crawl_results['bottom']['last_id']})")
    print(f"   ë‹¤ìŒ ì‹œì‘ ID: {final_last_id}")
    
    # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½
    total_collected = crawl_results['top']['count'] + crawl_results['bottom']['count']
    if total_collected > 0:
        print(f"ğŸ‰ í…ŒìŠ¤íŠ¸ ì„±ê³µ! ì´ {total_collected}ê°œ ìƒí’ˆ ìˆ˜ì§‘ ì™„ë£Œ")
    else:
        print(f"âš ï¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: ìˆ˜ì§‘ëœ ìƒí’ˆì´ ì—†ìŠµë‹ˆë‹¤")
    
    return crawl_results

# Airflow DAGì˜ ê¸°ë³¸ ì¸ì ì„¤ì •
default_args = {
    'owner': 'airflow',
    'start_date': datetime(2025, 8, 5, 15, 30),
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    'email_on_failure': False,
    'email_on_retry': False,
}

# Airflow DAG ì •ì˜
with DAG(
    dag_id='musinsa_crawler_html_only',
    default_args=default_args,
    description='ë¬´ì‹ ì‚¬ ìƒí’ˆ HTML í¬ë¡¤ë§ - í…ŒìŠ¤íŠ¸ìš©',
    schedule_interval='0 1 * * *', 
    catchup=False,
    is_paused_upon_creation=False,  # â† ì¶”ê°€ (ìƒì„± ì¦‰ì‹œ í™œì„±í™”)
    tags=['musinsa', 'crawler', 'html', 'test', 'kst'],
) as dag:
    
    # HTML í¬ë¡¤ë§ íƒœìŠ¤í¬
    task_crawl_html = PythonOperator(
        task_id='crawl_html_only',
        python_callable=crawl_products,
        doc_md="""
        ## ë¬´ì‹ ì‚¬ ìƒí’ˆ HTML í¬ë¡¤ë§ (í…ŒìŠ¤íŠ¸ìš©)
        
        ### ê¸°ëŠ¥
        - ë‹¨ì¼ ìŠ¤ë ˆë“œì—ì„œ ì¹´í…Œê³ ë¦¬ë¥¼ í™•ì¸í•˜ì—¬ ìƒì˜/í•˜ì˜ ì¤‘ í•˜ë‚˜ë§Œ 1ê°œ ìˆ˜ì§‘ í›„ ì¦‰ì‹œ ì¢…ë£Œ
        - ë§ˆì§€ë§‰ ìƒí’ˆ ID ìë™ ì €ì¥ ë° ë³µì›
        - ë‚ ì§œë³„ ë””ë ‰í† ë¦¬ì— HTML íŒŒì¼ ì €ì¥
        - ëª¨ë“  íƒ­ì„ í´ë¦­í•˜ì—¬ ë™ì  ì½˜í…ì¸  ì™„ì „ ë¡œë”©
        - S3ì— HTML íŒŒì¼ ë³‘ë ¬ ì—…ë¡œë“œ
        
        ### í…ŒìŠ¤íŠ¸ ëª¨ë“œ íŠ¹ì§•
        - ìƒì˜ ë˜ëŠ” í•˜ì˜ ì¤‘ ë¨¼ì € ë°œê²¬ë˜ëŠ” ì¹´í…Œê³ ë¦¬ 1ê°œë§Œ ìˆ˜ì§‘
        - ìˆ˜ì§‘ ì™„ë£Œ ì¦‰ì‹œ í¬ë¡¤ë§ ì¢…ë£Œ
        - ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ë° ê²€ì¦ ê°€ëŠ¥
        
        ### ì €ì¥ êµ¬ì¡°
        ```
        ë¡œì»¬:
        /home/tjddml/musinsa_htmls/
        â”œâ”€â”€ 2025-01-27/
        â”‚   â”œâ”€â”€ top/ (ìƒì˜ 1ê°œ)
        â”‚   â””â”€â”€ bottom/ (í•˜ì˜ 1ê°œ)
        â””â”€â”€ last_product_id.json
        
        S3:
        s3://ivle-malle/musinsa_htmls/
        â”œâ”€â”€ 2025-01-27/
        â”‚   â”œâ”€â”€ top/
        â”‚   â””â”€â”€ bottom/
        ```
        
        ### íŠ¹ì§•
        - ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš© í¬ë¡¤ë§
        - HTMLë§Œ ì €ì¥í•˜ì—¬ ë³„ë„ CSV ë³€í™˜ ë„êµ¬ì™€ ì—°ë™ ê°€ëŠ¥
        - ë‚ ì§œë³„ ë””ë ‰í† ë¦¬ êµ¬ì¡°ë¡œ ë°ì´í„° ê´€ë¦¬ ìš©ì´
        - ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¦¬ ì €ì¥
        - ë‹¨ì¼ ìŠ¤ë ˆë“œë¡œ íš¨ìœ¨ì ì¸ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
        - ëª¨ë“  íƒ­ í´ë¦­ìœ¼ë¡œ ì™„ì „í•œ HTML ìˆ˜ì§‘
        - S3ì— ë³‘ë ¬ ì—…ë¡œë“œë¡œ ë¹ ë¥¸ ì €ì¥
        """
    )

    # CSV ë³€í™˜ DAG ìë™ ì‹¤í–‰ íƒœìŠ¤í¬
    trigger_csv_converter = TriggerDagRunOperator(
        task_id='trigger_csv_converter',
        trigger_dag_id='musinsa_csv_converter_dag',
        wait_for_completion=False,  # ë¹„ë™ê¸° ì‹¤í–‰
        poke_interval=10,  # 10ì´ˆë§ˆë‹¤ ìƒíƒœ í™•ì¸
        allowed_states=['success'],  # ì„±ê³µ ìƒíƒœì—ì„œë§Œ íŠ¸ë¦¬ê±°
        failed_states=['failed'],  # ì‹¤íŒ¨ ìƒíƒœì—ì„œ íŠ¸ë¦¬ê±° ì•ˆí•¨
        doc_md="""
        ## CSV ë³€í™˜ DAG ìë™ ì‹¤í–‰
        
        ### ê¸°ëŠ¥
        - HTML í¬ë¡¤ë§ ì™„ë£Œ í›„ ìë™ìœ¼ë¡œ CSV ë³€í™˜ DAG ì‹¤í–‰
        - ë¡œì»¬ HTML íŒŒì¼ ì½ê¸° â†’ CSV ë³€í™˜ â†’ S3 ì—…ë¡œë“œ
        
        ### ì‹¤í–‰ íë¦„
        1. HTML í¬ë¡¤ë§ ì™„ë£Œ
        2. CSV ë³€í™˜ DAG ìë™ ì‹¤í–‰
        3. ìµœì¢… CSV íŒŒì¼ ìƒì„± ë° S3 ì—…ë¡œë“œ
        """
    )

    # íƒœìŠ¤í¬ ì˜ì¡´ì„± ì„¤ì •
    task_crawl_html >> trigger_csv_converter
