from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.trigger_dagrun import TriggerDagRunOperator
from airflow.models import Variable
from datetime import datetime, timedelta
import pandas as pd
import os
import ast
import boto3
from botocore.exceptions import ClientError
import logging
from dotenv import load_dotenv

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# DAG ê¸°ë³¸ ì„¤ì •
default_args = {
    'owner': 'airflow',
    'start_date': datetime(2025, 1, 1),
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    'email_on_failure': False,
    'email_on_retry': False,
}

# S3CSVLoader í´ë˜ìŠ¤ (í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©)
class S3CSVLoader:
    """S3ì—ì„œ CSV íŒŒì¼ì„ ë¡œë“œí•˜ëŠ” í´ë˜ìŠ¤ (í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©)"""
    
    def __init__(self):
        self.s3_client = None
        self.aws_access_key_id = None
        self.aws_secret_access_key = None
        self.aws_region = "ap-northeast-2"
        self.bucket_name = "ivle-malle"
        self.initialized = False
        
    def initialize(self):
        """S3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©)"""
        if self.initialized:
            return True
            
        try:
            logger.info(f"ğŸ” í™˜ê²½ë³€ìˆ˜ì—ì„œ ìê²© ì¦ëª… ë¡œë“œ ì¤‘...")
            
            # .env íŒŒì¼ ë¡œë“œ ì‹œë„
            try:
                load_dotenv()
                logger.info(f"âœ… .env íŒŒì¼ ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"âš ï¸ .env íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ (ì‹œìŠ¤í…œ í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©): {e}")
            
            # í™˜ê²½ë³€ìˆ˜ì—ì„œ ìê²© ì¦ëª… ë¡œë“œ
            self.aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID')
            self.aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY')
            self.aws_region = os.getenv('AWS_REGION', 'ap-northeast-2')
            self.bucket_name = os.getenv('S3_BUCKET_NAME', 'ivle-malle')
            
            logger.info(f"âœ… í™˜ê²½ë³€ìˆ˜ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ ì‹¤íŒ¨: {e}")
            logger.error(f"   .env íŒŒì¼ ë˜ëŠ” ì‹œìŠ¤í…œ í™˜ê²½ë³€ìˆ˜ì—ì„œ ë‹¤ìŒ ë³€ìˆ˜ë“¤ì„ ì„¤ì •í•´ì£¼ì„¸ìš”:")
            logger.error(f"   - AWS_ACCESS_KEY_ID")
            logger.error(f"   - AWS_SECRET_ACCESS_KEY")
            logger.error(f"   - AWS_REGION (ì„ íƒì‚¬í•­)")
            logger.error(f"   - S3_BUCKET_NAME (ì„ íƒì‚¬í•­)")
            return False
        
        # ìê²© ì¦ëª… í™•ì¸
        logger.info(f"ğŸ” AWS ì„¤ì • í™•ì¸:")
        logger.info(f"   AWS_ACCESS_KEY_ID: {'ì„¤ì •ë¨' if self.aws_access_key_id else 'ì„¤ì •ë˜ì§€ ì•ŠìŒ'}")
        logger.info(f"   AWS_SECRET_ACCESS_KEY: {'ì„¤ì •ë¨' if self.aws_secret_access_key else 'ì„¤ì •ë˜ì§€ ì•ŠìŒ'}")
        logger.info(f"   AWS_REGION: {self.aws_region}")
        logger.info(f"   S3_BUCKET_NAME: {self.bucket_name}")
        
        # ìê²© ì¦ëª…ì´ ì—†ìœ¼ë©´ ì—ëŸ¬ ë°œìƒ
        if not self.aws_access_key_id or not self.aws_secret_access_key:
            logger.error("âŒ AWS ìê²© ì¦ëª…ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
            logger.error("   .env íŒŒì¼ ë˜ëŠ” ì‹œìŠ¤í…œ í™˜ê²½ë³€ìˆ˜ì—ì„œ ë‹¤ìŒ ë³€ìˆ˜ë“¤ì„ ì„¤ì •í•´ì£¼ì„¸ìš”:")
            logger.error("   - AWS_ACCESS_KEY_ID")
            logger.error("   - AWS_SECRET_ACCESS_KEY")
            logger.error("   - AWS_REGION (ì„ íƒì‚¬í•­)")
            logger.error("   - S3_BUCKET_NAME (ì„ íƒì‚¬í•­)")
            return False
        
        # ìê²© ì¦ëª… í˜•ì‹ ê²€ì¦
        if len(self.aws_access_key_id) != 20:
            logger.warning(f"âš ï¸ Access Key ê¸¸ì´ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤: {len(self.aws_access_key_id)} (ì˜ˆìƒ: 20)")
            logger.warning(f"   Access Key: {self.aws_access_key_id[:5]}...{self.aws_access_key_id[-5:]}")
            return False
        if len(self.aws_secret_access_key) != 40:
            logger.warning(f"âš ï¸ Secret Key ê¸¸ì´ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤: {len(self.aws_secret_access_key)} (ì˜ˆìƒ: 40)")
            logger.warning(f"   Secret Key: {self.aws_secret_access_key[:5]}...{self.aws_secret_access_key[-5:]}")
            return False
        
        # ìê²© ì¦ëª… ë””ë²„ê¹… (ì²˜ìŒ/ë§ˆì§€ë§‰ 5ìë§Œ í‘œì‹œ)
        logger.info(f"ğŸ” ìê²© ì¦ëª… ë””ë²„ê¹…:")
        logger.info(f"   Access Key: {self.aws_access_key_id[:5]}...{self.aws_access_key_id[-5:]}")
        logger.info(f"   Secret Key: {self.aws_secret_access_key[:5]}...{self.aws_secret_access_key[-5:]}")
        logger.info(f"   Access Key ê¸¸ì´: {len(self.aws_access_key_id)}")
        logger.info(f"   Secret Key ê¸¸ì´: {len(self.aws_secret_access_key)}")
        
        # ìê²© ì¦ëª… ì •ë¦¬ (ê³µë°± ì œê±°)
        self.aws_access_key_id = self.aws_access_key_id.strip()
        self.aws_secret_access_key = self.aws_secret_access_key.strip()
        logger.info(f"ğŸ”§ ìê²© ì¦ëª… ì •ë¦¬ í›„:")
        logger.info(f"   Access Key: {self.aws_access_key_id[:5]}...{self.aws_access_key_id[-5:]}")
        logger.info(f"   Secret Key: {self.aws_secret_access_key[:5]}...{self.aws_secret_access_key[-5:]}")
        
        try:
            # S3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
            logger.info(f"ğŸ”§ S3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘...")
            self.s3_client = boto3.client(
                's3',
                aws_access_key_id=self.aws_access_key_id,
                aws_secret_access_key=self.aws_secret_access_key,
                region_name=self.aws_region
            )
            logger.info(f"âœ… S3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ: {self.bucket_name}")
            
            # ì—°ê²° í…ŒìŠ¤íŠ¸ (ê¶Œí•œ ë¬¸ì œ ì‹œ ê±´ë„ˆë›°ê¸°)
            try:
                logger.info(f"ğŸ” S3 ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘...")
                self.s3_client.head_bucket(Bucket=self.bucket_name)
                logger.info(f"âœ… S3 ì—°ê²° í…ŒìŠ¤íŠ¸ ì„±ê³µ")
            except ClientError as e:
                if e.response['Error']['Code'] == '403':
                    logger.warning(f"âš ï¸ S3 ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ (ê¶Œí•œ ë¶€ì¡±), ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤: {e}")
                else:
                    raise e
            
            self.initialized = True
            return True
            
        except ClientError as e:
            error_code = e.response['Error']['Code']
            error_message = e.response['Error']['Message']
            logger.error(f"âŒ S3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {error_code} - {error_message}")
            
            if error_code == 'AccessDenied':
                logger.error(f"ğŸ’¡ í•´ê²° ë°©ë²•:")
                logger.error(f"   1. AWS IAMì—ì„œ í•´ë‹¹ ì‚¬ìš©ìì—ê²Œ S3 ê¶Œí•œì„ ë¶€ì—¬í•˜ì„¸ìš”")
                logger.error(f"   2. í•„ìš”í•œ ê¶Œí•œ: s3:GetObject, s3:ListBucket, s3:HeadBucket")
                logger.error(f"   3. ë²„í‚· ì´ë¦„ì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•˜ì„¸ìš”: {self.bucket_name}")
            elif error_code == 'NoSuchBucket':
                logger.error(f"ğŸ’¡ ë²„í‚·ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.bucket_name}")
            elif error_code == 'SignatureDoesNotMatch':
                logger.error(f"ğŸ’¡ ìê²© ì¦ëª…ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. .env íŒŒì¼ ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”")
            
            self.s3_client = None
            return False
            
        except Exception as e:
            logger.error(f"âŒ S3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.s3_client = None
            return False
    
    def load_csv_from_s3(self, s3_key, encoding='utf-8'):
        """
        S3ì—ì„œ CSV íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ pandas DataFrameìœ¼ë¡œ ë°˜í™˜
        
        Args:
            s3_key (str): S3 ê°ì²´ í‚¤ (ì˜ˆ: 'musinsa_csvs/2025-08-06/product_info_combined_1357.csv')
            encoding (str): íŒŒì¼ ì¸ì½”ë”© (ê¸°ë³¸ê°’: 'utf-8')
        
        Returns:
            pandas.DataFrame: ë¡œë“œëœ CSV ë°ì´í„°
        """
        if not self.s3_client:
            logger.error("âŒ S3 í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        
        try:
            logger.info(f"ğŸ“¥ S3ì—ì„œ CSV íŒŒì¼ ë¡œë“œ ì¤‘: s3://{self.bucket_name}/{s3_key}")
            
            # S3ì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
            response = self.s3_client.get_object(Bucket=self.bucket_name, Key=s3_key)
            
            # CSV ë°ì´í„° ì½ê¸° (ì—¬ëŸ¬ ì¸ì½”ë”© ì‹œë„)
            try:
                df = pd.read_csv(response['Body'], encoding=encoding)
            except UnicodeDecodeError:
                logger.info(f"âš ï¸ {encoding} ì¸ì½”ë”© ì‹¤íŒ¨, cp949 ì‹œë„...")
                response = self.s3_client.get_object(Bucket=self.bucket_name, Key=s3_key)
                df = pd.read_csv(response['Body'], encoding='cp949')
            except Exception as e:
                logger.info(f"âš ï¸ cp949 ì¸ì½”ë”© ì‹¤íŒ¨, euc-kr ì‹œë„...")
                response = self.s3_client.get_object(Bucket=self.bucket_name, Key=s3_key)
                df = pd.read_csv(response['Body'], encoding='euc-kr')
            
            logger.info(f"âœ… CSV íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {len(df)} í–‰, {len(df.columns)} ì—´")
            logger.info(f"ğŸ“Š ì»¬ëŸ¼: {list(df.columns)}")
            
            return df
            
        except ClientError as e:
            error_code = e.response['Error']['Code']
            if error_code == 'NoSuchKey':
                logger.error(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: s3://{self.bucket_name}/{s3_key}")
            elif error_code == 'NoSuchBucket':
                logger.error(f"âŒ ë²„í‚·ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.bucket_name}")
            else:
                logger.error(f"âŒ S3 ì˜¤ë¥˜: {e}")
            return None
        except Exception as e:
            logger.error(f"âŒ CSV íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def get_latest_date_csv_files(self, base_prefix='musinsa_csvs/'):
        """
        ê°€ì¥ ìµœê·¼ ë‚ ì§œì˜ CSV íŒŒì¼ë“¤ì„ ì°¾ì•„ì„œ ë°˜í™˜
        
        Args:
            base_prefix (str): ê¸°ë³¸ ì ‘ë‘ì‚¬ (ê¸°ë³¸ê°’: 'musinsa_csvs/')
        
        Returns:
            tuple: (ìµœê·¼ ë‚ ì§œ, CSV íŒŒì¼ í‚¤ ë¦¬ìŠ¤íŠ¸)
        """
        if not self.s3_client:
            logger.error("âŒ S3 í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None, []
        
        try:
            logger.info(f"ğŸ” ê°€ì¥ ìµœê·¼ ë‚ ì§œì˜ CSV íŒŒì¼ ê²€ìƒ‰ ì¤‘: s3://{self.bucket_name}/{base_prefix}")
            
            # ëª¨ë“  ê°ì²´ ì¡°íšŒ
            response = self.s3_client.list_objects_v2(
                Bucket=self.bucket_name,
                Prefix=base_prefix
            )
            
            if 'Contents' not in response:
                logger.warning(f"âš ï¸ {base_prefix} ì ‘ë‘ì‚¬ë¡œ ì‹œì‘í•˜ëŠ” íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                return None, []
            
            # ë‚ ì§œë³„ë¡œ íŒŒì¼ ê·¸ë£¹í™”
            date_files = {}
            for obj in response['Contents']:
                key = obj['Key']
                if key.endswith('.csv') and 'product_info_' in key:
                    # ë‚ ì§œ ì¶”ì¶œ (musinsa_csvs/YYYY-MM-DD/filename.csv)
                    parts = key.split('/')
                    if len(parts) >= 3:
                        date_str = parts[1]  # YYYY-MM-DD
                        if date_str not in date_files:
                            date_files[date_str] = []
                        date_files[date_str].append(key)
            
            if not date_files:
                logger.warning(f"âš ï¸ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None, []
            
            # ê°€ì¥ ìµœê·¼ ë‚ ì§œ ì°¾ê¸°
            latest_date = max(date_files.keys())
            latest_files = date_files[latest_date]
            
            logger.info(f"âœ… ê°€ì¥ ìµœê·¼ ë‚ ì§œ: {latest_date}")
            logger.info(f"ğŸ“„ í•´ë‹¹ ë‚ ì§œì˜ CSV íŒŒì¼ {len(latest_files)}ê°œ:")
            for file_key in latest_files:
                logger.info(f"   ğŸ“„ {file_key}")
            
            return latest_date, latest_files
            
        except Exception as e:
            logger.error(f"âŒ ìµœê·¼ ë‚ ì§œ CSV íŒŒì¼ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return None, []
    
    def load_latest_csv_files(self, base_prefix='musinsa_csvs/', max_files=None):
        """
        ê°€ì¥ ìµœê·¼ ë‚ ì§œì˜ CSV íŒŒì¼ë“¤ì„ ë¡œë“œ
        
        Args:
            base_prefix (str): ê¸°ë³¸ ì ‘ë‘ì‚¬ (ê¸°ë³¸ê°’: 'musinsa_csvs/')
            max_files (int): ìµœëŒ€ ë¡œë“œí•  íŒŒì¼ ìˆ˜ (Noneì´ë©´ ëª¨ë‘ ë¡œë“œ)
        
        Returns:
            tuple: (ìµœê·¼ ë‚ ì§œ, pandas.DataFrame ë¦¬ìŠ¤íŠ¸)
        """
        latest_date, csv_files = self.get_latest_date_csv_files(base_prefix)
        
        if not csv_files:
            logger.error("âŒ ë¡œë“œí•  CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return None, []
        
        # íŒŒì¼ ìˆ˜ ì œí•œ
        if max_files and len(csv_files) > max_files:
            csv_files = csv_files[:max_files]
            logger.info(f"ğŸ“ ìµœëŒ€ {max_files}ê°œ íŒŒì¼ë§Œ ë¡œë“œí•©ë‹ˆë‹¤.")
        
        # CSV íŒŒì¼ë“¤ ë¡œë“œ
        dataframes = []
        for s3_key in csv_files:
            df = self.load_csv_from_s3(s3_key)
            if df is not None:
                dataframes.append(df)
                logger.info(f"âœ… {s3_key} ë¡œë“œ ì™„ë£Œ")
            else:
                logger.warning(f"âš ï¸ {s3_key} ë¡œë“œ ì‹¤íŒ¨")
        
        return latest_date, dataframes

# ì „ì—­ S3CSVLoader ì¸ìŠ¤í„´ìŠ¤
s3_loader = S3CSVLoader()

def load_brand_mapping():
    """ë¸Œëœë“œ ë§¤í•‘ ë°ì´í„° ë¡œë“œ"""
    try:
        # DAG íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ì—ì„œ brands.csv ì°¾ê¸°
        dag_dir = os.path.dirname(os.path.abspath(__file__))
        brands_file = os.path.join(dag_dir, 'brands.csv')
        
        if not os.path.exists(brands_file):
            print(f"âš ï¸ brands.csv íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {brands_file}")
            return {}
            
        brand_df = pd.read_csv(brands_file, encoding='utf-8-sig')
        
        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
        required_columns = ['í•œê¸€ë¸Œëœë“œëª…', 'ì˜ì–´ë¸Œëœë“œëª…']
        missing_columns = [col for col in required_columns if col not in brand_df.columns]
        if missing_columns:
            print(f"âš ï¸ brands.csvì— í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing_columns}")
            return {}
            
        brand_mapping = dict(zip(brand_df['í•œê¸€ë¸Œëœë“œëª…'], brand_df['ì˜ì–´ë¸Œëœë“œëª…']))
        print(f"âœ… ë¸Œëœë“œ ë§¤í•‘ ë¡œë“œ ì™„ë£Œ: {len(brand_mapping)}ê°œ ë¸Œëœë“œ")
        return brand_mapping
    except Exception as e:
        print(f"âŒ ë¸Œëœë“œ ë§¤í•‘ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return {}

def normalize_columns(df):
    """ì»¬ëŸ¼ëª… ì •ê·œí™”"""
    column_mapping = {
        'ì œí’ˆë²ˆí˜¸': 'ìƒí’ˆì½”ë“œ',
        'ì œí’ˆì´ë¦„': 'ìƒí’ˆëª…',
        'ë¸Œëœë“œëª…': 'í•œê¸€ë¸Œëœë“œëª…',
        'ì œí’ˆëŒ€ë¶„ë¥˜': 'ëŒ€ë¶„ë¥˜',
        'ì œí’ˆì†Œë¶„ë¥˜': 'ì†Œë¶„ë¥˜',
        'ì‚¬ì§„': 'ì´ë¯¸ì§€URL',
        'ì´ë¯¸ì§€ url': 'ì´ë¯¸ì§€URL',
        'ì œí’ˆì†Œì¬': 'ì†Œì¬',
        'ì¢‹ì•„ìš”': 'ì¢‹ì•„ìš”ìˆ˜',
        'ì¢‹ì•„ìš” ìˆ˜': 'ì¢‹ì•„ìš”ìˆ˜',
        'ìƒí’ˆë§í¬': 'ìƒí’ˆë§í¬'
    }
    
    df = df.rename(columns=column_mapping)
    
    if 'ë¸Œëœë“œ' in df.columns and 'í•œê¸€ë¸Œëœë“œëª…' not in df.columns:
        df['í•œê¸€ë¸Œëœë“œëª…'] = df['ë¸Œëœë“œ']
    
    return df

def clean_product_name(name: str) -> str:
    """ìƒí’ˆëª… ì „ì²˜ë¦¬"""
    if pd.isna(name):
        return ""
    
    import re
    
    name = str(name)  # íƒ€ì… ì•ˆì „ì„± ë³´ì¥
    name = re.sub(r"\[.*?\]|\(.*?\)|<.*?>", "", name)
    name = re.sub(r"[^ê°€-í£a-zA-Z0-9\s]", "", name)
    
    remove_keywords = ["ë¬´ë£Œë°°ì†¡", "ì„¸ì¼", "ì´ë²¤íŠ¸", "ì‹ ìƒ", "ì¸ê¸°", "ë² ìŠ¤íŠ¸", "í•«ë”œ", "ë‚¨ì„±", "ì—¬ì„±"]
    for kw in remove_keywords:
        name = name.replace(kw, "")
    
    return name.strip()

def clean_data_quality(df):
    """ë°ì´í„° í’ˆì§ˆ ì •ë¦¬"""
    original_count = len(df)
    print(f"ğŸ“Š ì›ë³¸ ë°ì´í„° í–‰ ìˆ˜: {original_count}")
    
    # ìƒ‰ìƒ í•„í„°ë§
    if 'ìƒ‰ìƒ' in df.columns:
        before_count = len(df)
        df = df[~df['ìƒ‰ìƒ'].str.contains('ìƒì„¸', na=False)]
        df = df.dropna(subset=['ìƒ‰ìƒ'])
        df = df[df['ìƒ‰ìƒ'].str.strip() != '']
        print(f"ğŸ“Š ìƒ‰ìƒ í•„í„°ë§ í›„: {len(df)} (ì œê±°: {before_count - len(df)})")
    
    # ìƒí’ˆëª… ì •ë¦¬
    if 'ìƒí’ˆëª…' in df.columns:
        df['ìƒí’ˆëª…'] = df['ìƒí’ˆëª…'].apply(clean_product_name)
        before_count = len(df)
        df = df.dropna(subset=['ìƒí’ˆëª…'])
        df = df[df['ìƒí’ˆëª…'].str.strip() != '']
        print(f"ğŸ“Š ìƒí’ˆëª… ì •ë¦¬ í›„: {len(df)} (ì œê±°: {before_count - len(df)})")
    
    # ì¤‘ë³µ ì œê±° (ìƒí’ˆëª… 8ê¸€ì ê¸°ì¤€)
    if 'ìƒí’ˆëª…' in df.columns:
        before_count = len(df)
        df['ìƒí’ˆëª…_8ê¸€ì'] = df['ìƒí’ˆëª…'].str[:8]
        df = df.drop_duplicates(subset=['ìƒí’ˆëª…_8ê¸€ì'], keep='first')
        df = df.drop('ìƒí’ˆëª…_8ê¸€ì', axis=1)
        print(f"ğŸ“Š ì¤‘ë³µ ì œê±° í›„: {len(df)} (ì œê±°: {before_count - len(df)})")
    
    print(f"ğŸ“Š ìµœì¢… ë°ì´í„° í–‰ ìˆ˜: {len(df)} (ì „ì²´ ì œê±°: {original_count - len(df)})")
    return df

def update_major_categories(df):
    """ëŒ€ë¶„ë¥˜ ì—…ë°ì´íŠ¸"""
    category_mapping = {
        'ê¸´ì†Œë§¤': 'ìƒì˜', 'ë°˜ì†Œë§¤': 'ìƒì˜', 'í›„ë“œí‹°': 'ìƒì˜',
        'ë‹ˆíŠ¸/ìŠ¤ì›¨í„°': 'ìƒì˜', 'ì…”ì¸ /ë¸”ë¼ìš°ìŠ¤': 'ìƒì˜', 'í”¼ì¼€/ì¹´ë¼': 'ìƒì˜', 'ìŠ¬ë¦¬ë¸Œë¦¬ìŠ¤': 'ìƒì˜',
        'ìˆíŒ¬ì¸ ': 'ë°”ì§€', 'ë°ë‹˜íŒ¬ì¸ ': 'ë°”ì§€', 'ì½”íŠ¼íŒ¬ì¸ ': 'ë°”ì§€',
        'ìŠˆíŠ¸íŒ¬ì¸ /ìŠ¬ë™ìŠ¤': 'ë°”ì§€', 'ì¹´ê³ íŒ¬ì¸ ': 'ë°”ì§€', 'íŠ¸ë ˆì´ë‹/ì¡°ê±°íŒ¬ì¸ ': 'ë°”ì§€',
        'ë¡±ìŠ¤ì»¤íŠ¸': 'ìŠ¤ì»¤íŠ¸', 'ë¯¸ë‹ˆìŠ¤ì»¤íŠ¸': 'ìŠ¤ì»¤íŠ¸', 'ë¯¸ë””ìŠ¤ì»¤íŠ¸': 'ìŠ¤ì»¤íŠ¸',
        'ë§¥ì‹œì›í”¼ìŠ¤': 'ì›í”¼ìŠ¤', 'ë¯¸ë‹ˆì›í”¼ìŠ¤': 'ì›í”¼ìŠ¤', 'ë¯¸ë””ì›í”¼ìŠ¤': 'ì›í”¼ìŠ¤',
    }
    
    if 'ì†Œë¶„ë¥˜' in df.columns:
        updated_count = 0
        for idx, row in df.iterrows():
            if row['ì†Œë¶„ë¥˜'] in category_mapping:
                df.at[idx, 'ëŒ€ë¶„ë¥˜'] = category_mapping[row['ì†Œë¶„ë¥˜']]
                updated_count += 1
        print(f"ğŸ“Š ëŒ€ë¶„ë¥˜ ì—…ë°ì´íŠ¸: {updated_count}ê°œ í–‰")
    
    return df

def map_subcategories(df):
    """ì†Œë¶„ë¥˜ ë§¤í•‘"""
    subcategory_mapping = {
        'ê¸´ì†Œë§¤í‹°ì…”ì¸ ': 'ê¸´ì†Œë§¤', 'ë°˜ì†Œë§¤í‹°ì…”ì¸ ': 'ë°˜ì†Œë§¤', 'ë‹ˆíŠ¸ìŠ¤ì›¨í„°': 'ë‹ˆíŠ¸/ìŠ¤ì›¨í„°',
        'ì…”ì¸ ë¸”ë¼ìš°ìŠ¤': 'ì…”ì¸ /ë¸”ë¼ìš°ìŠ¤', 'í”¼ì¼€ì¹´ë¼': 'í”¼ì¼€/ì¹´ë¼', 'ë°ë‹˜íœì¸ ': 'ë°ë‹˜íŒ¬ì¸ ',
        'ìŠˆíŠ¸íŒ¬ì¸ ìŠ¬ë™ìŠ¤': 'ìŠˆíŠ¸íŒ¬ì¸ /ìŠ¬ë™ìŠ¤', 'íŠ¸ë ˆì´ë‹ì¡°ê±°íŒ¬ì¸ ': 'íŠ¸ë ˆì´ë‹/ì¡°ê±°íŒ¬ì¸ ',
    }
    
    if 'ì†Œë¶„ë¥˜' in df.columns:
        updated_count = 0
        for subcategory in subcategory_mapping:
            mask = df['ì†Œë¶„ë¥˜'] == subcategory
            df.loc[mask, 'ì†Œë¶„ë¥˜'] = subcategory_mapping[subcategory]
            updated_count += mask.sum()
        print(f"ğŸ“Š ì†Œë¶„ë¥˜ ë§¤í•‘: {updated_count}ê°œ í–‰")
    
    return df

def clean_gender_data(df):
    """ì„±ë³„ ë°ì´í„° ì •ë¦¬"""
    def map_gender_simple(gender_str):
        try:
            if pd.isna(gender_str):
                return 'ê³µìš©'
                
            if isinstance(gender_str, str):
                if gender_str.startswith('[') and gender_str.endswith(']'):
                    gender_list = ast.literal_eval(gender_str)
                else:
                    return gender_str
            else:
                gender_list = gender_str
                
            if gender_list == ['ë‚¨ì„±']:
                return 'ë‚¨ì„±'
            elif gender_list == ['ì—¬ì„±']:
                return 'ì—¬ì„±'
            elif gender_list == ['ë‚¨ì„±', 'ì—¬ì„±'] or gender_list == ['ì—¬ì„±', 'ë‚¨ì„±']:
                return 'ê³µìš©'
            elif gender_list == ['ë¼ì´í”„']:
                return 'ê³µìš©'
            else:
                return 'ê³µìš©'  # ê¸°ë³¸ê°’
        except Exception as e:
            print(f"âš ï¸ ì„±ë³„ ë°ì´í„° íŒŒì‹± ì˜¤ë¥˜: {gender_str} -> {e}")
            return 'ê³µìš©'
    
    if 'ì„±ë³„' in df.columns:
        df['ì„±ë³„'] = df['ì„±ë³„'].apply(map_gender_simple)
        print(f"ğŸ“Š ì„±ë³„ ë°ì´í„° ì •ë¦¬ ì™„ë£Œ")
    
    return df

def map_english_brand_names(df, brand_mapping):
    """í•œê¸€ë¸Œëœë“œëª…ì„ ì˜ì–´ë¸Œëœë“œëª…ìœ¼ë¡œ ë§¤í•‘"""
    if 'í•œê¸€ë¸Œëœë“œëª…' in df.columns and brand_mapping:
        before_count = len(df)
        df['ì˜ì–´ë¸Œëœë“œëª…'] = df['í•œê¸€ë¸Œëœë“œëª…'].map(brand_mapping)
        df = df.dropna(subset=['ì˜ì–´ë¸Œëœë“œëª…'])
        print(f"ğŸ“Š ì˜ì–´ë¸Œëœë“œëª… ë§¤í•‘ í›„: {len(df)} (ì œê±°: {before_count - len(df)})")
    elif not brand_mapping:
        print("âš ï¸ ë¸Œëœë“œ ë§¤í•‘ ë°ì´í„°ê°€ ì—†ì–´ ì˜ì–´ë¸Œëœë“œëª… ë§¤í•‘ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    
    return df

def process_musinsa_csv_files(**context):
    """S3ì˜ musinsa_csvs ë””ë ‰í† ë¦¬ì—ì„œ ìµœì‹  CSV íŒŒì¼ë“¤ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜"""
    
    logger.info("ğŸš€ ë¬´ì‹ ì‚¬ CSV ì²˜ë¦¬ ì‹œì‘")
    
    # ë¸Œëœë“œ ë§¤í•‘ ë¡œë“œ
    brand_mapping = load_brand_mapping()
    
    # S3CSVLoader ì´ˆê¸°í™”
    if not s3_loader.initialize():
        raise Exception("S3CSVLoader ì´ˆê¸°í™” ì‹¤íŒ¨")
    
    logger.info(f"âœ… S3CSVLoader ì‚¬ìš©: {s3_loader.bucket_name}")
    
    try:
        # ê°€ì¥ ìµœê·¼ ë‚ ì§œì˜ CSV íŒŒì¼ë“¤ ë¡œë“œ
        logger.info("ğŸ” ê°€ì¥ ìµœê·¼ ë‚ ì§œì˜ CSV íŒŒì¼ë“¤ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
        latest_date, dataframes = s3_loader.load_latest_csv_files(max_files=10)  # ìµœëŒ€ 10ê°œ íŒŒì¼
        
        if not latest_date or not dataframes:
            logger.error("âŒ ë¡œë“œí•  CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            raise Exception("ë¡œë“œí•  CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        logger.info(f"âœ… {latest_date} ë‚ ì§œì˜ CSV íŒŒì¼ {len(dataframes)}ê°œ ë¡œë“œ ì™„ë£Œ")
        
    except Exception as e:
        logger.error(f"âŒ S3ì—ì„œ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise
    
    all_data = []
    successful_files = []
    failed_files = []
    
    # ê° DataFrame ì²˜ë¦¬
    for i, df in enumerate(dataframes):
        try:
            logger.info(f"ğŸ“Š DataFrame {i+1} ì²˜ë¦¬ ì¤‘: {len(df)}í–‰, {len(df.columns)}ì—´")
            
            # ë°ì´í„° ì „ì²˜ë¦¬
            df = normalize_columns(df)
            df = clean_data_quality(df)
            df = update_major_categories(df)
            df = map_subcategories(df)
            df = clean_gender_data(df)
            df = map_english_brand_names(df, brand_mapping)
            df['ì‚¬ì´íŠ¸ëª…'] = 'musinsa'
            
            logger.info(f"ğŸ“Š ì²˜ë¦¬ëœ ë°ì´í„°: {len(df)}í–‰")
            
            if len(df) > 0:  # ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ ì¶”ê°€
                all_data.append(df)
                successful_files.append(f"DataFrame_{i+1}")
            else:
                logger.warning(f"âš ï¸ ì²˜ë¦¬ í›„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤: DataFrame_{i+1}")
                failed_files.append(f"DataFrame_{i+1}")
            
        except Exception as e:
            logger.error(f"âŒ DataFrame ì²˜ë¦¬ ì‹¤íŒ¨ (DataFrame_{i+1}): {e}")
            failed_files.append(f"DataFrame_{i+1}")
            continue
    
    logger.info(f"ğŸ“Š ì²˜ë¦¬ ê²°ê³¼:")
    logger.info(f"   ì„±ê³µ: {len(successful_files)}ê°œ íŒŒì¼")
    logger.info(f"   ì‹¤íŒ¨: {len(failed_files)}ê°œ íŒŒì¼")
    
    if not all_data:
        logger.error("âŒ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        raise Exception("ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    # ë°ì´í„° ê²°í•©
    logger.info("ğŸ”„ ë°ì´í„° ê²°í•© ì¤‘...")
    combined_df = pd.concat(all_data, ignore_index=True)
    logger.info(f"ğŸ“Š ê²°í•©ëœ ë°ì´í„°: {len(combined_df)}í–‰")
    
    # ìµœì¢… ì»¬ëŸ¼ ì„ íƒ
    final_columns = [
        'ìƒí’ˆì½”ë“œ', 'ìƒí’ˆëª…', 'ëŒ€ë¶„ë¥˜', 'ì†Œë¶„ë¥˜', 'ì›ê°€', 'í• ì¸ê°€', 
        'ì´ë¯¸ì§€URL', 'í•œê¸€ë¸Œëœë“œëª…', 'ì„±ë³„', 'ì¢‹ì•„ìš”ìˆ˜', 'ì†Œì¬', 
        'ìƒ‰ìƒ', 'ìƒí’ˆë§í¬', 'ì˜ì–´ë¸Œëœë“œëª…', 'ì‚¬ì´íŠ¸ëª…'
    ]
    
    available_columns = [col for col in final_columns if col in combined_df.columns]
    combined_df = combined_df[available_columns]
    
    logger.info(f"ğŸ“Š ìµœì¢… ì»¬ëŸ¼: {available_columns}")
    
    # ì¶œë ¥ íŒŒì¼ ì €ì¥
    output_filename = f'/home/tjddml/airflow/musinsa/{datetime.now().strftime("%Y%m%d_%H%M")}.csv'
    combined_df.to_csv(output_filename, index=False, encoding='utf-8-sig')
    logger.info(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {output_filename}")
    
    # XComì— ê²°ê³¼ ì €ì¥
    context['task_instance'].xcom_push(key='output_filename', value=output_filename)
    context['task_instance'].xcom_push(key='total_rows', value=len(combined_df))
    context['task_instance'].xcom_push(key='total_columns', value=len(combined_df.columns))
    context['task_instance'].xcom_push(key='successful_files', value=len(successful_files))
    context['task_instance'].xcom_push(key='failed_files', value=len(failed_files))
    context['task_instance'].xcom_push(key='latest_date', value=latest_date)
    
    logger.info("âœ… ë¬´ì‹ ì‚¬ CSV ì²˜ë¦¬ ì™„ë£Œ")
    return output_filename

# DAG ì •ì˜
dag = DAG(
    'musinsa_data_processing',
    default_args=default_args,
    description='ë¬´ì‹ ì‚¬ CSV íŒŒì¼ ì „ì²˜ë¦¬ DAG',
    schedule_interval=None,
    catchup=False,
    tags=['musinsa', 'data-processing']
)

# Task ì •ì˜
process_musinsa_task = PythonOperator(
    task_id='process_musinsa_csv_files',
    python_callable=process_musinsa_csv_files,
    dag=dag
)

# wì»¨ì…‰ ì „ì²˜ë¦¬ DAG íŠ¸ë¦¬ê±°
trigger_wconcept_preprocessor = TriggerDagRunOperator(
    task_id='trigger_wconcept_preprocessor_csv',
    trigger_dag_id='wconcept_preprocessor_csv',
    wait_for_completion=False,
    poke_interval=10,
    allowed_states=['success'],
    failed_states=['failed'],
    dag=dag,
    doc_md="""
    ## wì»¨ì…‰ ì „ì²˜ë¦¬ DAG ìë™ ì‹¤í–‰
    
    ### ê¸°ëŠ¥
    - ë¬´ì‹ ì‚¬ ì „ì²˜ë¦¬ ì™„ë£Œ í›„ ìë™ìœ¼ë¡œ wì»¨ì…‰ ì „ì²˜ë¦¬ DAG ì‹¤í–‰
    - wì»¨ì…‰ ë°ì´í„° ì „ì²˜ë¦¬ ë° ì •ë¦¬
    
    ### ì‹¤í–‰ íë¦„
    1. ë¬´ì‹ ì‚¬ ì „ì²˜ë¦¬ ì™„ë£Œ
    2. wì»¨ì…‰ ì „ì²˜ë¦¬ DAG ìë™ ì‹¤í–‰
    3. wì»¨ì…‰ ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ
    """
)

# Task ì˜ì¡´ì„± ì„¤ì •
process_musinsa_task >> trigger_wconcept_preprocessor