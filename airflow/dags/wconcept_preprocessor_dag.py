from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.dummy import DummyOperator
from airflow.operators.trigger_dagrun import TriggerDagRunOperator
from airflow.models import Variable
import pandas as pd
import re
import os
from typing import Dict, List, Optional

# DAG ê¸°ë³¸ ì„¤ì •
default_args = {
    'owner': 'data_team',
    'depends_on_past': False,
    'start_date': datetime(2023, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=2),
    'catchup': False
}

# DAG ì •ì˜
dag = DAG(
    'wconcept_preprocessor_csv',
    default_args=default_args,
    description='Wì»¨ì…‰ ìƒí’ˆ ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ DAG (CSV ê¸°ë°˜)',
    schedule_interval=None,
    max_active_runs=1,
    tags=['preprocessing', 'wconcept', 'ecommerce', 'data-cleaning', 'csv-based']
)

# íŒŒì¼ ê²½ë¡œ ì„¤ì •
BASE_PATH = "/home/tjddml/airflow/dags/wconcept"
INPUT_FILE = "../w/parsed_products.csv"
BRANDS_FILE = "/home/tjddml/airflow/dags/brands.csv"
TEMP_FILES = {
    'step1': f"{BASE_PATH}/tmp_step1_loaded.csv",
    'step2': f"{BASE_PATH}/tmp_step2_cleaned.csv", 
    'step3': f"{BASE_PATH}/tmp_step3_colors.csv",
    'step4': f"{BASE_PATH}/tmp_step4_filtered.csv",
    'step5': f"{BASE_PATH}/tmp_step5_brands.csv",
    'step6': f"{BASE_PATH}/tmp_step6_brand_filtered.csv",
    'step7': f"{BASE_PATH}/tmp_step7_categories.csv"
}
OUTPUT_FILE = f"{BASE_PATH}/processed_products.csv"

class WconceptPreprocessor:
    def __init__(self):
        """Wì»¨ì…‰ ì „ì²˜ë¦¬ í´ë˜ìŠ¤"""
        # ëŒ€ë¶„ë¥˜-ì†Œë¶„ë¥˜ ë§¤í•‘ ì •ì˜
        self.category_mapping = {
            'ìƒì˜': ['í›„ë“œí‹°', 'ì…”ì¸ /ë¸”ë¼ìš°ìŠ¤', 'ê¸´ì†Œë§¤', 'ë°˜ì†Œë§¤', 'í”¼ì¼€/ì¹´ë¼', 'ë‹ˆíŠ¸/ìŠ¤ì›¨í„°', 'ìŠ¬ë¦¬ë¸Œë¦¬ìŠ¤'],
            'ë°”ì§€': ['ë°ë‹˜íŒ¬ì¸ ', 'íŠ¸ë ˆì´ë‹/ì¡°ê±°íŒ¬ì¸ ', 'ì½”íŠ¼íŒ¬ì¸ ', 'ìŠˆíŠ¸íŒ¬ì¸ /ìŠ¬ë™ìŠ¤', 'ìˆíŒ¬ì¸ ', 'ì¹´ê³ íŒ¬ì¸ '],
            'ì›í”¼ìŠ¤': ['ë¯¸ë‹ˆì›í”¼ìŠ¤', 'ë¯¸ë””ì›í”¼ìŠ¤', 'ë§¥ì‹œì›í”¼ìŠ¤'],
            'ìŠ¤ì»¤íŠ¸': ['ë¯¸ë‹ˆìŠ¤ì»¤íŠ¸', 'ë¯¸ë””ìŠ¤ì»¤íŠ¸', 'ë¡±ìŠ¤ì»¤íŠ¸']
        }

# íƒœìŠ¤í¬ í•¨ìˆ˜ë“¤ (ê° íƒœìŠ¤í¬ëŠ” CSV íŒŒì¼ì„ ì½ê³  ì²˜ë¦¬ í›„ ë‹¤ìŒ CSV íŒŒì¼ë¡œ ì €ì¥)

def load_data_task(**context):
    """1ë‹¨ê³„: ì´ˆê¸° ë°ì´í„° ë¡œë“œ ë° ë¸Œëœë“œ ë§¤í•‘ ì¤€ë¹„"""
    print("ğŸ“‚ 1ë‹¨ê³„: ë°ì´í„° ë¡œë“œ ì¤‘...")
    
    # ì›ë³¸ ë°ì´í„° ë¡œë“œ
    df = pd.read_csv(INPUT_FILE, encoding='utf-8-sig')
    print(f"âœ… ì›ë³¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}í–‰")
    
    # ë¸Œëœë“œ ë§¤í•‘ ë¡œë“œ ë° í™•ì¸
    brands_df = pd.read_csv(BRANDS_FILE, encoding='utf-8-sig')
    brands_mapping = dict(zip(brands_df['ì˜ì–´ë¸Œëœë“œëª…'], brands_df['í•œê¸€ë¸Œëœë“œëª…']))
    print(f"âœ… ë¸Œëœë“œ ë§¤í•‘ ë¡œë“œ ì™„ë£Œ: {len(brands_mapping)}ê°œ")
    
    # 1ë‹¨ê³„ ê²°ê³¼ ì €ì¥
    df.to_csv(TEMP_FILES['step1'], index=False, encoding='utf-8-sig')
    print(f"ğŸ’¾ 1ë‹¨ê³„ ê²°ê³¼ ì €ì¥: {TEMP_FILES['step1']}")
    
    return f"ë°ì´í„° ë¡œë“œ ì™„ë£Œ - {len(df)}í–‰"

def clean_product_names_task(**context):
    """2ë‹¨ê³„: ìƒí’ˆëª… ì „ì²˜ë¦¬"""
    print("ğŸ§¹ 2ë‹¨ê³„: ìƒí’ˆëª… ì „ì²˜ë¦¬ ì¤‘...")
    
    # ì´ì „ ë‹¨ê³„ ê²°ê³¼ ë¡œë“œ
    df = pd.read_csv(TEMP_FILES['step1'], encoding='utf-8-sig')
    print(f"ğŸ“‚ ì´ì „ ë‹¨ê³„ ë°ì´í„° ë¡œë“œ: {len(df)}í–‰")
    
    def clean_name(name):
        if pd.isna(name):
            return name
        
        # ê´„í˜¸ì™€ ëŒ€ê´„í˜¸ ì•ˆ ë‚´ìš© ì œê±°
        name = re.sub(r'\([^)]*\)', '', str(name))
        name = re.sub(r'\[[^\]]*\]', '', name)
        
        # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬
        name = re.sub(r'[^\w\sê°€-í£]', ' ', name)
        name = re.sub(r'\s+', ' ', name)
        
        return name.strip()
    
    # ìƒí’ˆëª… ì •ë¦¬
    df['ìƒí’ˆëª…'] = df['ìƒí’ˆëª…'].apply(clean_name)
    print("âœ… ìƒí’ˆëª… ì „ì²˜ë¦¬ ì™„ë£Œ")
    
    # 2ë‹¨ê³„ ê²°ê³¼ ì €ì¥
    df.to_csv(TEMP_FILES['step2'], index=False, encoding='utf-8-sig')
    print(f"ğŸ’¾ 2ë‹¨ê³„ ê²°ê³¼ ì €ì¥: {TEMP_FILES['step2']}")
    
    return f"ìƒí’ˆëª… ì „ì²˜ë¦¬ ì™„ë£Œ - {len(df)}í–‰"

def extract_colors_task(**context):
    """3ë‹¨ê³„: ìƒí’ˆëª…ì—ì„œ ìƒ‰ìƒ ì •ë³´ ì¶”ì¶œ"""
    print("ğŸ¨ 3ë‹¨ê³„: ìƒí’ˆëª…ì—ì„œ ìƒ‰ìƒ ì¶”ì¶œ ì¤‘...")
    
    # ì´ì „ ë‹¨ê³„ ê²°ê³¼ ë¡œë“œ
    df = pd.read_csv(TEMP_FILES['step2'], encoding='utf-8-sig')
    print(f"ğŸ“‚ ì´ì „ ë‹¨ê³„ ë°ì´í„° ë¡œë“œ: {len(df)}í–‰")
    
    # ìƒ‰ìƒ íŒ¨í„´ ì •ì˜
    color_patterns = {
        'black': [r'ë¸”ë™', r'black', r'\bbk\b', r'í‘'],
        'white': [r'í™”ì´íŠ¸', r'white', r'\bwh\b', r'ë°±ìƒ‰', r'ì˜¤í”„í™”ì´íŠ¸'],
        'ivory': [r'ì•„ì´ë³´ë¦¬', r'ivory', r'\biv\b'],
        'beige': [r'ë² ì´ì§€', r'beige', r'\bbg\b'],
        'brown': [r'ë¸Œë¼ìš´', r'brown', r'\bbr\b', r'ì¹´í‚¤', r'ì°¨ì½œ'],
        'navy': [r'ë„¤ì´ë¹„', r'navy', r'\bnv\b'],
        'blue': [r'ë¸”ë£¨', r'blue', r'\bbl\b', r'ìŠ¤ì¹´ì´ë¸”ë£¨'],
        'denim': [r'ë°ë‹˜', r'denim', r'ì¸ë””ê³ '],
        'green': [r'ê·¸ë¦°', r'green', r'\bgr\b'],
        'grey': [r'ê·¸ë ˆì´', r'grey', r'gray', r'\bgy\b'],
        'red': [r'ë ˆë“œ', r'red', r'\brd\b', r'ì™€ì¸'],
        'orange': [r'ì˜¤ë Œì§€', r'orange', r'\borg\b'],
        'yellow': [r'ì˜ë¡œìš°', r'yellow', r'\bye\b'],
        'pink': [r'í•‘í¬', r'pink', r'\bpk\b', r'ë¡œì¦ˆ'],
        'purple': [r'í¼í”Œ', r'purple', r'\bpp\b', r'ë³´ë¼'],
        'mint': [r'ë¯¼íŠ¸', r'mint'],
        'cream': [r'í¬ë¦¼', r'cream'],
        'gold': [r'ê³¨ë“œ', r'gold'],
        'silver': [r'ì‹¤ë²„', r'silver'],
        'multi': [r'ë©€í‹°', r'multi']
    }
    
    def extract_color(name):
        if pd.isna(name):
            return None
        
        name = str(name).lower()
        found_colors = []
        
        for color, patterns in color_patterns.items():
            for pattern in patterns:
                if re.search(pattern, name, re.IGNORECASE):
                    found_colors.append(color)
                    break
        
        return ', '.join(found_colors) if found_colors else None
    
    # ìƒ‰ìƒ ì¶”ì¶œ
    df['ìƒ‰ìƒ_ì¶”ì¶œ'] = df['ìƒí’ˆëª…'].apply(extract_color)
    
    # ê¸°ì¡´ ìƒ‰ìƒê³¼ ì¶”ì¶œëœ ìƒ‰ìƒ ê²°í•©
    df['ìƒ‰ìƒ'] = df['ìƒ‰ìƒ'].fillna('')
    df['ìƒ‰ìƒ_ì¶”ì¶œ'] = df['ìƒ‰ìƒ_ì¶”ì¶œ'].fillna('')
    
    # ìƒ‰ìƒì´ ë¹„ì–´ìˆìœ¼ë©´ ì¶”ì¶œëœ ìƒ‰ìƒ ì‚¬ìš©
    mask = (df['ìƒ‰ìƒ'].str.strip() == '') & (df['ìƒ‰ìƒ_ì¶”ì¶œ'].str.strip() != '')
    df.loc[mask, 'ìƒ‰ìƒ'] = df.loc[mask, 'ìƒ‰ìƒ_ì¶”ì¶œ']
    
    # ìƒ‰ìƒ_ì¶”ì¶œ ì»¬ëŸ¼ ì‚­ì œ
    df = df.drop('ìƒ‰ìƒ_ì¶”ì¶œ', axis=1)
    
    extracted_count = (df['ìƒ‰ìƒ'].str.strip() != '').sum()
    print(f"âœ… ìƒ‰ìƒ ì¶”ì¶œ ì™„ë£Œ: {extracted_count}ê°œ ìƒí’ˆì— ìƒ‰ìƒ ì •ë³´")
    
    # 3ë‹¨ê³„ ê²°ê³¼ ì €ì¥
    df.to_csv(TEMP_FILES['step3'], index=False, encoding='utf-8-sig')
    print(f"ğŸ’¾ 3ë‹¨ê³„ ê²°ê³¼ ì €ì¥: {TEMP_FILES['step3']}")
    
    return f"ìƒ‰ìƒ ì¶”ì¶œ ì™„ë£Œ - {extracted_count}ê°œ ìƒí’ˆ ìƒ‰ìƒ ì •ë³´ ì¶”ê°€"

def filter_missing_data_task(**context):
    """4ë‹¨ê³„: í•„ìˆ˜ ë°ì´í„° í•„í„°ë§ (ì„±ë³„, ì›ê°€ ì—†ìœ¼ë©´ ì‚­ì œ)"""
    print("ğŸ” 4ë‹¨ê³„: í•„ìˆ˜ ë°ì´í„° í•„í„°ë§ ì¤‘...")
    
    # ì´ì „ ë‹¨ê³„ ê²°ê³¼ ë¡œë“œ
    df = pd.read_csv(TEMP_FILES['step3'], encoding='utf-8-sig')
    initial_count = len(df)
    print(f"ğŸ“‚ ì´ì „ ë‹¨ê³„ ë°ì´í„° ë¡œë“œ: {initial_count}í–‰")
    
    # ì„±ë³„ì´ ì—†ëŠ” í–‰ ì œê±°
    df = df.dropna(subset=['ì„±ë³„'])
    df = df[df['ì„±ë³„'].str.strip() != '']
    
    # ì›ê°€ê°€ ì—†ëŠ” í–‰ ì œê±°
    df = df.dropna(subset=['ì›ê°€'])
    
    filtered_count = len(df)
    removed_count = initial_count - filtered_count
    print(f"âœ… í•„ìˆ˜ ë°ì´í„° í•„í„°ë§ ì™„ë£Œ: {initial_count} â†’ {filtered_count}í–‰ (ì‚­ì œ: {removed_count}í–‰)")
    
    # 4ë‹¨ê³„ ê²°ê³¼ ì €ì¥
    df.to_csv(TEMP_FILES['step4'], index=False, encoding='utf-8-sig')
    print(f"ğŸ’¾ 4ë‹¨ê³„ ê²°ê³¼ ì €ì¥: {TEMP_FILES['step4']}")
    
    return f"í•„ìˆ˜ ë°ì´í„° í•„í„°ë§ ì™„ë£Œ - {removed_count}í–‰ ì‚­ì œ, {filtered_count}í–‰ ìœ ì§€"



def map_brands_task(**context):
    """5ë‹¨ê³„: ë¸Œëœë“œëª… í•œê¸€ ë§¤í•‘"""
    print("ğŸ·ï¸ 5ë‹¨ê³„: ë¸Œëœë“œëª… ë§¤í•‘ ì¤‘...")
    
    # ì´ì „ ë‹¨ê³„ ê²°ê³¼ ë¡œë“œ
    df = pd.read_csv(TEMP_FILES['step4'], encoding='utf-8-sig')
    print(f"ğŸ“‚ ì´ì „ ë‹¨ê³„ ë°ì´í„° ë¡œë“œ: {len(df)}í–‰")
    
    # ë¸Œëœë“œ ë§¤í•‘ ë¡œë“œ
    brands_df = pd.read_csv(BRANDS_FILE, encoding='utf-8-sig')
    brands_mapping = dict(zip(brands_df['ì˜ì–´ë¸Œëœë“œëª…'], brands_df['í•œê¸€ë¸Œëœë“œëª…']))
    
    # ë¸Œëœë“œ ë§¤í•‘ í•¨ìˆ˜
    def map_brand(brand_name, brands_mapping):
        if pd.isna(brand_name):
            return None
        brand_name = str(brand_name).strip()
        if brand_name == "":
            return None
        if brand_name in brands_mapping:
            return brands_mapping[brand_name]
        brand_lower = brand_name.lower()
        for eng, kor in brands_mapping.items():
            if str(eng).lower() == brand_lower:
                return kor
        return None
    
    # applyë¡œ ë§¤í•‘ ì ìš©
    df['ë¸Œëœë“œëª…_í•œê¸€'] = df['ë¸Œëœë“œëª…'].apply(lambda x: map_brand(x, brands_mapping))
    
    # ë§¤í•‘ ì„±ê³µë¥  í™•ì¸
    mapped_count = df['ë¸Œëœë“œëª…_í•œê¸€'].notna().sum()
    total_count = len(df)
    print(f"âœ… ë¸Œëœë“œ ë§¤í•‘ ì™„ë£Œ: {mapped_count}/{total_count} ({mapped_count/total_count*100:.1f}%)")
    
    # ë‹¤ìŒ ë‹¨ê³„ìš© ì„ì‹œ íŒŒì¼ ì €ì¥
    df.to_csv(TEMP_FILES['step5'], index=False, encoding='utf-8-sig')
    
    return "ë¸Œëœë“œëª… ë§¤í•‘ ì™„ë£Œ"


def remove_unmapped_brands_task(**context):
    """6ë‹¨ê³„: ë§¤í•‘ ë¶ˆê°€ëŠ¥í•œ ë¸Œëœë“œ ì œê±°"""
    print("âŒ 6ë‹¨ê³„: ë§¤í•‘ ë¶ˆê°€ëŠ¥í•œ ë¸Œëœë“œ ì œê±° ì¤‘...")
    
    # ì´ì „ ë‹¨ê³„ ê²°ê³¼ ë¡œë“œ
    df = pd.read_csv(TEMP_FILES['step5'], encoding='utf-8-sig')
    initial_count = len(df)
    print(f"ğŸ“‚ ì´ì „ ë‹¨ê³„ ë°ì´í„° ë¡œë“œ: {initial_count}í–‰")
    
    # ë§¤í•‘ë˜ì§€ ì•Šì€ ë¸Œëœë“œ ì œê±°
    df = df.dropna(subset=['ë¸Œëœë“œëª…_í•œê¸€'])
    final_count = len(df)
    removed_count = initial_count - final_count
    
    print(f"âœ… ë§¤í•‘ ë¶ˆê°€ëŠ¥í•œ ë¸Œëœë“œ ì œê±° ì™„ë£Œ: {initial_count} â†’ {final_count}í–‰ (ì‚­ì œ: {removed_count}í–‰)")
    
    # 6ë‹¨ê³„ ê²°ê³¼ ì €ì¥
    df.to_csv(TEMP_FILES['step6'], index=False, encoding='utf-8-sig')
    print(f"ğŸ’¾ 6ë‹¨ê³„ ê²°ê³¼ ì €ì¥: {TEMP_FILES['step6']}")
    
    return f"ë¸Œëœë“œ ì œê±° ì™„ë£Œ - {removed_count}í–‰ ì‚­ì œ, {final_count}í–‰ ìœ ì§€"

def process_categories_task(**context):
    """7ë‹¨ê³„: ì¹´í…Œê³ ë¦¬ ì „ì²˜ë¦¬ (ì†Œë¶„ë¥˜ â†’ ëŒ€ë¶„ë¥˜ ë§¤í•‘)"""
    print("ğŸ“‚ 7ë‹¨ê³„: ì¹´í…Œê³ ë¦¬ ì „ì²˜ë¦¬ ì¤‘...")
    
    # ì´ì „ ë‹¨ê³„ ê²°ê³¼ ë¡œë“œ
    df = pd.read_csv(TEMP_FILES['step6'], encoding='utf-8-sig')
    print(f"ğŸ“‚ ì´ì „ ë‹¨ê³„ ë°ì´í„° ë¡œë“œ: {len(df)}í–‰")
    
    # ì¹´í…Œê³ ë¦¬ ë§¤í•‘ ì •ì˜
    category_mapping = {
        'ìƒì˜': ['í›„ë“œí‹°', 'ì…”ì¸ /ë¸”ë¼ìš°ìŠ¤', 'ê¸´ì†Œë§¤', 'ë°˜ì†Œë§¤', 'í”¼ì¼€/ì¹´ë¼', 'ë‹ˆíŠ¸/ìŠ¤ì›¨í„°', 'ìŠ¬ë¦¬ë¸Œë¦¬ìŠ¤'],
        'ë°”ì§€': ['ë°ë‹˜íŒ¬ì¸ ', 'íŠ¸ë ˆì´ë‹/ì¡°ê±°íŒ¬ì¸ ', 'ì½”íŠ¼íŒ¬ì¸ ', 'ìŠˆíŠ¸íŒ¬ì¸ /ìŠ¬ë™ìŠ¤', 'ìˆíŒ¬ì¸ ', 'ì¹´ê³ íŒ¬ì¸ '],
        'ì›í”¼ìŠ¤': ['ë¯¸ë‹ˆì›í”¼ìŠ¤', 'ë¯¸ë””ì›í”¼ìŠ¤', 'ë§¥ì‹œì›í”¼ìŠ¤'],
        'ìŠ¤ì»¤íŠ¸': ['ë¯¸ë‹ˆìŠ¤ì»¤íŠ¸', 'ë¯¸ë””ìŠ¤ì»¤íŠ¸', 'ë¡±ìŠ¤ì»¤íŠ¸']
    }
    
    def standardize_subcategory(subcategory, main_category=None):
        """ì†Œë¶„ë¥˜ í‘œì¤€í™” (ëŒ€ë¶„ë¥˜ì— ë”°ë¼ ì •í™•í•œ ë§¤í•‘)"""
        if pd.isna(subcategory):
            return None
        
        subcategory = str(subcategory).strip()
        
        # ê¸°ë³¸ ë§¤í•‘ ê·œì¹™
        basic_mappings = {
            'íŠ¸ë ˆì´ë‹/ì¡°ê±°': 'íŠ¸ë ˆì´ë‹/ì¡°ê±°íŒ¬ì¸ ',
            'ì¹´ê³ ': 'ì¹´ê³ íŒ¬ì¸ ',
            'ì¡°ê±°': 'íŠ¸ë ˆì´ë‹/ì¡°ê±°íŒ¬ì¸ ',
            'ìŠ¬ë™ìŠ¤': 'ìŠˆíŠ¸íŒ¬ì¸ /ìŠ¬ë™ìŠ¤',
            'ì‡¼ì¸ ': 'ìˆíŒ¬ì¸ ',
            'ê¸´íŒ”': 'ê¸´ì†Œë§¤',
            'ë°˜íŒ”': 'ë°˜ì†Œë§¤',
            'ìŠ¤ì›»': 'ë‹ˆíŠ¸/ìŠ¤ì›¨í„°',
            'í„°í‹€ë„¥': 'ë‹ˆíŠ¸/ìŠ¤ì›¨í„°',
            'ì¹˜ë…¸': 'ì½”íŠ¼íŒ¬ì¸ ',
            'í›„ë“œ': 'í›„ë“œí‹°',
            'ì…”ì¸ ': 'ì…”ì¸ /ë¸”ë¼ìš°ìŠ¤',
            'ë¸”ë¼ìš°ìŠ¤': 'ì…”ì¸ /ë¸”ë¼ìš°ìŠ¤',
            'ë°ë‹˜': 'ë°ë‹˜íŒ¬ì¸ ',
            'í”¼ì¼€': 'í”¼ì¼€/ì¹´ë¼',
            'ì¹´ë¼': 'í”¼ì¼€/ì¹´ë¼',
            'ìŠ¬ë¦¬ë¸Œë¦¬ìŠ¤': 'ìŠ¬ë¦¬ë¸Œë¦¬ìŠ¤'
        }
        
        # ê¸°ë³¸ ë§¤í•‘ ì ìš©
        for old_term, new_term in basic_mappings.items():
            if old_term in subcategory:
                return new_term
        
        # ëŒ€ë¶„ë¥˜ì— ë”°ë¥¸ ê¸¸ì´ë³„ ë§¤í•‘
        if main_category == 'ì›í”¼ìŠ¤':
            if 'ë¯¸ë‹ˆ' in subcategory:
                return 'ë¯¸ë‹ˆì›í”¼ìŠ¤'
            elif 'ë¯¸ë””' in subcategory:
                return 'ë¯¸ë””ì›í”¼ìŠ¤'
            elif 'ë¡±' in subcategory or 'ë§¥ì‹œ' in subcategory:
                return 'ë§¥ì‹œì›í”¼ìŠ¤'
            else:
                return 'ë¯¸ë””ì›í”¼ìŠ¤'  # ê¸°ë³¸ê°’
        elif main_category == 'ìŠ¤ì»¤íŠ¸':
            if 'ë¯¸ë‹ˆ' in subcategory:
                return 'ë¯¸ë‹ˆìŠ¤ì»¤íŠ¸'
            elif 'ë¯¸ë””' in subcategory:
                return 'ë¯¸ë””ìŠ¤ì»¤íŠ¸'
            elif 'ë¡±' in subcategory:
                return 'ë¡±ìŠ¤ì»¤íŠ¸'
            else:
                return 'ë¯¸ë””ìŠ¤ì»¤íŠ¸'  # ê¸°ë³¸ê°’
        
        # ë§¤í•‘ë˜ì§€ ì•Šì€ ê²½ìš° None ë°˜í™˜ (ì‚­ì œ ëŒ€ìƒ)
        return None
    
    def map_subcategory_to_main(subcategory):
        if pd.isna(subcategory):
            return None
        
        subcategory = str(subcategory).strip()
        
        # ê° ëŒ€ë¶„ë¥˜ë³„ë¡œ ì†Œë¶„ë¥˜ ë§¤í•‘
        for main_cat, sub_cats in category_mapping.items():
            for sub_cat in sub_cats:
                if sub_cat in subcategory:
                    return main_cat
        
        # íŠ¹ë³„í•œ ê²½ìš° ì²˜ë¦¬
        if 'í‹°ì…”ì¸ ' in subcategory:
            return 'ìƒì˜'
        elif 'íŒ¬ì¸ ' in subcategory or 'ë°”ì§€' in subcategory:
            return 'ë°”ì§€'
        elif 'ì›í”¼ìŠ¤' in subcategory:
            return 'ì›í”¼ìŠ¤'
        elif 'ìŠ¤ì»¤íŠ¸' in subcategory:
            return 'ìŠ¤ì»¤íŠ¸'
        
        return None
    
    # ëŒ€ë¶„ë¥˜ê°€ ìƒì˜/ë°”ì§€/ì›í”¼ìŠ¤/ìŠ¤ì»¤íŠ¸ê°€ ì•„ë‹Œ ë°ì´í„° ëª¨ë‘ ì œê±°
    valid_categories = ['ìƒì˜', 'ë°”ì§€', 'ì›í”¼ìŠ¤', 'ìŠ¤ì»¤íŠ¸']
    before_filter = len(df)
    
    df = df[df['ëŒ€ë¶„ë¥˜'].isin(valid_categories)]
    after_filter = len(df)
    removed_count = before_filter - after_filter
    
    print(f"ğŸ—‘ï¸ ìœ íš¨í•˜ì§€ ì•Šì€ ëŒ€ë¶„ë¥˜ ì œê±°: {removed_count}í–‰ ì‚­ì œ")
    print(f"ğŸ“Š í•„í„°ë§ í›„ ë°ì´í„°: {after_filter}í–‰")
    
    # ì†Œë¶„ë¥˜ë§Œ í‘œì¤€í™” (ê¸°ì¡´ ëŒ€ë¶„ë¥˜ ì •ë³´ ì‚¬ìš©)
    df['ì†Œë¶„ë¥˜_ì •ë¦¬'] = df.apply(
        lambda row: standardize_subcategory(row['ì†Œë¶„ë¥˜'], row['ëŒ€ë¶„ë¥˜']), 
        axis=1
    )
    
    # ë§¤í•‘ë˜ì§€ ì•Šì€ ì†Œë¶„ë¥˜ í™•ì¸ ë° ì œê±°
    unmapped = df[df['ì†Œë¶„ë¥˜_ì •ë¦¬'].isna()]
    if len(unmapped) > 0:
        unique_unmapped = unmapped['ì†Œë¶„ë¥˜'].unique()
        print(f"âš ï¸ ë§¤í•‘ë˜ì§€ ì•Šì€ ì†Œë¶„ë¥˜ ({len(unique_unmapped)}ê°œ): {list(unique_unmapped)[:10]}...")
        
        # ë§¤í•‘ë˜ì§€ ì•Šì€ í–‰ë“¤ ì œê±°
        before_removal = len(df)
        df = df.dropna(subset=['ì†Œë¶„ë¥˜_ì •ë¦¬'])
        after_removal = len(df)
        removed_count = before_removal - after_removal
        
        print(f"ğŸ—‘ï¸ ë§¤í•‘ë˜ì§€ ì•Šì€ ì†Œë¶„ë¥˜ ì œê±°: {removed_count}í–‰ ì‚­ì œ")
        print(f"ğŸ“Š ì œê±° í›„ ë°ì´í„°: {after_removal}í–‰")
    
    print(f"âœ… ì¹´í…Œê³ ë¦¬ í•„í„°ë§ ì™„ë£Œ")
    
    # 7ë‹¨ê³„ ê²°ê³¼ ì €ì¥
    df.to_csv(TEMP_FILES['step7'], index=False, encoding='utf-8-sig')
    print(f"ğŸ’¾ 7ë‹¨ê³„ ê²°ê³¼ ì €ì¥: {TEMP_FILES['step7']}")
    
    return "ì¹´í…Œê³ ë¦¬ ì „ì²˜ë¦¬ ì™„ë£Œ"

def save_processed_data_task(**context):
    """8ë‹¨ê³„: ìµœì¢… ë°ì´í„° ì €ì¥"""
    print("ğŸ’¾ 8ë‹¨ê³„: ìµœì¢… ë°ì´í„° ì €ì¥ ì¤‘...")
    
    # ì´ì „ ë‹¨ê³„ ê²°ê³¼ ë¡œë“œ
    df = pd.read_csv(TEMP_FILES['step7'], encoding='utf-8-sig')
    print(f"ğŸ“‚ ì´ì „ ë‹¨ê³„ ë°ì´í„° ë¡œë“œ: {len(df)}í–‰")
    
    # ì»¬ëŸ¼ëª… ë³€ê²½ ë° ì¬êµ¬ì„±
    final_df = df.copy()
    
    # ë¸Œëœë“œëª… ì»¬ëŸ¼ ë³€ê²½
    final_df['í•œê¸€ë¸Œëœë“œëª…'] = final_df['ë¸Œëœë“œëª…_í•œê¸€']
    final_df['ì˜ì–´ë¸Œëœë“œëª…'] = final_df['ë¸Œëœë“œëª…']
    
    # ì†Œë¶„ë¥˜ ì»¬ëŸ¼ë§Œ ë³€ê²½ (ëŒ€ë¶„ë¥˜ëŠ” ë””ë ‰í† ë¦¬ì—ì„œ ì´ë¯¸ ì„¤ì •ë¨)
    final_df['ì†Œë¶„ë¥˜'] = final_df['ì†Œë¶„ë¥˜_ì •ë¦¬']
    
    # ì‚¬ì´íŠ¸ëª… ì»¬ëŸ¼ ì¶”ê°€
    final_df['ì‚¬ì´íŠ¸ëª…'] = 'wconcept'
    
    # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ (ìˆœì„œ ì¡°ì •)
    columns_to_save = [
        'ìƒí’ˆì½”ë“œ', 'ìƒí’ˆëª…', 'í•œê¸€ë¸Œëœë“œëª…', 'ì˜ì–´ë¸Œëœë“œëª…', 
        'ëŒ€ë¶„ë¥˜', 'ì†Œë¶„ë¥˜', 'ì›ê°€', 'í• ì¸ê°€', 
        'ì„±ë³„', 'ì´ë¯¸ì§€URL', 'ì†Œì¬', 'ìƒ‰ìƒ', 'ì¢‹ì•„ìš”ìˆ˜', 'ìƒí’ˆë§í¬', 'ì‚¬ì´íŠ¸ëª…'
    ]
    
    # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ
    existing_columns = [col for col in columns_to_save if col in final_df.columns]
    final_df = final_df[existing_columns]
    
    # ìµœì¢… ê²°ê³¼ ì €ì¥
    final_df.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig')
    
    # ì„ì‹œ íŒŒì¼ ì •ë¦¬
    cleanup_temp_files()
    
    # ìµœì¢… í†µê³„ ì¶œë ¥
    print("=" * 50)
    print("âœ… ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
    print(f"ğŸ’¾ ìµœì¢… íŒŒì¼ ì €ì¥: {OUTPUT_FILE}")
    print(f"\nğŸ“ˆ ìµœì¢… í†µê³„:")
    print(f"- ì´ ìƒí’ˆ ìˆ˜: {len(final_df):,}ê°œ")
    print(f"- ì´ ì»¬ëŸ¼ ìˆ˜: {len(final_df.columns)}ê°œ")
    print(f"- ë¸Œëœë“œ ìˆ˜: {final_df['í•œê¸€ë¸Œëœë“œëª…'].nunique():,}ê°œ")
    print(f"- ëŒ€ë¶„ë¥˜ë³„ ë¶„í¬:")
    if 'ëŒ€ë¶„ë¥˜' in final_df.columns:
        category_counts = final_df['ëŒ€ë¶„ë¥˜'].value_counts()
        for category, count in category_counts.items():
            print(f"  â€¢ {category}: {count:,}ê°œ")
    
    return f"ìµœì¢… ë°ì´í„° ì €ì¥ ì™„ë£Œ - {len(final_df):,}í–‰, {len(existing_columns)}ì»¬ëŸ¼"

def cleanup_temp_files():
    """ì„ì‹œ íŒŒì¼ ì •ë¦¬"""
    print("ğŸ—‘ï¸ ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì¤‘...")
    cleaned_count = 0
    
    for step_name, file_path in TEMP_FILES.items():
        try:
            if os.path.exists(file_path):
                os.remove(file_path)
                cleaned_count += 1
                print(f"  âœ… ì‚­ì œ: {file_path}")
        except Exception as e:
            print(f"  âš ï¸ ì‚­ì œ ì‹¤íŒ¨ {file_path}: {e}")
    
    print(f"ğŸ—‘ï¸ ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ: {cleaned_count}ê°œ íŒŒì¼ ì‚­ì œ")

# DAG íƒœìŠ¤í¬ ì •ì˜
start = DummyOperator(
    task_id='start', 
    dag=dag,
    doc_md="ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘"
)

load_data = PythonOperator(
    task_id='load_data',
    python_callable=load_data_task,
    dag=dag,
    doc_md="1ë‹¨ê³„: ì›ë³¸ ë°ì´í„° ë¡œë“œ ë° ë¸Œëœë“œ ë§¤í•‘ ì¤€ë¹„"
)

clean_product_names = PythonOperator(
    task_id='clean_product_names',
    python_callable=clean_product_names_task,
    dag=dag,
    doc_md="2ë‹¨ê³„: ìƒí’ˆëª… ì •ë¦¬ (ê´„í˜¸, íŠ¹ìˆ˜ë¬¸ì ì œê±°)"
)

extract_colors = PythonOperator(
    task_id='extract_colors',
    python_callable=extract_colors_task,
    dag=dag,
    doc_md="3ë‹¨ê³„: ìƒí’ˆëª…ì—ì„œ ìƒ‰ìƒ ì •ë³´ ì¶”ì¶œ ë° ë³´ì™„"
)

filter_missing_data = PythonOperator(
    task_id='filter_missing_data',
    python_callable=filter_missing_data_task,
    dag=dag,
    doc_md="4ë‹¨ê³„: í•„ìˆ˜ ë°ì´í„° í•„í„°ë§ (ì„±ë³„, ì›ê°€ ì—†ëŠ” ìƒí’ˆ ì œê±°)"
)

map_brands = PythonOperator(
    task_id='map_brands',
    python_callable=map_brands_task,
    dag=dag,
    doc_md="5ë‹¨ê³„: ì˜ì–´ ë¸Œëœë“œëª…ì„ í•œê¸€ ë¸Œëœë“œëª…ìœ¼ë¡œ ë§¤í•‘"
)

remove_unmapped_brands = PythonOperator(
    task_id='remove_unmapped_brands',
    python_callable=remove_unmapped_brands_task,
    dag=dag,
    doc_md="6ë‹¨ê³„: ë§¤í•‘ë˜ì§€ ì•Šì€ ë¸Œëœë“œ ë°ì´í„° ì œê±°"
)

process_categories = PythonOperator(
    task_id='process_categories',
    python_callable=process_categories_task,
    dag=dag,
    doc_md="7ë‹¨ê³„: ì¹´í…Œê³ ë¦¬ ì •ë¦¬ (ì†Œë¶„ë¥˜ í‘œì¤€í™” ë° ëŒ€ë¶„ë¥˜ ë§¤í•‘)"
)

save_processed_data = PythonOperator(
    task_id='save_processed_data',
    python_callable=save_processed_data_task,
    dag=dag,
    doc_md="8ë‹¨ê³„: ìµœì¢… ì „ì²˜ë¦¬ ë°ì´í„° ì €ì¥ ë° ì„ì‹œíŒŒì¼ ì •ë¦¬"
)

# ì»´ë°”ì¸ í”„ë¡œì„¸ì‹± DAG íŠ¸ë¦¬ê±°
trigger_combined_preprocessor = TriggerDagRunOperator(
    task_id='trigger_combined_preprocessor',
    trigger_dag_id='combined_preprocessor',
    wait_for_completion=False,
    poke_interval=10,
    allowed_states=['success'],
    failed_states=['failed'],
    dag=dag,
    doc_md="""
    ## ì»´ë°”ì¸ í”„ë¡œì„¸ì‹± DAG ìë™ ì‹¤í–‰
    
    ### ê¸°ëŠ¥
    - wconcept ì „ì²˜ë¦¬ ì™„ë£Œ í›„ ìë™ìœ¼ë¡œ ì»´ë°”ì¸ í”„ë¡œì„¸ì‹± DAG ì‹¤í–‰
    - ëª¨ë“  ì‚¬ì´íŠ¸ ë°ì´í„° ê²°í•© ë° S3 ì—…ë¡œë“œ
    
    ### ì‹¤í–‰ íë¦„
    1. wconcept ì „ì²˜ë¦¬ ì™„ë£Œ
    2. ì»´ë°”ì¸ í”„ë¡œì„¸ì‹± DAG ìë™ ì‹¤í–‰  
    3. ìµœì¢… í†µí•© CSV íŒŒì¼ ìƒì„± ë° S3 ì—…ë¡œë“œ
    """
)

end = DummyOperator(
    task_id='end', 
    dag=dag,
    doc_md="ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ"
)

# DAG ì˜ì¡´ì„± ì„¤ì •
start >> load_data >> clean_product_names >> extract_colors >> filter_missing_data >> map_brands >> remove_unmapped_brands >> process_categories >> save_processed_data >> trigger_combined_preprocessor >> end