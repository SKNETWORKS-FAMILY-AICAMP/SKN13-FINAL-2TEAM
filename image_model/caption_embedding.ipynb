{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ed95d81",
   "metadata": {},
   "source": [
    "# Ï∫°ÏÖò ÏÉùÏÑ± : gpt-4o-miniÎ•º ÌÜµÌï¥ crop Îêú Ïù¥ÎØ∏ÏßÄÏùò Ï∫°ÏÖòÏùÑ ÏÉùÏÑ±"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6eb50c",
   "metadata": {},
   "source": [
    "## top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e62993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Stage 1: Ïù¥ÎØ∏ÏßÄ ‚Üí GPT-4o-mini ÌÉúÍπÖ ‚Üí CSV Ï†ÄÏû•\n",
    "# - Í≤∞Í≥º CSV: top_caption.csv  (columns: product_id, combined_text)\n",
    "# - category/typeÏùÄ ÏõêÎ≥∏ CSV(top.csv)ÏóêÏÑú Ï°∞Ìöå\n",
    "# - top (ÏÉÅÏùò)Îßå Ï≤òÎ¶¨\n",
    "\n",
    "import os, re, io, base64, asyncio\n",
    "from typing import List, Dict, Tuple\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from dotenv import load_dotenv\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "# ================== Í≤ΩÎ°ú/ÌôòÍ≤Ω ==================\n",
    "IMAGE_DIR = \"crop_29cm\"\n",
    "PRODUCT_INFO_CSV = \"29cm/29cm_1000.csv\"\n",
    "OUT_CSV = \"29cm/top_caption.csv\"\n",
    "\n",
    "MAX_SIDE = 768\n",
    "JPEG_QUALITY = 85\n",
    "MAX_CONCURRENCY = 4\n",
    "\n",
    "load_dotenv()\n",
    "aclient = AsyncOpenAI()  # OPENAI_API_KEY ÌïÑÏöî\n",
    "\n",
    "# ================== key Î™©Î°ù ==================\n",
    "TOK_KEYS = [\n",
    "    \"color.main\",\"color.sub\",\"pattern\",\"pattern_scale\",\"material.fabric\",\n",
    "    \"sleeve.length\",\"sleeve.width\",\"sleeve.style\",\"fit\",\"neckline\",\"collar\",\n",
    "    \"closure\",\"graphic\",\"graphic_position\",\"graphic_size\",\"length.top\",\n",
    "    \"sleeve.cuff\",\"shoulder\"\n",
    "]\n",
    "\n",
    "# ================== GPT ÌîÑÎ°¨ÌîÑÌä∏ ==================\n",
    "PROMPT = \"\"\"\n",
    "You are a fashion vision tagger for tops (e.g., t-shirt, hoodie, blouse, sweater, sleeveless, athleisure).\n",
    "Analyze ONLY the top garment. Ignore accessories, pants, skirts, skin, hair, or background.\n",
    "If the attribute truly does not exist, output \"none\".\n",
    "\n",
    "Return EXACTLY 18 lowercase, comma-separated tokens as key=value pairs,\n",
    "using these keys IN THIS ORDER (keys must match exactly; no extra fields):\n",
    "\n",
    "1) color.main\n",
    "   black / white / gray / beige / cream / brown / navy / blue / green / yellow / orange / red / pink / purple / unknown\n",
    "2) color.sub\n",
    "   second-most visible (‚â•15% of garment area) else none\n",
    "3) pattern\n",
    "   solid / stripe / check / houndstooth / herringbone / dot / floral / paisley / animal / camouflage / text / scenic / logo / geometric / abstract / lace-knit / mixed / unknown\n",
    "4) pattern_scale\n",
    "   small / medium / large / none / unknown   (if pattern=solid ‚áí none)\n",
    "5) material.fabric\n",
    "   knit / denim / leather / suede / corduroy / chiffon / satin / lace / tweed / wool-blend / woven-cotton / woven-poly / other / unknown\n",
    "6) sleeve.length\n",
    "   sleeveless / short / half / three-quarter / long / unknown\n",
    "7) sleeve.width\n",
    "   slim / regular / wide / unknown\n",
    "8) sleeve.style\n",
    "   none / puff / balloon / raglan / kimono / off-shoulder / cold-shoulder / bishop / roll-up / spaghetti / tank / unknown\n",
    "9) fit\n",
    "   slim / regular / oversized / unknown\n",
    "10) neckline\n",
    "   round / v / square / halter / off-shoulder / strapless / cowl / one-shoulder / boat / unknown\n",
    "11) collar\n",
    "   none / shirt / polo / mandarin / high-neck / hood / unknown\n",
    "12) closure\n",
    "   zipper / buttons / hooks / drawstring / pullover / none / unknown\n",
    "13) graphic\n",
    "   none / logo / text / image / photo / art / abstract / all-over / unknown\n",
    "14) graphic_position\n",
    "   chest / sleeve / back / hem / multi / center / none / unknown\n",
    "15) graphic_size\n",
    "   small / medium / large / all-over / none / unknown\n",
    "16) length.top\n",
    "   cropped / regular / longline / tunic / unknown\n",
    "17) sleeve.cuff\n",
    "   plain / ribbed / elastic / buttoned / rolled / none / unknown\n",
    "18) shoulder\n",
    "   dropped / raglan / regular / padded / off-shoulder / one-shoulder / unknown\n",
    "\n",
    "---\n",
    "\n",
    "CONSISTENCY RULES\n",
    "- if pattern=solid ‚áí pattern_scale=none\n",
    "- if graphic=none ‚áí graphic_position=none and graphic_size=none\n",
    "- if graphic=all-over ‚áí pattern=solid\n",
    "- if pattern ‚â† solid and graphic ‚â† none ‚áí both can coexist (e.g., floral shirt with chest logo)\n",
    "- if sleeve.length=sleeveless and sleeve.style in {spaghetti, tank} ‚áí keep both\n",
    "- neckline and collar are independent; use none if not present\n",
    "- When unsure, output \"unknown\"\n",
    "\n",
    "---\n",
    "\n",
    "FORMAT GUARD\n",
    "- Exactly 18 tokens, lowercase, comma-separated\n",
    "- key=value for every token\n",
    "- No spaces around commas, no explanations\n",
    "- Example:\n",
    "  \"color.main=white,color.sub=none,pattern=solid,pattern_scale=none,material.fabric=knit,sleeve.length=short,sleeve.width=regular,sleeve.style=none,fit=slim,neckline=round,collar=shirt,closure=buttons,graphic=logo,graphic_position=chest,graphic_size=small,length.top=regular,sleeve.cuff=ribbed,shoulder=regular\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# ================== ÎèÑÏö∞ÎØ∏ Ìï®Ïàò ==================\n",
    "def image_to_b64(image_path: str, max_side: int = MAX_SIDE, jpeg_quality: int = JPEG_QUALITY):\n",
    "    im = Image.open(image_path).convert(\"RGB\")\n",
    "    w, h = im.size\n",
    "    scale = max(w, h) / float(max_side)\n",
    "    if scale > 1.0:\n",
    "        try: resample = Image.Resampling.BICUBIC\n",
    "        except AttributeError: resample = Image.BICUBIC\n",
    "        im = im.resize((int(w/scale), int(h/scale)), resample)\n",
    "    buf = io.BytesIO()\n",
    "    im.save(buf, format=\"JPEG\", quality=jpeg_quality, optimize=True)\n",
    "    return base64.b64encode(buf.getvalue()).decode(\"utf-8\"), \"image/jpeg\"\n",
    "\n",
    "# ÌååÏùºÎ™ÖÏóêÏÑú product_id Ï∂îÏ∂ú\n",
    "PID_FROM_NAME = re.compile(r\"^top_([^.\\\\/]+)\", re.IGNORECASE)\n",
    "def extract_product_id_from_filename(path: str) -> str:\n",
    "    base = os.path.basename(path)\n",
    "    m = PID_FROM_NAME.search(base)\n",
    "    return (m.group(1) if m else \"\").strip().lower()\n",
    "\n",
    "# Ï†úÌíà CSV Î∂àÎü¨Ïò§Í∏∞ (ÏÉÅÏùòÎßå)\n",
    "def load_top_map(csv_path: str) -> Dict[str, Tuple[str,str]]:\n",
    "    df = pd.read_csv(csv_path, dtype=str)\n",
    "    df = df.fillna(\"\").apply(lambda col: col.str.strip().str.lower())\n",
    "    df_top = df[df[\"ÎåÄÎ∂ÑÎ•ò\"] == \"ÏÉÅÏùò\"]  # topÎßå ÌïÑÌÑ∞\n",
    "    return dict(zip(df_top[\"ÏÉÅÌíàÏΩîÎìú\"], zip(df_top[\"ÎåÄÎ∂ÑÎ•ò\"], df_top[\"ÏÜåÎ∂ÑÎ•ò\"])))\n",
    "\n",
    "def map_categories_and_sub(major: str, sub: str) -> Tuple[str, str]:\n",
    "    major_map = {\"ÏÉÅÏùò\":\"top\"}\n",
    "    sub_map = {\n",
    "        \"ÌõÑÎìúÌã∞\":\"hoodie\",\"ÏÖîÏ∏†Î∏îÎùºÏö∞Ïä§\":\"shirt-blouse\",\"Í∏¥ÏÜåÎß§\":\"longsleeve\",\n",
    "        \"Î∞òÏÜåÎß§\":\"shortsleeve\",\"ÌîºÏºÄÏπ¥Îùº\":\"polo\",\"ÎãàÌä∏Ïä§Ïõ®ÌÑ∞\":\"knit-sweater\",\n",
    "        \"Ïä¨Î¶¨Î∏åÎ¶¨Ïä§\":\"sleeveless\"\n",
    "    }\n",
    "    return major_map.get(major, \"top\"), sub_map.get(sub, \"unknown\")\n",
    "\n",
    "# ================== normalize ==================\n",
    "def normalize_caption_18(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    text = \" \".join(text.splitlines()).strip().strip('\"').strip(\"'\")\n",
    "\n",
    "    # key=value Ï∂îÏ∂ú\n",
    "    raw_parts = [p.strip().lower() for p in text.split(\",\") if \"=\" in p]\n",
    "    kv_map = {}\n",
    "    for p in raw_parts:\n",
    "        if \"=\" in p:\n",
    "            k, v = p.split(\"=\", 1)\n",
    "            kv_map[k.strip()] = v.strip()\n",
    "\n",
    "    # ÎàÑÎùΩÎêú ÌÇ§ unknown Ï±ÑÏö∞Í∏∞\n",
    "    fixed = []\n",
    "    for k in TOK_KEYS:\n",
    "        val = kv_map.get(k, \"unknown\")\n",
    "        fixed.append(f\"{k}={val}\")\n",
    "\n",
    "    # Î£∞ Î≥¥Ï†ï\n",
    "    kv = {kv.split(\"=\")[0]: kv.split(\"=\")[1] for kv in fixed}\n",
    "    if kv[\"pattern\"] == \"solid\":\n",
    "        kv[\"pattern_scale\"] = \"none\"\n",
    "    if kv[\"graphic\"] == \"none\":\n",
    "        kv[\"graphic_position\"] = \"none\"\n",
    "        kv[\"graphic_size\"] = \"none\"\n",
    "\n",
    "    return \",\".join([f\"{k}={kv[k]}\" for k in TOK_KEYS])\n",
    "\n",
    "def tokens_to_combined_text(tokens_csv: str, category: str, type_: str) -> str:\n",
    "    parts = [p.strip() for p in tokens_csv.split(\",\")]\n",
    "    fixed = []\n",
    "    for i, tok in enumerate(parts):\n",
    "        if \"=\" in tok: fixed.append(tok)\n",
    "        else: fixed.append(f\"{TOK_KEYS[i]}={tok}\")\n",
    "    return f\"category={category} | type={type_} | \" + \" | \".join(fixed)\n",
    "\n",
    "# ================== GPT Ìò∏Ï∂ú ==================\n",
    "async def tag_one(image_path: str) -> Tuple[str, str]:\n",
    "    b64, mime = image_to_b64(image_path)\n",
    "    messages = [{\n",
    "        \"role\":\"user\",\n",
    "        \"content\":[\n",
    "            {\"type\":\"text\",\"text\":PROMPT},\n",
    "            {\"type\":\"image_url\",\"image_url\":{\"url\":f\"data:{mime};base64,{b64}\"}}\n",
    "        ]\n",
    "    }]\n",
    "    resp = await aclient.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        max_tokens=160,\n",
    "        temperature=0.0,\n",
    "        top_p=1.0,\n",
    "        seed=12345,\n",
    "    )\n",
    "    text = resp.choices[0].message.content.strip()\n",
    "    return image_path, normalize_caption_18(text)\n",
    "\n",
    "async def run_stage1(image_paths: List[str], pid2cat: Dict[str, Tuple[str,str]]) -> pd.DataFrame:\n",
    "    sem = asyncio.Semaphore(MAX_CONCURRENCY)\n",
    "    out = []\n",
    "    done = 0\n",
    "\n",
    "    async def worker(p):\n",
    "        async with sem:\n",
    "            try:\n",
    "                _, cap18 = await tag_one(p)\n",
    "            except Exception as e:\n",
    "                print(\"caption error:\", p, e)\n",
    "                cap18 = \",\".join([f\"{k}=unknown\" for k in TOK_KEYS])\n",
    "            pid = extract_product_id_from_filename(p)\n",
    "            major, sub = pid2cat.get(pid, (\"ÏÉÅÏùò\",\"\"))\n",
    "            cat_en, type_en = map_categories_and_sub(major, sub)\n",
    "            combined = tokens_to_combined_text(cap18, cat_en, type_en)\n",
    "            return {\"product_id\": pid, \"combined_text\": combined}\n",
    "\n",
    "    tasks = [asyncio.create_task(worker(p)) for p in image_paths]\n",
    "\n",
    "    for fut in asyncio.as_completed(tasks):\n",
    "        row = await fut\n",
    "        out.append(row)\n",
    "        done += 1\n",
    "        if done % 10 == 0 or done == len(tasks):\n",
    "            df_partial = pd.DataFrame(out).drop_duplicates(\"product_id\")\n",
    "            df_partial.to_csv(OUT_CSV, index=False, encoding=\"utf-8\")\n",
    "            print(f\"[Stage1] {done}/{len(tasks)} saved ‚Üí {OUT_CSV}\")\n",
    "\n",
    "    return pd.DataFrame(out).drop_duplicates(\"product_id\")\n",
    "\n",
    "# ================== Ïã§Ìñâ ==================\n",
    "exts = (\".jpg\",\".jpeg\",\".png\",\".webp\",\".bmp\",\".jfif\")\n",
    "image_paths = [\n",
    "    os.path.join(IMAGE_DIR,f)\n",
    "    for f in os.listdir(IMAGE_DIR)\n",
    "    if f.lower().endswith(exts) and f.lower().startswith(\"top_\")\n",
    "]\n",
    "if len(image_paths) == 0:\n",
    "    raise RuntimeError(\"Ï≤òÎ¶¨Ìï† top Ïù¥ÎØ∏ÏßÄÍ∞Ä ÏóÜÏäµÎãàÎã§.\")\n",
    "\n",
    "pid2cat = load_top_map(PRODUCT_INFO_CSV)\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "df = await run_stage1(image_paths, pid2cat)\n",
    "df.to_csv(OUT_CSV, index=False, encoding=\"utf-8\")\n",
    "print(\"‚úÖ Ï†ÄÏû• ÏôÑÎ£å:\", OUT_CSV)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb149a2",
   "metadata": {},
   "source": [
    "## pants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe6dcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Stage 1: Ïù¥ÎØ∏ÏßÄ ‚Üí GPT-4o-mini ÌÉúÍπÖ ‚Üí combined_text ÏÉùÏÑ± ‚Üí CSV Ï†ÄÏû•\n",
    "# - Í≤∞Í≥º CSV: pants_caption.csv  (columns: product_id, combined_text)\n",
    "# - category/typeÏùÄ ÏõêÎ≥∏ CSV(pants.csv)ÏóêÏÑú Ï°∞Ìöå\n",
    "# - pants (ÌïòÏùò)Îßå Ï≤òÎ¶¨\n",
    "\n",
    "import os, re, io, base64, asyncio\n",
    "from typing import List, Dict, Tuple\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from dotenv import load_dotenv\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "# ================== Í≤ΩÎ°ú/ÌôòÍ≤Ω ==================\n",
    "IMAGE_DIR = \"crop_0.5\"          # crop Ïù¥ÎØ∏ÏßÄ Ìè¥Îçî (ÌååÏùºÎ™Ö: pants_ÏÉÅÌíàÏΩîÎìú.ext / jeans_ / shorts_ Îì±)\n",
    "PRODUCT_INFO_CSV = \"29cm_1000.csv\"  # [ÏÉÅÌíàÏΩîÎìú, ÎåÄÎ∂ÑÎ•ò, ÏÜåÎ∂ÑÎ•ò]\n",
    "OUT_CSV = \"pants_caption.csv\"   # ÏµúÏ¢Ö ÏÇ∞Ï∂úÎ¨º\n",
    "\n",
    "MAX_SIDE = 768\n",
    "JPEG_QUALITY = 85\n",
    "MAX_CONCURRENCY = 4\n",
    "\n",
    "load_dotenv()\n",
    "aclient = AsyncOpenAI()  # OPENAI_API_KEY ÌïÑÏöî\n",
    "\n",
    "# ================== ÌÜ†ÌÅ∞ ÌÇ§ (PANTS 16Í∞ú) ==================\n",
    "TOK_KEYS = [\n",
    "    \"fit\",\"rise\",\"waistband\",\"closure\",\"cuffs\",\"front.structure\",\n",
    "    \"pockets.style\",\"pockets.secure\",\"material.fabric\",\"denim.wash\",\n",
    "    \"pattern\",\"pattern_scale\",\"color.main\",\"color.sub\",\"leg.length\",\"hem.opening\"\n",
    "]\n",
    "\n",
    "# ================== ÌóàÏö©Í∞í/Î≥ÑÏπ≠ ==================\n",
    "ALLOWED_PATTERN = {\n",
    "    \"solid\",\"stripe\",\"check\",\"houndstooth\",\"herringbone\",\"dot\",\"floral\",\"paisley\",\n",
    "    \"animal\",\"camouflage\",\"text\",\"scenic\",\"logo\",\"geometric\",\"abstract\",\"lace-knit\",\"mixed\",\"unknown\"\n",
    "}\n",
    "PATTERN_ALIASES = {\n",
    "    \"newspaper\":\"text\",\"typographic\":\"text\",\"letters\":\"text\",\"letter\":\"text\",\"textual\":\"text\",\"script\":\"text\",\"font\":\"text\",\n",
    "    \"city\":\"scenic\",\"building\":\"scenic\",\"buildings\":\"scenic\",\"map\":\"scenic\",\"landmark\":\"scenic\",\"architecture\":\"scenic\",\n",
    "    \"chevron\":\"herringbone\",\n",
    "    \"animal print\":\"animal\",\"leopard\":\"animal\",\"zebra\":\"animal\",\"snake\":\"animal\",\"giraffe\":\"animal\",\"cow\":\"animal\",\"tiger\":\"animal\",\n",
    "    \"camo\":\"camouflage\",\"military\":\"camouflage\",\n",
    "    \"monogram\":\"logo\"\n",
    "}\n",
    "ALLOWED_PATSCALE = {\"small\",\"medium\",\"large\",\"none\",\"unknown\"}\n",
    "ALLOWED_COLOR = {\"black\",\"white\",\"gray\",\"beige\",\"cream\",\"brown\",\"navy\",\"blue\",\"green\",\"yellow\",\"orange\",\"red\",\"pink\",\"purple\",\"unknown\"}\n",
    "\n",
    "ALLOWED_FIT = {\"skinny\",\"slim\",\"straight\",\"tapered\",\"wide\",\"relaxed\",\"bootcut\",\"flared\",\"loose\",\"unknown\"}\n",
    "ALLOWED_RISE = {\"low\",\"mid\",\"high\",\"unknown\"}\n",
    "ALLOWED_WAISTBAND = {\"fixed\",\"elastic\",\"drawstring\",\"elastic+drawstring\",\"unknown\"}\n",
    "ALLOWED_CLOSURE = {\"zipper\",\"buttons\",\"drawstring\",\"none\",\"unknown\"}\n",
    "ALLOWED_CUFFS = {\"none\",\"elastic\",\"rib\",\"rolled\",\"raw\",\"zipped\",\"unknown\"}\n",
    "ALLOWED_FRONT = {\"flat-front\",\"pleated-single\",\"pleated-double\",\"darts-only\",\"unknown\"}\n",
    "ALLOWED_POCKET_STYLE = {\"5-pocket\",\"slant\",\"welt\",\"patch\",\"cargo\",\"zip\",\"none\",\"unknown\"}\n",
    "ALLOWED_POCKET_SECURE = {\"none\",\"button\",\"zip\",\"flap\",\"mixed\",\"unknown\"}\n",
    "ALLOWED_FABRIC = {\"denim\",\"jersey\",\"fleece\",\"woven-cotton\",\"twill\",\"corduroy\",\"woven-poly\",\"nylon\",\"ripstop\",\"wool-blend\",\"leather\",\"satin\",\"other\",\"unknown\"}\n",
    "ALLOWED_DENIM_WASH = {\"raw\",\"dark-wash\",\"mid-wash\",\"light-wash\",\"acid-wash\",\"stone-wash\",\"bleach\",\"coated\",\"colored\",\"whiskered-faded\",\"distressed\",\"clean\",\"none\",\"unknown\"}\n",
    "ALLOWED_LENGTH = {\"shorts\",\"cropped\",\"ankle\",\"full\",\"unknown\"}\n",
    "ALLOWED_HEMOPEN = {\"narrow\",\"regular\",\"wide\",\"unknown\"}\n",
    "\n",
    "def _normalize_pattern_value(p: str) -> str:\n",
    "    p = (p or \"\").strip().lower()\n",
    "    p = PATTERN_ALIASES.get(p, p)\n",
    "    return p if p in ALLOWED_PATTERN else \"unknown\"\n",
    "\n",
    "def _nz(s: str) -> str:\n",
    "    return (s or \"\").strip().lower()\n",
    "\n",
    "# ================== GPT ÌîÑÎ°¨ÌîÑÌä∏ (Î∞îÏßÄ 16ÌÜ†ÌÅ∞) ==================\n",
    "PROMPT = \"\"\"\n",
    "You are a fashion vision tagger for fashion product retrieval.\n",
    "Analyze ONLY the pants region even if other items/body parts are visible.\n",
    "Never infer hidden details; if not clearly visible, output \"unknown\".\n",
    "If not applicable, output \"none\".\n",
    "Ignore tops, footwear, accessories, or background items. Only describe the pants themselves.\n",
    "IMPORTANT: Do NOT output product category (e.g., denim/jogger/slacks/‚Ä¶). These are provided externally.\n",
    "\n",
    "OUTPUT\n",
    "Return ONE line with EXACTLY 16 lowercase, comma-separated tokens as key=value pairs,\n",
    "using these keys IN THIS ORDER (keys must match exactly; no extra fields):\n",
    "\n",
    "1) fit\n",
    "   skinny / slim / straight / tapered / wide / relaxed / bootcut / flared / loose / unknown\n",
    "2) rise\n",
    "   low / mid / high / unknown\n",
    "3) waistband\n",
    "   fixed / elastic / drawstring / elastic+drawstring / unknown\n",
    "4) closure\n",
    "   zipper / buttons / drawstring / none / unknown\n",
    "5) cuffs\n",
    "   none / elastic / rib / rolled / raw / zipped / unknown\n",
    "6) front.structure\n",
    "   flat-front / pleated-single / pleated-double / darts-only / unknown\n",
    "7) pockets.style\n",
    "   5-pocket / slant / welt / patch / cargo / zip / none / unknown\n",
    "8) pockets.secure\n",
    "   none / button / zip / flap / mixed / unknown\n",
    "9) material.fabric\n",
    "   denim / jersey / fleece / woven-cotton / twill / corduroy / woven-poly / nylon / ripstop / wool-blend / leather / satin / other / unknown\n",
    "10) denim.wash\n",
    "   raw / dark-wash / mid-wash / light-wash / acid-wash / stone-wash / bleach / coated / colored / whiskered-faded / distressed / clean / none / unknown\n",
    "   (if material.fabric ‚â† denim ‚áí set to \"none\")\n",
    "11) pattern\n",
    "   solid / stripe / check / houndstooth / herringbone / dot / floral / paisley / animal / camouflage / text / scenic / logo / geometric / abstract / lace-knit / mixed / unknown\n",
    "12) pattern_scale\n",
    "   small / medium / large / none / unknown (if pattern=solid ‚áí none)\n",
    "13) color.main\n",
    "   black / white / gray / beige / cream / brown / navy / blue / green / yellow / orange / red / pink / purple / unknown\n",
    "14) color.sub\n",
    "   second-most color on the pants (‚â•15% of pant area) else none\n",
    "15) leg.length\n",
    "   shorts / cropped / ankle / full / unknown\n",
    "16) hem.opening\n",
    "   narrow / regular / wide / unknown\n",
    "\n",
    "---\n",
    "\n",
    "SELECTION GUIDELINES\n",
    "- fit: overall leg silhouette (tapered narrows to hem; straight stays constant; relaxed/loose is roomy; bootcut/flared widens from knee).\n",
    "- rise: relative to natural waist (low <, high >; else mid). If covered by tops ‚áí unknown.\n",
    "- waistband vs closure:\n",
    "  elastic / elastic+drawstring ‚áí gathered stretch; drawstring shows a visible cord.\n",
    "  fixed waist ‚áí usually zipper or buttons; if only a cord is visible ‚áí closure=drawstring.\n",
    "- cuffs: rib/elastic bands ‚áí jogger-like; rolled = turn-ups; raw = cut-off fray; zipped = hem zippers.\n",
    "- front.structure: flat-front (no pleats), pleated-single/double, darts-only (no visible pleats).\n",
    "- pockets.style: 5-pocket (jeans layout), slant (chino/slacks), welt (back slit with welts), patch (sewn-on), cargo (large thigh), zip (visible zipper pockets).\n",
    "  If multiple are equally present, prefer cargo > 5-pocket > slant > welt > patch > zip.\n",
    "- material.fabric: pick the dominant cloth; coated denim is still denim (mark coating under denim.wash).\n",
    "- denim.wash (DENIM ONLY): raw, dark-/mid-/light-wash; acid-/stone-/bleach; coated/colored; whiskered-faded; distressed; clean. If not denim ‚áí none.\n",
    "- pattern vs wash: whiskers/fades/distressing are denim.wash, NOT pattern. pattern is prints/weaves (check, camo, stripe, etc.).\n",
    "- pattern_scale: small < ~1/20; medium ~1/20‚Äì1/6; large > ~1/6 of visible leg height.\n",
    "- colors: measure on pant area only (ignore belt/background/shoes).\n",
    "- leg.length: shorts above knee; cropped noticeably above ankle; ankle around ankle bone; full covers ankle.\n",
    "- hem.opening: the openness at hem relative to knee/thigh. flared/bootcut ‚áí typically wide; skinny/tapered ‚áí typically narrow.\n",
    "- A small single brand logo patch/embroidery does not count as a pattern; keep pattern=solid unless logos repeat across the fabric.\n",
    "\n",
    "---\n",
    "\n",
    "CONSISTENCY RULES\n",
    "- waistband ‚àà {elastic, elastic+drawstring} without a fixed band ‚áí closure should be drawstring or none (avoid zipper/buttons unless clearly visible).\n",
    "- if pattern=solid ‚áí pattern_scale=none (enforce).\n",
    "- if material.fabric ‚â† denim ‚áí denim.wash=none (enforce).\n",
    "- fit vs hem.opening: if fit ‚àà {flared, bootcut} and hem.opening is unknown ‚áí set hem.opening=wide. If fit ‚àà {skinny, slim, tapered} and hem.opening is unknown ‚áí set hem.opening=narrow.\n",
    "- cuffs ‚àà {elastic, rib, zipped} with unknown hem.opening ‚áí prefer hem.opening=narrow.\n",
    "- when attributes conflict and you cannot resolve, set the conflicting token(s) to \"unknown\".\n",
    "\n",
    "---\n",
    "\n",
    "PATTERN DISAMBIGUATION PRIORITY\n",
    "animal > (stripe | check) > camouflage > logo > text > scenic\n",
    "> (houndstooth | herringbone) > (floral | paisley) > (geometric | abstract) > lace-knit > solid.\n",
    "\n",
    "---\n",
    "\n",
    "FORMAT RULES\n",
    "- Exactly 16 tokens, lowercase, comma-separated\n",
    "- If <70% certain ‚áí unknown; if truly absent ‚áí none.\n",
    "- Each token must be key=value\n",
    "- No extra words, no explanations\n",
    "- Example valid output:\n",
    "  \"fit=slim,rise=mid,waistband=fixed,closure=zipper,cuffs=none,front.structure=flat-front,pockets.style=5-pocket,pockets.secure=none,material.fabric=denim,denim.wash=mid-wash,pattern=solid,pattern_scale=none,color.main=blue,color.sub=none,leg.length=full,hem.opening=regular\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# ================== ÎèÑÏö∞ÎØ∏ Ìï®Ïàò ==================\n",
    "def image_to_b64(image_path: str, max_side: int = MAX_SIDE, jpeg_quality: int = JPEG_QUALITY):\n",
    "    im = Image.open(image_path).convert(\"RGB\")\n",
    "    w, h = im.size\n",
    "    scale = max(w, h) / float(max_side)\n",
    "    if scale > 1.0:\n",
    "        try: resample = Image.Resampling.BICUBIC\n",
    "        except AttributeError: resample = Image.BICUBIC\n",
    "        im = im.resize((int(w/scale), int(h/scale)), resample)\n",
    "    buf = io.BytesIO()\n",
    "    im.save(buf, format=\"JPEG\", quality=jpeg_quality, optimize=True)\n",
    "    return base64.b64encode(buf.getvalue()).decode(\"utf-8\"), \"image/jpeg\"\n",
    "\n",
    "# ÌååÏùºÎ™ÖÏóêÏÑú product_id Ï∂îÏ∂ú ‚Äî pants/jeans/shorts Ï†ëÎëê ÌóàÏö©\n",
    "PID_FROM_NAME = re.compile(r\"^(?:pants|jeans|shorts)_([^.\\\\/]+)\", re.IGNORECASE)\n",
    "def extract_product_id_from_filename(path: str) -> str:\n",
    "    base = os.path.basename(path)\n",
    "    m = PID_FROM_NAME.search(base)\n",
    "    return (m.group(1) if m else \"\").strip().lower()\n",
    "\n",
    "# Ï†úÌíà CSV Î∂àÎü¨Ïò§Í∏∞ (ÌïòÏùòÎßå)\n",
    "def load_pants_map(csv_path: str) -> Dict[str, Tuple[str,str]]:\n",
    "    df = pd.read_csv(csv_path, dtype=str)\n",
    "    df = df.fillna(\"\").apply(lambda col: col.str.strip().str.lower())\n",
    "    # ÎåÄÎ∂ÑÎ•òÍ∞Ä 'ÌïòÏùò' ÎòêÎäî 'Î∞îÏßÄ'Ïù∏ Í≤ÉÎßå ÏÇ¨Ïö©\n",
    "    df_pants = df[df[\"ÎåÄÎ∂ÑÎ•ò\"].isin([\"ÌïòÏùò\",\"Î∞îÏßÄ\"])]\n",
    "    return dict(zip(df_pants[\"ÏÉÅÌíàÏΩîÎìú\"], zip(df_pants[\"ÎåÄÎ∂ÑÎ•ò\"], df_pants[\"ÏÜåÎ∂ÑÎ•ò\"])))\n",
    "\n",
    "# Ïπ¥ÌÖåÍ≥†Î¶¨/ÌÉÄÏûÖ Îß§Ìïë (ko ‚Üí en)\n",
    "def map_categories_and_type(major: str, sub: str) -> Tuple[str, str]:\n",
    "    def norm(x: str) -> str:\n",
    "        return (x or \"\").replace(\" \", \"\").replace(\"/\", \"\").replace(\"-\", \"\").strip().lower()\n",
    "    major_map = {\"ÌïòÏùò\":\"pants\",\"Î∞îÏßÄ\":\"pants\"}\n",
    "    sub_map = {\n",
    "        \"Îç∞ÎãòÌå¨Ï∏†\":\"denim-pants\",\n",
    "        \"Ìä∏Î†àÏù¥ÎãùÏ°∞Í±∞Ìå¨Ï∏†\":\"jogger-pants\",\n",
    "        \"ÏΩîÌäºÌå¨Ï∏†\":\"cotton-pants\",\n",
    "        \"ÏäàÌä∏Ìå¨Ï∏†Ïä¨ÎûôÏä§\":\"slacks\",\n",
    "        \"ÏäàÌä∏Ïä¨ÎûôÏä§\":\"slacks\",\n",
    "        \"ÏàèÌå¨Ï∏†\":\"short-pants\",\n",
    "        \"Î†àÍπÖÏä§\":\"leggings\",\n",
    "        \"Ïπ¥Í≥†Ìå¨Ï∏†\":\"cargo-pants\",\n",
    "    }\n",
    "    major_en = major_map.get(norm(major), \"pants\")\n",
    "    type_en  = sub_map.get(norm(sub), \"unknown\")\n",
    "    return major_en, type_en\n",
    "\n",
    "# Ï∫°ÏÖò Ï†ïÍ∑úÌôî (16ÌÜ†ÌÅ∞ Í∞íÎßå Ï†ïÍ∑úÌôî; key=Í∞Ä Ìè¨Ìï®ÎèºÎèÑ ÏïàÏ†Ñ)\n",
    "def normalize_caption_pants16(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    text = \" \".join(text.splitlines()).strip().strip('\"').strip(\"'\")\n",
    "    parts = [p.strip().lower() for p in text.split(\",\") if p]\n",
    "\n",
    "    # key=valueÍ∞Ä Îì§Ïñ¥ÏôîÏúºÎ©¥ Í∞íÎßå Ï∂îÏ∂ú\n",
    "    vals = []\n",
    "    for i, p in enumerate(parts):\n",
    "        if \"=\" in p:\n",
    "            k, v = p.split(\"=\", 1)\n",
    "            vals.append(v.strip())\n",
    "        else:\n",
    "            vals.append(p)\n",
    "\n",
    "    # Ìå®Îî©/Ìä∏Î¶º\n",
    "    if len(vals) < 16:\n",
    "        vals += [\"unknown\"] * (16 - len(vals))\n",
    "    elif len(vals) > 16:\n",
    "        vals = vals[:16]\n",
    "\n",
    "    # Ïù∏Îç±Ïä§\n",
    "    i_fit, i_rise, i_waist, i_closure, i_cuffs, i_front = 0,1,2,3,4,5\n",
    "    i_pstyle, i_psecure = 6,7\n",
    "    i_fabric, i_wash = 8,9\n",
    "    i_pat, i_pscale = 10,11\n",
    "    i_cmain, i_csub = 12,13\n",
    "    i_len, i_hem = 14,15\n",
    "\n",
    "    def _in(x, allowed): return x if x in allowed else \"unknown\"\n",
    "\n",
    "    vals[i_fit]     = _in(vals[i_fit], ALLOWED_FIT)\n",
    "    vals[i_rise]    = _in(vals[i_rise], ALLOWED_RISE)\n",
    "    vals[i_waist]   = _in(vals[i_waist], ALLOWED_WAISTBAND)\n",
    "    vals[i_closure] = _in(vals[i_closure], ALLOWED_CLOSURE)\n",
    "    vals[i_cuffs]   = _in(vals[i_cuffs], ALLOWED_CUFFS)\n",
    "    vals[i_front]   = _in(vals[i_front], ALLOWED_FRONT)\n",
    "    vals[i_pstyle]  = _in(vals[i_pstyle], ALLOWED_POCKET_STYLE)\n",
    "    vals[i_psecure] = _in(vals[i_psecure], ALLOWED_POCKET_SECURE)\n",
    "    vals[i_fabric]  = _in(vals[i_fabric], ALLOWED_FABRIC)\n",
    "\n",
    "    # denim.wash\n",
    "    wash = _nz(vals[i_wash])\n",
    "    if vals[i_fabric] != \"denim\":\n",
    "        wash = \"none\"\n",
    "    elif wash not in ALLOWED_DENIM_WASH:\n",
    "        wash = \"unknown\"\n",
    "    vals[i_wash] = wash\n",
    "\n",
    "    # Ìå®ÌÑ¥/Ïä§ÏºÄÏùº\n",
    "    pattern = _normalize_pattern_value(vals[i_pat])\n",
    "    vals[i_pat] = pattern\n",
    "    pscale = _nz(vals[i_pscale])\n",
    "    if pattern == \"solid\":\n",
    "        pscale = \"none\"\n",
    "    elif pscale not in ALLOWED_PATSCALE:\n",
    "        pscale = \"unknown\"\n",
    "    vals[i_pscale] = pscale\n",
    "\n",
    "    # ÏÉâÏÉÅ\n",
    "    vals[i_cmain] = _in(vals[i_cmain], ALLOWED_COLOR)\n",
    "    csub = _nz(vals[i_csub])\n",
    "    vals[i_csub] = csub if (csub in ALLOWED_COLOR or csub == \"none\") else \"unknown\"\n",
    "\n",
    "    # Í∏∏Ïù¥/Î∞ëÎã®\n",
    "    vals[i_len] = _in(vals[i_len], ALLOWED_LENGTH)\n",
    "    vals[i_hem] = _in(vals[i_hem], ALLOWED_HEMOPEN)\n",
    "\n",
    "    # ÏùºÍ¥ÄÏÑ± Î≥¥Ï†ï\n",
    "    if vals[i_waist] in {\"elastic\",\"elastic+drawstring\"} and vals[i_closure] in {\"zipper\",\"buttons\"}:\n",
    "        vals[i_closure] = \"unknown\"\n",
    "    if vals[i_hem] == \"unknown\":\n",
    "        if vals[i_fit] in {\"flared\",\"bootcut\"}: vals[i_hem] = \"wide\"\n",
    "        elif vals[i_fit] in {\"skinny\",\"slim\",\"tapered\"}: vals[i_hem] = \"narrow\"\n",
    "    if vals[i_hem] == \"unknown\" and vals[i_cuffs] in {\"elastic\",\"rib\",\"zipped\"}:\n",
    "        vals[i_hem] = \"narrow\"\n",
    "\n",
    "    return \",\".join(vals)\n",
    "\n",
    "\n",
    "# ================== GPT Ìò∏Ï∂ú ==================\n",
    "async def tag_one(image_path: str) -> Tuple[str, str]:\n",
    "    b64, mime = image_to_b64(image_path)\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": PROMPT},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:{mime};base64,{b64}\"}}\n",
    "        ]\n",
    "    }]\n",
    "    resp = await aclient.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        max_tokens=160,\n",
    "        temperature=0.0,\n",
    "        top_p=1.0,\n",
    "        seed=12345,\n",
    "    )\n",
    "    text = resp.choices[0].message.content.strip()\n",
    "    return image_path, normalize_caption_pants16(text)\n",
    "\n",
    "async def run_stage1(image_paths: List[str], pid2cat: Dict[str, Tuple[str,str]]) -> pd.DataFrame:\n",
    "    sem = asyncio.Semaphore(MAX_CONCURRENCY)\n",
    "    out = []\n",
    "    done = 0\n",
    "\n",
    "    async def worker(p):\n",
    "        async with sem:\n",
    "            try:\n",
    "                _, cap16 = await tag_one(p)\n",
    "            except Exception as e:\n",
    "                print(\"caption error:\", p, e)\n",
    "                cap16 = \",\".join([f\"{k}=unknown\" for k in TOK_KEYS])\n",
    "            pid = extract_product_id_from_filename(p)\n",
    "            major, sub = pid2cat.get(pid, (\"ÌïòÏùò\", \"\"))  # pantsÎßå\n",
    "            cat_en, type_en = map_categories_and_type(major, sub)\n",
    "            combined = tokens_to_combined_text(cap16, cat_en, type_en)\n",
    "            return {\"product_id\": pid, \"combined_text\": combined}\n",
    "\n",
    "    tasks = [asyncio.create_task(worker(p)) for p in image_paths]\n",
    "\n",
    "    for fut in asyncio.as_completed(tasks):\n",
    "        row = await fut\n",
    "        out.append(row)\n",
    "        done += 1\n",
    "\n",
    "        # üîπ 10Í∞úÎßàÎã§ Ï§ëÍ∞Ñ Ï†ÄÏû• (ÎçÆÏñ¥Ïì∞Í∏∞)\n",
    "        if done % 10 == 0 or done == len(tasks):\n",
    "            df_partial = pd.DataFrame(out).drop_duplicates(subset=[\"product_id\"], keep=\"first\")\n",
    "            df_partial.to_csv(OUT_CSV, index=False, encoding=\"utf-8\")\n",
    "            print(f\"[Stage1] {done}/{len(tasks)} saved ‚Üí {OUT_CSV}\")\n",
    "\n",
    "    # ÎßàÏßÄÎßâÏóê ÏµúÏ¢Ö DataFrame Î∞òÌôò\n",
    "    return pd.DataFrame(out).drop_duplicates(subset=[\"product_id\"], keep=\"first\")\n",
    "\n",
    "# ================== Ïã§Ìñâ ==================\n",
    "# pants_* / jeans_* / shorts_* Ïù¥ÎØ∏ÏßÄÎßå Ï∂îÏ∂ú\n",
    "exts = (\".jpg\",\".jpeg\",\".png\",\".webp\",\".bmp\",\".jfif\")\n",
    "image_paths = [\n",
    "    os.path.join(IMAGE_DIR, f)\n",
    "    for f in os.listdir(IMAGE_DIR)\n",
    "    if f.lower().endswith(exts) and (f.lower().startswith(\"pants_\") or f.lower().startswith(\"jeans_\") or f.lower().startswith(\"shorts_\"))\n",
    "]\n",
    "if len(image_paths) == 0:\n",
    "    raise RuntimeError(\"Ï≤òÎ¶¨Ìï† pants/jeans/shorts Ïù¥ÎØ∏ÏßÄÍ∞Ä ÏóÜÏäµÎãàÎã§.\")\n",
    "\n",
    "pid2cat = load_pants_map(PRODUCT_INFO_CSV)\n",
    "\n",
    "# üîπ JupyterÏóêÏÑúÎäî asyncio.run() ÎåÄÏã† await ÏÇ¨Ïö©\n",
    "import nest_asyncio, asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "df = await run_stage1(image_paths, pid2cat)   # ‚úÖ Ïó¨Í∏∞ÏÑú awaitÎ°ú Ïã§Ìñâ\n",
    "df.to_csv(OUT_CSV, index=False, encoding=\"utf-8\")\n",
    "print(\"‚úÖ Ï†ÄÏû• ÏôÑÎ£å:\", OUT_CSV)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d3bd76",
   "metadata": {},
   "source": [
    "## skirt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf60f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Stage 1: Ïù¥ÎØ∏ÏßÄ ‚Üí GPT-4o-mini ÌÉúÍπÖ ‚Üí combined_text ÏÉùÏÑ± ‚Üí CSV Ï†ÄÏû•\n",
    "# - Í≤∞Í≥º CSV: skirt_caption.csv  (columns: product_id, combined_text)\n",
    "# - category/typeÏùÄ ÏõêÎ≥∏ CSV(skirt.csv)ÏóêÏÑú Ï°∞Ìöå\n",
    "# - skirt (Ïä§Ïª§Ìä∏)Îßå Ï≤òÎ¶¨\n",
    "\n",
    "import os, re, io, base64, asyncio\n",
    "from typing import List, Dict, Tuple\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from dotenv import load_dotenv\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "# ================== Í≤ΩÎ°ú/ÌôòÍ≤Ω ==================\n",
    "IMAGE_DIR = \"crop_29cm\"\n",
    "PRODUCT_INFO_CSV = \"29cm/29cm_1000.csv\"\n",
    "OUT_CSV = \"29cm/skirt_caption.csv\"\n",
    "\n",
    "MAX_SIDE = 768\n",
    "JPEG_QUALITY = 85\n",
    "MAX_CONCURRENCY = 4\n",
    "\n",
    "load_dotenv()\n",
    "aclient = AsyncOpenAI()  # OPENAI_API_KEY ÌïÑÏöî\n",
    "\n",
    "# ================== ÌÜ†ÌÅ∞ ÌÇ§ (15Í∞ú) ==================\n",
    "TOK_KEYS = [\n",
    "    \"color.main\",\"color.sub\",\"color.tone\",\"pattern\",\"pattern_scale\",\n",
    "    \"material.fabric\",\"silhouette\",\"pleated\",\"flare_level\",\"wrap\",\n",
    "    \"closure\",\"details.pockets\",\"details.slit\",\"details.hem_finish\",\n",
    "    \"style\"\n",
    "]\n",
    "\n",
    "# ================== GPT ÌîÑÎ°¨ÌîÑÌä∏ ==================\n",
    "PROMPT = \"\"\"\n",
    "You are a vision tagger for fashion product retrieval.\n",
    "Analyze ONLY the skirt region even if other items/body parts are visible.\n",
    "Never infer hidden details.  \n",
    "If an attribute is not clearly visible:\n",
    "- For closure, details.pockets, details.slit: output \"none\"\n",
    "- For all other attributes: output \"unknown\"\n",
    "\n",
    "If the attribute truly does not exist, output \"none\".  \n",
    "Ignore hands, legs, accessories, or background items. Only describe the skirt itself.\n",
    "\n",
    "Return ONE line with EXACTLY 15 lowercase, comma-separated tokens as key=value pairs,\n",
    "using these keys IN THIS ORDER (keys must match exactly; no extra fields):\n",
    "\n",
    "1) color.main\n",
    "   black / white / gray / beige / cream / brown / navy / blue / green / yellow / orange / red / pink / purple / unknown\n",
    "2) color.sub\n",
    "   second-most visible (‚â•15% of skirt area) else none\n",
    "3) color.tone\n",
    "   light / mid / dark / unknown\n",
    "4) pattern\n",
    "   solid / stripe / check / houndstooth / herringbone / dot / floral / paisley / animal / camouflage / text / scenic / logo / geometric / abstract / lace-knit / mixed / unknown\n",
    "5) pattern_scale\n",
    "   small / medium / large / none / unknown (if pattern=solid ‚áí none)\n",
    "6) material.fabric\n",
    "   knit / denim / leather / suede / corduroy / chiffon / satin / lace / tweed / wool-blend / woven-cotton / woven-poly / tulle / other / unknown\n",
    "7) silhouette\n",
    "   a-line / h-line / pencil / trumpet / mermaid / pleated / tulip / bubble / asymmetric / wrap / layered / gypsy / tiered / prairie / flounced / bias / draped / peplum / pant-skirt / slit / sarong / other / unknown\n",
    "8) pleated\n",
    "   yes / no / unknown\n",
    "9) flare_level\n",
    "   low / medium / high / none / unknown\n",
    "10) wrap\n",
    "   yes / no / unknown\n",
    "11) closure\n",
    "   zipper / buttons / hooks / drawstring / none / unknown\n",
    "12) details.pockets\n",
    "   cargo / welt / patch / none / unknown\n",
    "13) details.slit\n",
    "   front / side / back / none / unknown\n",
    "14) details.hem_finish\n",
    "   cutoff / clean / uneven / rolled / raw / unknown\n",
    "15) style\n",
    "   casual / formal / office / evening / street / school / sporty / unknown\n",
    "\n",
    "---\n",
    "\n",
    "CONSISTENCY RULES\n",
    "- if pattern=solid ‚áí pattern_scale=none\n",
    "- if silhouette=pleated ‚áí pleated=yes\n",
    "- if pleated=yes but silhouette‚â†pleated ‚áí silhouette must not be \"unknown\"\n",
    "- if wrap=yes ‚áí closure=none\n",
    "\n",
    "---\n",
    "\n",
    "FORMAT RULES\n",
    "- Exactly 15 tokens, lowercase, comma-separated\n",
    "- key=value for every token\n",
    "- No extra words, no explanations\n",
    "- Example valid output:\n",
    "  \"color.main=black,color.sub=none,color.tone=dark,pattern=solid,pattern_scale=none,material.fabric=chiffon,silhouette=pleated,pleated=yes,flare_level=high,wrap=no,closure=zipper,details.pockets=none,details.slit=side,details.hem_finish=clean,style=casual\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# ================== normalize (A ÏΩîÎìú Í∑∏ÎåÄÎ°ú Ïú†ÏßÄ) ==================\n",
    "def normalize_caption_15(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    text = \" \".join(text.splitlines()).strip().strip('\"').strip(\"'\")\n",
    "    parts = [p.strip().lower() for p in text.split(\",\") if p]\n",
    "    if len(parts) < 15:\n",
    "        parts += [\"unknown\"] * (15 - len(parts))\n",
    "    elif len(parts) > 15:\n",
    "        parts = parts[:15]\n",
    "\n",
    "    i = {name: idx for idx, name in enumerate(TOK_KEYS)}\n",
    "\n",
    "    if parts[i[\"pattern\"]] == \"solid\":\n",
    "        parts[i[\"pattern_scale\"]] = \"none\"\n",
    "    if parts[i[\"pleated\"]] not in (\"yes\",\"no\",\"unknown\"):\n",
    "        parts[i[\"pleated\"]] = \"unknown\"\n",
    "    if parts[i[\"wrap\"]] not in (\"yes\",\"no\",\"unknown\"):\n",
    "        parts[i[\"wrap\"]] = \"unknown\"\n",
    "    if parts[i[\"flare_level\"]] not in (\"low\",\"medium\",\"high\",\"none\",\"unknown\"):\n",
    "        parts[i[\"flare_level\"]] = \"unknown\"\n",
    "    if parts[i[\"details.slit\"]] not in (\"front\",\"side\",\"back\",\"none\",\"unknown\"):\n",
    "        parts[i[\"details.slit\"]] = \"unknown\"\n",
    "    if parts[i[\"details.hem_finish\"]] not in (\"cutoff\",\"clean\",\"uneven\",\"rolled\",\"raw\",\"unknown\"):\n",
    "        parts[i[\"details.hem_finish\"]] = \"unknown\"\n",
    "    if parts[i[\"silhouette\"]] == \"pleated\":\n",
    "        parts[i[\"pleated\"]] = \"yes\"\n",
    "    if parts[i[\"wrap\"]] == \"yes\":\n",
    "        parts[i[\"closure\"]] = \"none\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return \",\".join(parts)\n",
    "\n",
    "def tokens_to_combined_text(tokens_csv: str, category: str, type_: str) -> str:\n",
    "    parts = [p.strip() for p in tokens_csv.split(\",\")]\n",
    "    fixed = []\n",
    "    for i, tok in enumerate(parts):\n",
    "        if \"=\" in tok:\n",
    "            fixed.append(tok)\n",
    "        else:\n",
    "            fixed.append(f\"{TOK_KEYS[i]}={tok}\")\n",
    "    return f\"category={category} | type={type_} | \" + \" | \".join(fixed)\n",
    "\n",
    "# ================== Ï†úÌíà CSV Îß§Ìïë ==================\n",
    "def load_category_map_fixed(csv_path: str) -> Dict[str, Tuple[str, str]]:\n",
    "    df = pd.read_csv(csv_path, dtype=str).fillna(\"\").apply(lambda col: col.str.strip().str.lower())\n",
    "    df_skirt = df[df[\"ÎåÄÎ∂ÑÎ•ò\"] == \"Ïä§Ïª§Ìä∏\"]\n",
    "    return dict(zip(df_skirt[\"ÏÉÅÌíàÏΩîÎìú\"], zip(df_skirt[\"ÎåÄÎ∂ÑÎ•ò\"], df_skirt[\"ÏÜåÎ∂ÑÎ•ò\"])))\n",
    "\n",
    "PID_RE = re.compile(r\"^skirt_([^.\\\\/]+)\", re.IGNORECASE)\n",
    "def extract_product_id_from_filename(path: str) -> str:\n",
    "    base = os.path.basename(path)\n",
    "    m = PID_RE.search(base)\n",
    "    return (m.group(1) if m else \"\").strip().lower()\n",
    "\n",
    "def map_categories_and_sub(major: str, sub: str) -> Tuple[str, str]:\n",
    "    major_map = {\"Ïä§Ïª§Ìä∏\": \"skirt\"}\n",
    "    sub_map = {\"ÎØ∏ÎãàÏä§Ïª§Ìä∏\":\"miniskirt\",\"ÎØ∏ÎîîÏä§Ïª§Ìä∏\":\"midiskirt\",\"Î°±Ïä§Ïª§Ìä∏\":\"longskirt\"}\n",
    "    return major_map.get(major, \"skirt\"), sub_map.get(sub, \"unknown\")\n",
    "\n",
    "# ================== GPT Ìò∏Ï∂ú ==================\n",
    "def image_to_b64(image_path: str, max_side: int = MAX_SIDE, jpeg_quality: int = JPEG_QUALITY):\n",
    "    im = Image.open(image_path).convert(\"RGB\")\n",
    "    w, h = im.size\n",
    "    scale = max(w, h) / float(max_side)\n",
    "    if scale > 1.0:\n",
    "        try: resample = Image.Resampling.BICUBIC\n",
    "        except AttributeError: resample = Image.BICUBIC\n",
    "        im = im.resize((int(w/scale), int(h/scale)), resample)\n",
    "    buf = io.BytesIO()\n",
    "    im.save(buf, format=\"JPEG\", quality=jpeg_quality, optimize=True)\n",
    "    return base64.b64encode(buf.getvalue()).decode(\"utf-8\"), \"image/jpeg\"\n",
    "\n",
    "async def tag_one(image_path: str) -> Tuple[str, str]:\n",
    "    b64, mime = image_to_b64(image_path)\n",
    "    messages = [{\n",
    "        \"role\":\"user\",\n",
    "        \"content\":[\n",
    "            {\"type\":\"text\",\"text\":PROMPT},\n",
    "            {\"type\":\"image_url\",\"image_url\":{\"url\":f\"data:{mime};base64,{b64}\"}}\n",
    "        ]\n",
    "    }]\n",
    "    resp = await aclient.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        max_tokens=160,\n",
    "        temperature=0.0,\n",
    "        top_p=1.0,\n",
    "        seed=12345,\n",
    "    )\n",
    "    text = resp.choices[0].message.content.strip()\n",
    "    return image_path, normalize_caption_15(text)\n",
    "\n",
    "# ================== Stage1 Ïã§Ìñâ ==================\n",
    "async def run_stage1(image_paths: List[str], pid2cat: Dict[str, Tuple[str,str]]) -> pd.DataFrame:\n",
    "    sem = asyncio.Semaphore(MAX_CONCURRENCY)\n",
    "    out = []\n",
    "    done = 0\n",
    "\n",
    "    async def worker(p):\n",
    "        async with sem:\n",
    "            try:\n",
    "                _, cap15 = await tag_one(p)\n",
    "            except Exception as e:\n",
    "                print(\"caption error:\", p, e)\n",
    "                cap15 = \",\".join([f\"{k}=unknown\" for k in TOK_KEYS])\n",
    "            pid = extract_product_id_from_filename(p)\n",
    "            major, sub = pid2cat.get(pid, (\"Ïä§Ïª§Ìä∏\", \"\"))\n",
    "            cat_en, type_en = map_categories_and_sub(major, sub)\n",
    "            combined = tokens_to_combined_text(cap15, cat_en, type_en)\n",
    "            return {\"product_id\": pid, \"combined_text\": combined}\n",
    "\n",
    "    tasks = [asyncio.create_task(worker(p)) for p in image_paths]\n",
    "\n",
    "    for fut in asyncio.as_completed(tasks):\n",
    "        row = await fut\n",
    "        out.append(row)\n",
    "        done += 1\n",
    "        if done % 10 == 0 or done == len(tasks):\n",
    "            df_partial = pd.DataFrame(out).drop_duplicates(\"product_id\")\n",
    "            df_partial.to_csv(OUT_CSV, index=False, encoding=\"utf-8\")\n",
    "            print(f\"[Stage1] {done}/{len(tasks)} saved ‚Üí {OUT_CSV}\")\n",
    "\n",
    "    return pd.DataFrame(out).drop_duplicates(\"product_id\")\n",
    "\n",
    "# ================== Ïã§Ìñâ ==================\n",
    "exts = (\".jpg\",\".jpeg\",\".png\",\".webp\",\".bmp\",\".jfif\")\n",
    "image_paths = [\n",
    "    os.path.join(IMAGE_DIR,f)\n",
    "    for f in os.listdir(IMAGE_DIR)\n",
    "    if f.lower().endswith(exts) and f.lower().startswith(\"skirt_\")\n",
    "]\n",
    "if len(image_paths) == 0:\n",
    "    raise RuntimeError(\"Ï≤òÎ¶¨Ìï† skirt Ïù¥ÎØ∏ÏßÄÍ∞Ä ÏóÜÏäµÎãàÎã§.\")\n",
    "\n",
    "pid2cat = load_category_map_fixed(PRODUCT_INFO_CSV)\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "df = await run_stage1(image_paths, pid2cat)\n",
    "df.to_csv(OUT_CSV, index=False, encoding=\"utf-8\")\n",
    "print(\"‚úÖ Ï†ÄÏû• ÏôÑÎ£å:\", OUT_CSV)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badab0e0",
   "metadata": {},
   "source": [
    "## dress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ceb32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Stage 1: Ïù¥ÎØ∏ÏßÄ ‚Üí GPT-4o-mini ÌÉúÍπÖ ‚Üí combined_text ÏÉùÏÑ± ‚Üí CSV Ï†ÄÏû•\n",
    "# - Í≤∞Í≥º CSV: dress_caption.csv  (columns: product_id, combined_text)\n",
    "# - category/typeÏùÄ ÏõêÎ≥∏ CSV(29cm_1000.csv)ÏóêÏÑú Ï°∞Ìöå\n",
    "# - dress (ÏõêÌîºÏä§)Îßå Ï≤òÎ¶¨\n",
    "\n",
    "import os, re, io, base64, asyncio\n",
    "from typing import List, Dict, Tuple\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from dotenv import load_dotenv\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "# ================== Í≤ΩÎ°ú/ÌôòÍ≤Ω ==================\n",
    "IMAGE_DIR = \"crop_0.5\"          # crop Ïù¥ÎØ∏ÏßÄ Ìè¥Îçî (ÌååÏùºÎ™Ö: dress_ÏÉÅÌíàÏΩîÎìú.ext Îì±)\n",
    "PRODUCT_INFO_CSV = \"29cm_1000.csv\"  # [ÏÉÅÌíàÏΩîÎìú, ÎåÄÎ∂ÑÎ•ò, ÏÜåÎ∂ÑÎ•ò]\n",
    "OUT_CSV = \"dress_caption.csv\"       # ÏµúÏ¢Ö ÏÇ∞Ï∂úÎ¨º\n",
    "\n",
    "MAX_SIDE = 768\n",
    "JPEG_QUALITY = 85\n",
    "MAX_CONCURRENCY = 4\n",
    "\n",
    "load_dotenv()\n",
    "aclient = AsyncOpenAI()  # OPENAI_API_KEY ÌïÑÏöî\n",
    "\n",
    "# ================== ÌÜ†ÌÅ∞ ÌÇ§ ==================\n",
    "TOK_KEYS = [\n",
    "    \"bodice.fit\",\"skirt.volume\",\"dress.length\",\"hemline.shape\",\"hem.finish\",\n",
    "    \"waistline\",\"neckline\",\"sleeve.length\",\"sleeve.style\",\"skirt.structure\",\n",
    "    \"pattern\",\"pattern_scale\",\"material.fabric\",\"color.main\",\"color.sub\",\"closure\",\"skirt.slit\"\n",
    "]\n",
    "\n",
    "# ================== ÌóàÏö©Í∞í/Î≥ÑÏπ≠ ==================\n",
    "ALLOWED_PATTERNS = {\n",
    "    \"solid\",\"stripe\",\"check\",\"houndstooth\",\"herringbone\",\"dot\",\"floral\",\"paisley\",\n",
    "    \"animal\",\"camouflage\",\"text\",\"scenic\",\"logo\",\"geometric\",\"abstract\",\"lace-knit\",\"mixed\",\"unknown\"\n",
    "}\n",
    "PATTERN_ALIASES = {\n",
    "    \"newspaper\":\"text\",\"typographic\":\"text\",\"letters\":\"text\",\"letter\":\"text\",\"textual\":\"text\",\"script\":\"text\",\"font\":\"text\",\n",
    "    \"city\":\"scenic\",\"building\":\"scenic\",\"map\":\"scenic\",\"chevron\":\"herringbone\",\n",
    "    \"animal print\":\"animal\",\"leopard\":\"animal\",\"zebra\":\"animal\",\"snake\":\"animal\",\"tiger\":\"animal\",\"cow\":\"animal\",\n",
    "    \"camo\":\"camouflage\",\"monogram\":\"logo\",\"heather\":\"abstract\",\"marled\":\"abstract\",\"tie-dye\":\"abstract\",\"ikat\":\"abstract\"\n",
    "}\n",
    "ALLOWED_PATSCALE = {\"small\",\"medium\",\"large\",\"none\",\"unknown\"}\n",
    "ALLOWED_COLOR = {\"black\",\"white\",\"gray\",\"beige\",\"cream\",\"brown\",\"navy\",\"blue\",\"green\",\"yellow\",\"orange\",\"red\",\"pink\",\"purple\",\"unknown\"}\n",
    "ALLOWED_HEM_FINISH = {\"clean\",\"rolled\",\"lettuce\",\"scalloped\",\"lace-trim\",\"ruffle-trim\",\"fringed\",\"binding\",\"raw\",\"cutoff\",\"pleated-hem\",\"unknown\"}\n",
    "ALLOWED_SLIT = {\"none\",\"front\",\"side\",\"back\",\"two-side\",\"high-slit\",\"unknown\"}\n",
    "ALLOWED_MATERIAL = {\n",
    "    \"cotton\",\"linen\",\"wool\",\"silk\",\"satin\",\"denim\",\"leather\",\"suede\",\"tweed\",\"knit\",\"rib-knit\",\n",
    "    \"lace\",\"chiffon\",\"tulle\",\"velvet\",\"corduroy\",\"fleece\",\"jersey\",\"terry\",\"seersucker\",\"poplin\",\n",
    "    \"crepe\",\"organza\",\"brocade\",\"jacquard\",\"modal\",\"rayon\",\"viscose\",\"lyocell\",\"tencel\",\n",
    "    \"polyester\",\"nylon\",\"elastane\",\"spandex\",\"acrylic\",\"pu\",\"mesh\",\"eyelet\",\"crochet\",\n",
    "    \"faux-fur\",\"faux-leather\",\"blended\",\"unknown\"\n",
    "}\n",
    "MATERIAL_ALIASES = {\n",
    "    \"poly\":\"polyester\",\"polyamide\":\"nylon\",\"spandex\":\"elastane\",\"elastan\":\"elastane\",\"lycra\":\"elastane\",\n",
    "    \"pleather\":\"faux-leather\",\"fake leather\":\"faux-leather\",\"artificial leather\":\"faux-leather\",\n",
    "    \"tencel‚Ñ¢\":\"tencel\",\"viscosa\":\"viscose\",\"modal rayon\":\"modal\",\"cotten\":\"cotton\"\n",
    "}\n",
    "\n",
    "def _normalize_pattern_value(p: str) -> str:\n",
    "    p = (p or \"\").strip().lower()\n",
    "    p = PATTERN_ALIASES.get(p, p)\n",
    "    return p if p in ALLOWED_PATTERNS else \"unknown\"\n",
    "\n",
    "def _nz(s: str) -> str:\n",
    "    return (s or \"\").strip().lower()\n",
    "\n",
    "# ================== GPT ÌîÑÎ°¨ÌîÑÌä∏ ==================\n",
    "PROMPT = \"\"\"\n",
    "You are a vision tagger for fashion product retrieval.\n",
    "Analyze ONLY the dress (one-piece) region even if other items/body parts are visible.\n",
    "Never infer hidden details; if not clearly visible, output \"unknown\".\n",
    "If not applicable, output \"none\".\n",
    "Ignore hands, legs, accessories, or background items. Only describe the dress itself.\n",
    "\n",
    "Return ONE line with EXACTLY 17 lowercase, comma-separated tokens as key=value pairs,\n",
    "using these keys IN THIS ORDER (keys must match exactly; no extra fields):\n",
    "\n",
    "1) bodice.fit\n",
    "   fitted / semi / relaxed / unknown\n",
    "2) skirt.volume\n",
    "   slim / straight / a-line / full / mermaid / ball-gown / pencil / tulip / unknown\n",
    "3) dress.length\n",
    "   mini / knee / midi / ankle / maxi / unknown\n",
    "4) hemline.shape\n",
    "   straight / high-low / asymmetric / mermaid / ruffled / layered / wrap / train / handkerchief / bubble / shirttail / none / unknown\n",
    "5) hem.finish\n",
    "   clean / rolled / lettuce / scalloped / lace-trim / ruffle-trim / fringed / binding / raw / cutoff / pleated-hem / unknown\n",
    "6) waistline\n",
    "   none / natural / high / empire / drop / unknown\n",
    "7) neckline\n",
    "   round / v / square / halter / collar / off-shoulder / strapless / cowl / keyhole / boat / one-shoulder / unknown\n",
    "8) sleeve.length\n",
    "   sleeveless / cap / short / elbow / three-quarter / long / one-shoulder / strapless / unknown\n",
    "9) sleeve.style\n",
    "   none / puff / balloon / raglan / kimono / off-shoulder / cold-shoulder / bishop / roll-up / unknown\n",
    "10) skirt.structure\n",
    "   none / pleated / gathered / tiered / circle / bias / ruched / wrap / peplum / unknown\n",
    "11) pattern\n",
    "   solid / stripe / check / houndstooth / herringbone / dot / floral / paisley / animal / camouflage / text / scenic / logo / geometric / abstract / lace-knit / mixed / unknown\n",
    "12) pattern_scale\n",
    "   small / medium / large / none / unknown (if pattern=solid ‚áí none)\n",
    "13) material.fabric\n",
    "   cotton / linen / wool / silk / satin / denim / leather / suede / tweed / knit / rib-knit / lace / chiffon / tulle / velvet / corduroy / fleece / jersey / terry / seersucker / poplin / crepe / organza / brocade / jacquard / modal / rayon / viscose / lyocell / tencel / polyester / nylon / elastane / spandex / acrylic / pu / mesh / eyelet / crochet / faux-fur / faux-leather / blended / unknown\n",
    "14) color.main\n",
    "   black / white / gray / beige / cream / brown / navy / blue / green / yellow / orange / red / pink / purple / unknown\n",
    "15) color.sub\n",
    "   second-most ‚â• ~15% else none\n",
    "16) closure\n",
    "   zipper / buttons / hooks / drawstring / none / unknown\n",
    "17) skirt.slit\n",
    "   none / front / side / back / two-side / high-slit / unknown\n",
    "\n",
    "---\n",
    "\n",
    "CONSISTENCY\n",
    "- if pattern=solid ‚áí pattern_scale=none.\n",
    "- if sleeve.length=strapless ‚áí neckline=strapless and sleeve.style=none.\n",
    "- if sleeve.length=one-shoulder ‚áí neckline=one-shoulder and sleeve.style=none.\n",
    "- hemline.shape describes the shape; hem.finish describes the finishing/trim; treat them independently.\n",
    "- When unsure, output \"unknown\".\n",
    "\n",
    "---\n",
    "\n",
    "FORMAT RULES\n",
    "- Exactly 17 tokens, lowercase, comma-separated\n",
    "- Each token must be key=value\n",
    "- No extra words, no explanations\n",
    "- Example valid output:\n",
    "  \"bodice.fit=fitted,skirt.volume=a-line,dress.length=midi,hemline.shape=straight,hem.finish=clean,waistline=natural,neckline=round,sleeve.length=short,sleeve.style=none,skirt.structure=pleated,pattern=solid,pattern_scale=none,material.fabric=cotton,color.main=blue,color.sub=none,closure=zipper,skirt.slit=side\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# ================== ÎèÑÏö∞ÎØ∏ Ìï®Ïàò ==================\n",
    "def image_to_b64(image_path: str, max_side: int = MAX_SIDE, jpeg_quality: int = JPEG_QUALITY):\n",
    "    im = Image.open(image_path).convert(\"RGB\")\n",
    "    w, h = im.size\n",
    "    scale = max(w, h) / float(max_side)\n",
    "    if scale > 1.0:\n",
    "        try: resample = Image.Resampling.BICUBIC\n",
    "        except AttributeError: resample = Image.BICUBIC\n",
    "        im = im.resize((int(w/scale), int(h/scale)), resample)\n",
    "    buf = io.BytesIO()\n",
    "    im.save(buf, format=\"JPEG\", quality=jpeg_quality, optimize=True)\n",
    "    return base64.b64encode(buf.getvalue()).decode(\"utf-8\"), \"image/jpeg\"\n",
    "\n",
    "# ÌååÏùºÎ™ÖÏóêÏÑú product_id Ï∂îÏ∂ú\n",
    "PID_FROM_NAME = re.compile(r\"^dress_([^.\\\\/]+)\", re.IGNORECASE)\n",
    "def extract_product_id_from_filename(path: str) -> str:\n",
    "    base = os.path.basename(path)\n",
    "    m = PID_FROM_NAME.search(base)\n",
    "    return (m.group(1) if m else \"\").strip().lower()\n",
    "\n",
    "# Ï†úÌíà CSV Î∂àÎü¨Ïò§Í∏∞ (ÏõêÌîºÏä§Îßå)\n",
    "def load_dress_map(csv_path: str) -> Dict[str, Tuple[str,str]]:\n",
    "    df = pd.read_csv(csv_path, dtype=str)\n",
    "    df = df.fillna(\"\").apply(lambda col: col.str.strip().str.lower())\n",
    "    df_dress = df[df[\"ÎåÄÎ∂ÑÎ•ò\"] == \"ÏõêÌîºÏä§\"]  # dressÎßå ÌïÑÌÑ∞\n",
    "    return dict(zip(df_dress[\"ÏÉÅÌíàÏΩîÎìú\"], zip(df_dress[\"ÎåÄÎ∂ÑÎ•ò\"], df_dress[\"ÏÜåÎ∂ÑÎ•ò\"])))\n",
    "\n",
    "def map_categories_and_length(major: str, sub: str) -> Tuple[str, str]:\n",
    "    major_map = {\"ÏõêÌîºÏä§\":\"dress\"}\n",
    "    sub_map = {\n",
    "        \"ÎØ∏ÎãàÏõêÌîºÏä§\":\"minidress\",\n",
    "        \"ÎØ∏ÎîîÏõêÌîºÏä§\":\"mididress\",\n",
    "        \"Îß•ÏãúÏõêÌîºÏä§\":\"maxidress\",\n",
    "    }\n",
    "    return major_map.get(major, \"dress\"), sub_map.get(sub, \"unknown\")\n",
    "\n",
    "# Ï∫°ÏÖò Ï†ïÍ∑úÌôî\n",
    "def normalize_caption_dress17(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    text = \" \".join(text.splitlines()).strip().strip('\"').strip(\"'\")\n",
    "    parts = [p.strip().lower() for p in text.split(\",\") if p]\n",
    "\n",
    "    if len(parts) < 17:\n",
    "        parts += [\"unknown\"] * (17 - len(parts))\n",
    "    elif len(parts) > 17:\n",
    "        parts = parts[:17]\n",
    "\n",
    "    IDX = {k:i for i,k in enumerate(TOK_KEYS)}\n",
    "\n",
    "    # pattern ‚Üî scale\n",
    "    parts[IDX[\"pattern\"]] = _normalize_pattern_value(parts[IDX[\"pattern\"]])\n",
    "    if parts[IDX[\"pattern\"]] == \"solid\":\n",
    "        parts[IDX[\"pattern_scale\"]] = \"none\"\n",
    "    elif parts[IDX[\"pattern_scale\"]] not in ALLOWED_PATSCALE:\n",
    "        parts[IDX[\"pattern_scale\"]] = \"unknown\"\n",
    "\n",
    "    # ÏÉâÏÉÅ\n",
    "    if parts[IDX[\"color.main\"]] not in ALLOWED_COLOR: parts[IDX[\"color.main\"]] = \"unknown\"\n",
    "    if parts[IDX[\"color.sub\"]] not in ALLOWED_COLOR and parts[IDX[\"color.sub\"]] != \"none\":\n",
    "        parts[IDX[\"color.sub\"]] = \"unknown\"\n",
    "\n",
    "    # ÏÜåÏû¨\n",
    "    mat = MATERIAL_ALIASES.get(_nz(parts[IDX[\"material.fabric\"]]), _nz(parts[IDX[\"material.fabric\"]]))\n",
    "    parts[IDX[\"material.fabric\"]] = mat if mat in ALLOWED_MATERIAL else \"unknown\"\n",
    "\n",
    "    # hem.finish / slit\n",
    "    if parts[IDX[\"hem.finish\"]] not in ALLOWED_HEM_FINISH:\n",
    "        parts[IDX[\"hem.finish\"]] = \"unknown\"\n",
    "    if parts[IDX[\"skirt.slit\"]] not in ALLOWED_SLIT:\n",
    "        parts[IDX[\"skirt.slit\"]] = \"unknown\"\n",
    "\n",
    "    return \",\".join(parts)\n",
    "\n",
    "def tokens_to_combined_text(tokens_csv: str, category: str, type_: str) -> str:\n",
    "    parts = [p.strip() for p in tokens_csv.split(\",\")]\n",
    "    fixed = []\n",
    "    for i, tok in enumerate(parts):\n",
    "        if \"=\" in tok: fixed.append(tok)\n",
    "        else: fixed.append(f\"{TOK_KEYS[i]}={tok}\")\n",
    "    return f\"category={category} | type={type_} | \" + \" | \".join(fixed)\n",
    "\n",
    "# ================== ÌÜ†ÌÅ∞ ‚Üí combined_text ==================\n",
    "def tokens_to_combined_text(tokens_csv: str, category: str, type_: str) -> str:\n",
    "    parts = [p.strip() for p in tokens_csv.split(\",\")]\n",
    "    fixed = []\n",
    "    for i, tok in enumerate(parts):\n",
    "        if \"=\" in tok:\n",
    "            fixed.append(tok)\n",
    "        else:\n",
    "            fixed.append(f\"{TOK_KEYS[i]}={tok}\")\n",
    "    return f\"category={category} | type={type_} | \" + \" | \".join(fixed)\n",
    "\n",
    "\n",
    "# ================== GPT Ìò∏Ï∂ú ==================\n",
    "async def tag_one(image_path: str) -> Tuple[str, str]:\n",
    "    b64, mime = image_to_b64(image_path)\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": PROMPT},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:{mime};base64,{b64}\"}}\n",
    "        ]\n",
    "    }]\n",
    "    resp = await aclient.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        max_tokens=160,\n",
    "        temperature=0.0,\n",
    "        top_p=1.0,\n",
    "        seed=12345,\n",
    "    )\n",
    "    text = resp.choices[0].message.content.strip()\n",
    "    return image_path, normalize_caption_dress17(text)\n",
    "\n",
    "async def run_stage1(image_paths: List[str], pid2cat: Dict[str, Tuple[str,str]]) -> pd.DataFrame:\n",
    "    sem = asyncio.Semaphore(MAX_CONCURRENCY)\n",
    "    out = []\n",
    "    done = 0\n",
    "\n",
    "    async def worker(p):\n",
    "        async with sem:\n",
    "            try:\n",
    "                _, cap17 = await tag_one(p)\n",
    "            except Exception as e:\n",
    "                print(\"caption error:\", p, e)\n",
    "                cap17 = \",\".join([f\"{k}=unknown\" for k in TOK_KEYS])\n",
    "            pid = extract_product_id_from_filename(p)\n",
    "            major, sub = pid2cat.get(pid, (\"ÏõêÌîºÏä§\", \"\"))  # dressÎßå\n",
    "            cat_en, type_en = map_categories_and_length(major, sub)\n",
    "            combined = tokens_to_combined_text(cap17, cat_en, type_en)\n",
    "            return {\"product_id\": pid, \"combined_text\": combined}\n",
    "\n",
    "    tasks = [asyncio.create_task(worker(p)) for p in image_paths]\n",
    "\n",
    "    for fut in asyncio.as_completed(tasks):\n",
    "        row = await fut\n",
    "        out.append(row)\n",
    "        done += 1\n",
    "\n",
    "        # üîπ 10Í∞úÎßàÎã§ Ï§ëÍ∞Ñ Ï†ÄÏû• (ÎçÆÏñ¥Ïì∞Í∏∞)\n",
    "        if done % 10 == 0 or done == len(tasks):\n",
    "            df_partial = pd.DataFrame(out).drop_duplicates(subset=[\"product_id\"], keep=\"first\")\n",
    "            df_partial.to_csv(OUT_CSV, index=False, encoding=\"utf-8\")\n",
    "            print(f\"[Stage1] {done}/{len(tasks)} saved ‚Üí {OUT_CSV}\")\n",
    "\n",
    "    # ÎßàÏßÄÎßâÏóê ÏµúÏ¢Ö DataFrame Î∞òÌôò\n",
    "    return pd.DataFrame(out).drop_duplicates(subset=[\"product_id\"], keep=\"first\")\n",
    "\n",
    "\n",
    "\n",
    "# ================== Ïã§Ìñâ ==================\n",
    "# dress_* Ïù¥ÎØ∏ÏßÄÎßå Ï∂îÏ∂ú\n",
    "exts = (\".jpg\",\".jpeg\",\".png\",\".webp\",\".bmp\",\".jfif\")\n",
    "image_paths = [\n",
    "    os.path.join(IMAGE_DIR, f)\n",
    "    for f in os.listdir(IMAGE_DIR)\n",
    "    if f.lower().endswith(exts) and f.lower().startswith(\"dress_\")\n",
    "]\n",
    "if len(image_paths) == 0:\n",
    "    raise RuntimeError(\"Ï≤òÎ¶¨Ìï† dress Ïù¥ÎØ∏ÏßÄÍ∞Ä ÏóÜÏäµÎãàÎã§.\")\n",
    "\n",
    "pid2cat = load_dress_map(PRODUCT_INFO_CSV)\n",
    "\n",
    "# üîπ JupyterÏóêÏÑúÎäî asyncio.run() ÎåÄÏã† await ÏÇ¨Ïö©\n",
    "import nest_asyncio, asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "df = await run_stage1(image_paths, pid2cat)   # ‚úÖ Ïó¨Í∏∞ÏÑú awaitÎ°ú Ïã§Ìñâ\n",
    "df.to_csv(OUT_CSV, index=False, encoding=\"utf-8\")\n",
    "print(\"‚úÖ Ï†ÄÏû• ÏôÑÎ£å:\", OUT_CSV)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b10cc68",
   "metadata": {},
   "source": [
    "# caption split : 77 ÌÜ†ÌÅ∞Ïù¥ ÎÑòÏñ¥ Ï∫°ÏÖòÏùÑ Îëê Í∞úÎ°ú ÎÇòÎàÑÍ∏∞"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ded0c3",
   "metadata": {},
   "source": [
    "## top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be58e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV Î°úÎìú\n",
    "df = pd.read_csv(\"29cm/top_caption.csv\")\n",
    "\n",
    "def split_top_caption(text: str):\n",
    "    \"\"\"top category ÏÜçÏÑ±Îì§ÏùÑ caption1, caption2Î°ú Î∂ÑÎ¶¨\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\", \"\"\n",
    "\n",
    "    parts = [p.strip() for p in text.split(\"|\")]\n",
    "\n",
    "    # caption1 ÌÇ§ (ÏïûÎ∂ÄÎ∂Ñ: ÏïÑÏù¥ÌÖú Ï†ïÏ≤¥ÏÑ±)\n",
    "    caption1_keys = [\n",
    "        \"category\", \"type\", \"color.main\", \"color.sub\", \"pattern\", \"pattern_scale\", \"material.fabric\"\n",
    "    ]\n",
    "\n",
    "    caption1, caption2 = [], []\n",
    "    for p in parts:\n",
    "        key = p.split(\"=\")[0].strip()\n",
    "        if key in caption1_keys:\n",
    "            caption1.append(p)\n",
    "        else:\n",
    "            caption2.append(p)\n",
    "\n",
    "    return \" | \".join(caption1), \" | \".join(caption2)\n",
    "\n",
    "# caption1, caption2 Ïª¨Îüº ÏÉùÏÑ±\n",
    "df[[\"caption1\", \"caption2\"]] = df[\"combined_text\"].apply(\n",
    "    lambda x: pd.Series(split_top_caption(str(x)))\n",
    ")\n",
    "\n",
    "# Ï†ÄÏû•\n",
    "df.to_csv(\"29cm/top_caption_split.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"‚úÖ caption1, caption2 Î∂ÑÎ¶¨ ÏôÑÎ£å ‚Üí 29cm/top_caption_split.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c42d74",
   "metadata": {},
   "source": [
    "## pants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42981877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV Î°úÎìú\n",
    "df = pd.read_csv(\"29cm/pants_caption.csv\")\n",
    "\n",
    "def split_pants_caption(text: str):\n",
    "    \"\"\"pants category ÏÜçÏÑ±Îì§ÏùÑ caption1, caption2Î°ú Î∂ÑÎ¶¨\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\", \"\"\n",
    "\n",
    "    parts = [p.strip() for p in text.split(\"|\")]\n",
    "\n",
    "    # caption1: Ï†ÑÎ∞òÏ†Å Ïô∏Ìòï (Ìïè, Í∏∞Ïû•, Ïû¨Ïßà, ÏÉâÏÉÅ, Ìå®ÌÑ¥ Îì± ÌÅ∞ Í∑∏Î¶º)\n",
    "    caption1_keys = [\n",
    "        \"category\", \"type\", \"fit\", \"rise\", \"leg.length\",\n",
    "        \"material.fabric\", \"pattern\", \"pattern_scale\",\n",
    "        \"color.main\", \"color.sub\"\n",
    "    ]\n",
    "\n",
    "    caption1, caption2 = [], []\n",
    "    for p in parts:\n",
    "        key = p.split(\"=\")[0].strip()\n",
    "        if key in caption1_keys:\n",
    "            caption1.append(p)\n",
    "        else:\n",
    "            caption2.append(p)\n",
    "\n",
    "    return \" | \".join(caption1), \" | \".join(caption2)\n",
    "\n",
    "# caption1, caption2 Ïª¨Îüº ÏÉùÏÑ±\n",
    "df[[\"caption1\", \"caption2\"]] = df[\"combined_text\"].apply(\n",
    "    lambda x: pd.Series(split_pants_caption(str(x)))\n",
    ")\n",
    "\n",
    "# Ï†ÄÏû•\n",
    "df.to_csv(\"29cm/pants_caption_split.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"‚úÖ caption1, caption2 Î∂ÑÎ¶¨ ÏôÑÎ£å ‚Üí 29cm/pants_caption_split.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add718f2",
   "metadata": {},
   "source": [
    "## skirt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee39457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV Î°úÎìú\n",
    "df = pd.read_csv(\"29cm/skirt_caption.csv\")\n",
    "\n",
    "def split_skirt_caption(text: str):\n",
    "    \"\"\"skirt category ÏÜçÏÑ±Îì§ÏùÑ caption1, caption2Î°ú Î∂ÑÎ¶¨\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\", \"\"\n",
    "\n",
    "    parts = [p.strip() for p in text.split(\"|\")]\n",
    "\n",
    "    # caption1: Ï†ÑÎ∞òÏ†Å Ïô∏Ìòï (Ïπ¥ÌÖåÍ≥†Î¶¨, ÌÉÄÏûÖ, ÏÉâÏÉÅ, Ìå®ÌÑ¥, ÏÜåÏû¨, Ïã§Î£®Ïó£ Îì±)\n",
    "    caption1_keys = [\n",
    "        \"category\", \"type\", \"skirt.length\", \"silhouette\",\n",
    "        \"material.fabric\", \"pattern\", \"pattern_scale\",\n",
    "        \"color.main\", \"color.sub\", \"color.tone\"\n",
    "    ]\n",
    "\n",
    "    caption1, caption2 = [], []\n",
    "    for p in parts:\n",
    "        key = p.split(\"=\")[0].strip()\n",
    "        if key in caption1_keys:\n",
    "            caption1.append(p)\n",
    "        else:\n",
    "            caption2.append(p)\n",
    "\n",
    "    return \" | \".join(caption1), \" | \".join(caption2)\n",
    "\n",
    "# caption1, caption2 Ïª¨Îüº ÏÉùÏÑ±\n",
    "df[[\"caption1\", \"caption2\"]] = df[\"combined_text\"].apply(\n",
    "    lambda x: pd.Series(split_skirt_caption(str(x)))\n",
    ")\n",
    "\n",
    "# Ï†ÄÏû•\n",
    "df.to_csv(\"29cm/skirt_caption_split.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"‚úÖ caption1, caption2 Î∂ÑÎ¶¨ ÏôÑÎ£å ‚Üí 29cm/skirt_caption_split.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c79415e",
   "metadata": {},
   "source": [
    "## dress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf00862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV Î°úÎìú\n",
    "df = pd.read_csv(\"29cm/dress_caption.csv\")\n",
    "\n",
    "def split_dress_caption(text: str):\n",
    "    \"\"\"dress category ÏÜçÏÑ±Îì§ÏùÑ caption1, caption2Î°ú Î∂ÑÎ¶¨\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\", \"\"\n",
    "\n",
    "    parts = [p.strip() for p in text.split(\"|\")]\n",
    "\n",
    "    # caption1: Ï†ÑÎ∞òÏ†Å Ïô∏Ìòï (ÎìúÎ†àÏä§ Í∏∏Ïù¥, Ïã§Î£®Ïó£, ÏÜåÏû¨, ÏÉâÏÉÅ, Ìå®ÌÑ¥ Îì± ÌÅ∞ Í∑∏Î¶º)\n",
    "    caption1_keys = [\n",
    "        \"category\", \"type\", \"dress.length\", \"skirt.volume\",\n",
    "        \"material.fabric\", \"pattern\", \"pattern_scale\",\n",
    "        \"color.main\", \"color.sub\"\n",
    "    ]\n",
    "\n",
    "    caption1, caption2 = [], []\n",
    "    for p in parts:\n",
    "        key = p.split(\"=\")[0].strip()\n",
    "        if key in caption1_keys:\n",
    "            caption1.append(p)\n",
    "        else:\n",
    "            caption2.append(p)\n",
    "\n",
    "    return \" | \".join(caption1), \" | \".join(caption2)\n",
    "\n",
    "# caption1, caption2 Ïª¨Îüº ÏÉùÏÑ±\n",
    "df[[\"caption1\", \"caption2\"]] = df[\"combined_text\"].apply(\n",
    "    lambda x: pd.Series(split_dress_caption(str(x)))\n",
    ")\n",
    "\n",
    "# Ï†ÄÏû•\n",
    "df.to_csv(\"29cm/dress_caption_split.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"‚úÖ caption1, caption2 Î∂ÑÎ¶¨ ÏôÑÎ£å ‚Üí 29cm/dress_caption_split.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a23bd61",
   "metadata": {},
   "source": [
    "# Embedding : Îëê Í∞úÎ°ú ÎÇòÎâú Ï∫°ÏÖòÏùÑ Í∞ÅÍ∞Å ÏûÑÎ≤†Îî©ÌïòÏó¨ ÌèâÍ∑†ÏùÑ ÎÇ¥Ïñ¥ Ìï©ÏπòÍ≥† text ÏôÄ cropÎêú Ïù¥ÎØ∏ÏßÄÎ•º Clip Î™®Îç∏ÏùÑ ÌÜµÌï¥ ÏûÑÎ≤†Îî© "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cb45c0",
   "metadata": {},
   "source": [
    "## top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d10358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Top Embedding Pipeline (caption1+caption2 ÌèâÍ∑† text ÏûÑÎ≤†Îî© + multi = text+image 0.5:0.5)\n",
    "# CSV Ï†ÄÏû• Ïãú information(ÏõêÎ≥∏ Ï∫°ÏÖò) Ïª¨ÎüºÎßå ÎÇ®ÍπÄ\n",
    "\n",
    "import os, io, json, torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from fashion_clip.fashion_clip import FashionCLIP\n",
    "\n",
    "# ==============================\n",
    "# 0) ÏÑ§Ï†ï\n",
    "# ==============================\n",
    "IMAGE_DIR = \"crop_29cm\"                       # cropÎêú Ïù¥ÎØ∏ÏßÄ Ìè¥Îçî\n",
    "CAPTIONS_CSV = \"29cm/top_caption_split.csv\"   # caption1, caption2 Ìè¨Ìï®Îêú CSV\n",
    "PRODUCT_INFO_CSV = \"29cm/29cm_1000.csv\"       # [ÏÉÅÌíàÏΩîÎìú, ÎåÄÎ∂ÑÎ•ò, ÏÜåÎ∂ÑÎ•ò]\n",
    "OUTPUT_CSV = \"29cm/top_embeddings_real_final.csv\"  # ÏµúÏ¢Ö ÏÇ∞Ï∂úÎ¨º\n",
    "\n",
    "# ÎîîÎ∞îÏù¥Ïä§ ÏÑ†ÌÉù\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "# Î™®Îç∏ Î°úÎìú\n",
    "fclip = FashionCLIP(\"fashion-clip\")\n",
    "try:\n",
    "    fclip.to(device)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ==============================\n",
    "# 1) Ïú†Ìã∏ Ìï®Ïàò\n",
    "# ==============================\n",
    "def l2_normalize(x: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=np.float32)\n",
    "    if x.ndim == 1:\n",
    "        x = x[None, :]\n",
    "    n = np.linalg.norm(x, axis=1, keepdims=True)\n",
    "    n = np.maximum(n, eps)\n",
    "    return (x / n).astype(np.float32)\n",
    "\n",
    "def _to_numpy_2d(x) -> np.ndarray:\n",
    "    if isinstance(x, np.ndarray):\n",
    "        arr = x\n",
    "    elif torch.is_tensor(x):\n",
    "        arr = x.detach().cpu().numpy()\n",
    "    else:\n",
    "        arr = np.asarray(x)\n",
    "    if arr.ndim == 1:\n",
    "        arr = arr[None, :]\n",
    "    return arr.astype(np.float32)\n",
    "\n",
    "def torch_empty_cache():\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "    elif device == \"mps\":\n",
    "        try:\n",
    "            torch.mps.empty_cache()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def determine_best_batch_size(start: int = 512) -> int:\n",
    "    \"\"\"OOM ÌîºÌïòÎ©¥ÏÑú Í∞ÄÏû• ÌÅ∞ batch_size ÏÑ†ÌÉù\"\"\"\n",
    "    if start >= 512:\n",
    "        candidates = [start,384,256,192,128,96,64,48,32,24,16,12,8,4,2,1]\n",
    "    elif start >= 256:\n",
    "        candidates = [start,192,128,96,64,48,32,24,16,12,8,4,2,1]\n",
    "    else:\n",
    "        candidates = [start,48,32,24,16,12,8,4,2,1]\n",
    "    for bs in candidates:\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                _ = fclip.encode_text([\"warmup\"] * bs, batch_size=bs)\n",
    "                dummy = Image.new(\"RGB\", (224,224), (255,255,255))\n",
    "                _ = fclip.encode_images([dummy] * bs, batch_size=bs)\n",
    "            print(f\">>> OK batch_size={bs}\")\n",
    "            return bs\n",
    "        except Exception as e:\n",
    "            print(f\"OOM/Fail at batch_size={bs} ‚Üí {e}\")\n",
    "            torch_empty_cache()\n",
    "    return 1\n",
    "\n",
    "# ==============================\n",
    "# 2) ÏûÑÎ≤†Îî© Ìï®Ïàò\n",
    "# ==============================\n",
    "def embed_in_batches(records, batch_size: int):\n",
    "    out = []\n",
    "    n = len(records)\n",
    "    total_batches = (n + batch_size - 1) // batch_size\n",
    "\n",
    "    for bi in range(total_batches):\n",
    "        s, e = bi*batch_size, min((bi+1)*batch_size, n)\n",
    "        chunk = records[s:e]\n",
    "\n",
    "        texts1 = [r[\"caption1\"] for r in chunk]\n",
    "        texts2 = [r[\"caption2\"] for r in chunk]\n",
    "        combined_texts = [r[\"caption1\"] + \" | \" + r[\"caption2\"] for r in chunk]\n",
    "        images = [Image.open(r[\"image_path\"]).convert(\"RGB\") for r in chunk]\n",
    "\n",
    "        try:\n",
    "            with torch.no_grad(), torch.autocast(device_type='cuda', dtype=torch.float16, enabled=(device=='cuda')):\n",
    "                # caption1, caption2 ÏûÑÎ≤†Îî©\n",
    "                t1_emb = fclip.encode_text(texts1, batch_size=len(texts1))\n",
    "                t2_emb = fclip.encode_text(texts2, batch_size=len(texts2))\n",
    "                # combined_text ÏûÑÎ≤†Îî© (information Ïª¨ÎüºÏö©)\n",
    "                t_comb_emb = fclip.encode_text(combined_texts, batch_size=len(combined_texts))\n",
    "                # Ïù¥ÎØ∏ÏßÄ ÏûÑÎ≤†Îî©\n",
    "                v_emb = fclip.encode_images(images, batch_size=len(images))\n",
    "        finally:\n",
    "            for img in images:\n",
    "                try: img.close()\n",
    "                except: pass\n",
    "\n",
    "        t1_np = _to_numpy_2d(t1_emb)\n",
    "        t2_np = _to_numpy_2d(t2_emb)\n",
    "        t_comb_np = _to_numpy_2d(t_comb_emb)\n",
    "        v_np  = _to_numpy_2d(v_emb)\n",
    "\n",
    "        # text-only = caption1 + caption2 ÌèâÍ∑†\n",
    "        text_only = l2_normalize((t1_np + t2_np) / 2)\n",
    "\n",
    "        # multi = text + image (0.5:0.5)\n",
    "        multi = l2_normalize(0.5*text_only + 0.5*l2_normalize(v_np))\n",
    "\n",
    "        # Í≤∞Í≥º Ï†ÄÏû• (caption1, caption2 ÎåÄÏã† informationÎßå Ï†ÄÏû•)\n",
    "        for r, info, t_vec, m_vec in zip(chunk, combined_texts, text_only, multi):\n",
    "            out.append({\n",
    "                \"id\": r[\"product_id\"],\n",
    "                \"category\": r[\"category\"],\n",
    "                \"type\": r[\"type\"],\n",
    "                \"information\": info,\n",
    "                \"text\": json.dumps(t_vec.tolist()),\n",
    "                \"multi\": json.dumps(m_vec.tolist())\n",
    "            })\n",
    "\n",
    "        torch_empty_cache()\n",
    "        print(f\"[EMBED] batch {bi+1}/{total_batches} ‚Üí {e}/{n}\")\n",
    "\n",
    "    return out\n",
    "\n",
    "# ==============================\n",
    "# 3) Î©îÏù∏\n",
    "# ==============================\n",
    "def main():\n",
    "    if not os.path.exists(CAPTIONS_CSV):\n",
    "        raise FileNotFoundError(f\"Ï∫°ÏÖò CSVÍ∞Ä ÏóÜÏäµÎãàÎã§: {CAPTIONS_CSV}\")\n",
    "    if not os.path.exists(PRODUCT_INFO_CSV):\n",
    "        raise FileNotFoundError(f\"ÏÉÅÌíàÏ†ïÎ≥¥ CSVÍ∞Ä ÏóÜÏäµÎãàÎã§: {PRODUCT_INFO_CSV}\")\n",
    "\n",
    "    captions_df = pd.read_csv(CAPTIONS_CSV)\n",
    "    product_df = pd.read_csv(PRODUCT_INFO_CSV, dtype=str).fillna(\"\")\n",
    "    product_df = product_df.apply(lambda col: col.str.strip().str.lower())\n",
    "\n",
    "    product_map = dict(zip(product_df[\"ÏÉÅÌíàÏΩîÎìú\"], zip(product_df[\"ÎåÄÎ∂ÑÎ•ò\"], product_df[\"ÏÜåÎ∂ÑÎ•ò\"])))\n",
    "\n",
    "    # ÌïúÍ∏Ä ‚Üí ÏòÅÏñ¥ Îß§Ìïë\n",
    "    major_map = {\"ÏÉÅÏùò\": \"top\"}\n",
    "    sub_map = {\n",
    "        \"ÌõÑÎìúÌã∞\": \"hoodie\",\n",
    "        \"ÏÖîÏ∏†Î∏îÎùºÏö∞Ïä§\": \"shirt-blouse\",\n",
    "        \"Í∏¥ÏÜåÎß§\": \"longsleeve\",\n",
    "        \"Î∞òÏÜåÎß§\": \"shortsleeve\",\n",
    "        \"ÌîºÏºÄÏπ¥Îùº\": \"polo\",\n",
    "        \"ÎãàÌä∏Ïä§Ïõ®ÌÑ∞\": \"knit-sweater\",\n",
    "        \"Ïä¨Î¶¨Î∏åÎ¶¨Ïä§\": \"sleeveless\",\n",
    "    }\n",
    "\n",
    "    records = []\n",
    "    for _, row in captions_df.iterrows():\n",
    "        pid = str(row[\"product_id\"]).lower()\n",
    "        caption1 = str(row[\"caption1\"])\n",
    "        caption2 = str(row[\"caption2\"])\n",
    "\n",
    "        # category/type Îß§Ìïë\n",
    "        raw_category, raw_type = product_map.get(pid, (\"unknown\",\"unknown\"))\n",
    "        category = major_map.get(raw_category, raw_category)\n",
    "        type_ = sub_map.get(raw_type, raw_type)\n",
    "\n",
    "        # Ïù¥ÎØ∏ÏßÄ Í≤ΩÎ°ú ÌÉêÏÉâ\n",
    "        img_path = None\n",
    "        for ext in [\".jpg\",\".jpeg\",\".png\",\".webp\",\".bmp\",\".jfif\"]:\n",
    "            candidate = os.path.join(IMAGE_DIR, f\"top_{pid}{ext}\")\n",
    "            if os.path.exists(candidate):\n",
    "                img_path = candidate\n",
    "                break\n",
    "        if not img_path:\n",
    "            continue\n",
    "\n",
    "        records.append({\n",
    "            \"product_id\": pid,\n",
    "            \"category\": category,\n",
    "            \"type\": type_,\n",
    "            \"caption1\": caption1,\n",
    "            \"caption2\": caption2,\n",
    "            \"image_path\": img_path\n",
    "        })\n",
    "\n",
    "    if not records:\n",
    "        raise RuntimeError(\"Ï≤òÎ¶¨Ìï† Î†àÏΩîÎìúÍ∞Ä ÏóÜÏäµÎãàÎã§.\")\n",
    "\n",
    "    # Î∞∞Ïπò ÌÅ¨Í∏∞ Í≤∞Ï†ï\n",
    "    start_bs = 512 if device in (\"cuda\",\"mps\") else 256\n",
    "    best_bs = determine_best_batch_size(start=start_bs)\n",
    "    print(f\">>> Using batch_size={best_bs} on {device}\")\n",
    "\n",
    "    embedded = embed_in_batches(records, batch_size=best_bs)\n",
    "\n",
    "    # Ï†ÄÏû• (ÏµúÏ¢Ö Ïª¨Îüº: id, category, type, information, text, multi)\n",
    "    df = pd.DataFrame(embedded, columns=[\"id\",\"category\",\"type\",\"information\",\"text\",\"multi\"])\n",
    "    df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "    print(f\"‚úÖ ÏûÑÎ≤†Îî© Ï†ÄÏû• ÏôÑÎ£å: {OUTPUT_CSV}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311df10d",
   "metadata": {},
   "source": [
    "## pants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d126ca3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Top Embedding Pipeline (caption1+caption2 ÌèâÍ∑† text ÏûÑÎ≤†Îî© + multi = text+image 0.5:0.5)\n",
    "# CSV Ï†ÄÏû• Ïãú information(ÏõêÎ≥∏ Ï∫°ÏÖò) Ïª¨ÎüºÎßå ÎÇ®ÍπÄ\n",
    "\n",
    "import os, io, json, torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from fashion_clip.fashion_clip import FashionCLIP\n",
    "\n",
    "# ==============================\n",
    "# 0) ÏÑ§Ï†ï\n",
    "# ==============================\n",
    "IMAGE_DIR = \"crop_29cm\"                       # cropÎêú Ïù¥ÎØ∏ÏßÄ Ìè¥Îçî\n",
    "CAPTIONS_CSV = \"29cm/pants_caption_split.csv\"   # caption1, caption2 Ìè¨Ìï®Îêú CSV\n",
    "PRODUCT_INFO_CSV = \"29cm/29cm_1000.csv\"       # [ÏÉÅÌíàÏΩîÎìú, ÎåÄÎ∂ÑÎ•ò, ÏÜåÎ∂ÑÎ•ò]\n",
    "OUTPUT_CSV = \"29cm/pants_embeddings_real_final.csv\"  # ÏµúÏ¢Ö ÏÇ∞Ï∂úÎ¨º\n",
    "\n",
    "# ÎîîÎ∞îÏù¥Ïä§ ÏÑ†ÌÉù\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "# Î™®Îç∏ Î°úÎìú\n",
    "fclip = FashionCLIP(\"fashion-clip\")\n",
    "try:\n",
    "    fclip.to(device)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ==============================\n",
    "# 1) Ïú†Ìã∏ Ìï®Ïàò\n",
    "# ==============================\n",
    "def l2_normalize(x: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=np.float32)\n",
    "    if x.ndim == 1:\n",
    "        x = x[None, :]\n",
    "    n = np.linalg.norm(x, axis=1, keepdims=True)\n",
    "    n = np.maximum(n, eps)\n",
    "    return (x / n).astype(np.float32)\n",
    "\n",
    "def _to_numpy_2d(x) -> np.ndarray:\n",
    "    if isinstance(x, np.ndarray):\n",
    "        arr = x\n",
    "    elif torch.is_tensor(x):\n",
    "        arr = x.detach().cpu().numpy()\n",
    "    else:\n",
    "        arr = np.asarray(x)\n",
    "    if arr.ndim == 1:\n",
    "        arr = arr[None, :]\n",
    "    return arr.astype(np.float32)\n",
    "\n",
    "def torch_empty_cache():\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "    elif device == \"mps\":\n",
    "        try:\n",
    "            torch.mps.empty_cache()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def determine_best_batch_size(start: int = 512) -> int:\n",
    "    \"\"\"OOM ÌîºÌïòÎ©¥ÏÑú Í∞ÄÏû• ÌÅ∞ batch_size ÏÑ†ÌÉù\"\"\"\n",
    "    if start >= 512:\n",
    "        candidates = [start,384,256,192,128,96,64,48,32,24,16,12,8,4,2,1]\n",
    "    elif start >= 256:\n",
    "        candidates = [start,192,128,96,64,48,32,24,16,12,8,4,2,1]\n",
    "    else:\n",
    "        candidates = [start,48,32,24,16,12,8,4,2,1]\n",
    "    for bs in candidates:\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                _ = fclip.encode_text([\"warmup\"] * bs, batch_size=bs)\n",
    "                dummy = Image.new(\"RGB\", (224,224), (255,255,255))\n",
    "                _ = fclip.encode_images([dummy] * bs, batch_size=bs)\n",
    "            print(f\">>> OK batch_size={bs}\")\n",
    "            return bs\n",
    "        except Exception as e:\n",
    "            print(f\"OOM/Fail at batch_size={bs} ‚Üí {e}\")\n",
    "            torch_empty_cache()\n",
    "    return 1\n",
    "\n",
    "# ==============================\n",
    "# 2) ÏûÑÎ≤†Îî© Ìï®Ïàò\n",
    "# ==============================\n",
    "def embed_in_batches(records, batch_size: int):\n",
    "    out = []\n",
    "    n = len(records)\n",
    "    total_batches = (n + batch_size - 1) // batch_size\n",
    "\n",
    "    for bi in range(total_batches):\n",
    "        s, e = bi*batch_size, min((bi+1)*batch_size, n)\n",
    "        chunk = records[s:e]\n",
    "\n",
    "        texts1 = [r[\"caption1\"] for r in chunk]\n",
    "        texts2 = [r[\"caption2\"] for r in chunk]\n",
    "        combined_texts = [r[\"caption1\"] + \" | \" + r[\"caption2\"] for r in chunk]\n",
    "        images = [Image.open(r[\"image_path\"]).convert(\"RGB\") for r in chunk]\n",
    "\n",
    "        try:\n",
    "            with torch.no_grad(), torch.autocast(device_type='cuda', dtype=torch.float16, enabled=(device=='cuda')):\n",
    "                # caption1, caption2 ÏûÑÎ≤†Îî©\n",
    "                t1_emb = fclip.encode_text(texts1, batch_size=len(texts1))\n",
    "                t2_emb = fclip.encode_text(texts2, batch_size=len(texts2))\n",
    "                # combined_text ÏûÑÎ≤†Îî© (information Ïª¨ÎüºÏö©)\n",
    "                t_comb_emb = fclip.encode_text(combined_texts, batch_size=len(combined_texts))\n",
    "                # Ïù¥ÎØ∏ÏßÄ ÏûÑÎ≤†Îî©\n",
    "                v_emb = fclip.encode_images(images, batch_size=len(images))\n",
    "        finally:\n",
    "            for img in images:\n",
    "                try: img.close()\n",
    "                except: pass\n",
    "\n",
    "        t1_np = _to_numpy_2d(t1_emb)\n",
    "        t2_np = _to_numpy_2d(t2_emb)\n",
    "        t_comb_np = _to_numpy_2d(t_comb_emb)\n",
    "        v_np  = _to_numpy_2d(v_emb)\n",
    "\n",
    "        # text-only = caption1 + caption2 ÌèâÍ∑†\n",
    "        text_only = l2_normalize((t1_np + t2_np) / 2)\n",
    "\n",
    "        # multi = text + image (0.5:0.5)\n",
    "        multi = l2_normalize(0.5*text_only + 0.5*l2_normalize(v_np))\n",
    "\n",
    "        # Í≤∞Í≥º Ï†ÄÏû• (caption1, caption2 ÎåÄÏã† informationÎßå Ï†ÄÏû•)\n",
    "        for r, info, t_vec, m_vec in zip(chunk, combined_texts, text_only, multi):\n",
    "            out.append({\n",
    "                \"id\": r[\"product_id\"],\n",
    "                \"category\": r[\"category\"],\n",
    "                \"type\": r[\"type\"],\n",
    "                \"information\": info,\n",
    "                \"text\": json.dumps(t_vec.tolist()),\n",
    "                \"multi\": json.dumps(m_vec.tolist())\n",
    "            })\n",
    "\n",
    "        torch_empty_cache()\n",
    "        print(f\"[EMBED] batch {bi+1}/{total_batches} ‚Üí {e}/{n}\")\n",
    "\n",
    "    return out\n",
    "\n",
    "# ==============================\n",
    "# 3) Î©îÏù∏\n",
    "# ==============================\n",
    "def main():\n",
    "    if not os.path.exists(CAPTIONS_CSV):\n",
    "        raise FileNotFoundError(f\"Ï∫°ÏÖò CSVÍ∞Ä ÏóÜÏäµÎãàÎã§: {CAPTIONS_CSV}\")\n",
    "    if not os.path.exists(PRODUCT_INFO_CSV):\n",
    "        raise FileNotFoundError(f\"ÏÉÅÌíàÏ†ïÎ≥¥ CSVÍ∞Ä ÏóÜÏäµÎãàÎã§: {PRODUCT_INFO_CSV}\")\n",
    "\n",
    "    captions_df = pd.read_csv(CAPTIONS_CSV)\n",
    "    product_df = pd.read_csv(PRODUCT_INFO_CSV, dtype=str).fillna(\"\")\n",
    "    product_df = product_df.apply(lambda col: col.str.strip().str.lower())\n",
    "\n",
    "    product_map = dict(zip(product_df[\"ÏÉÅÌíàÏΩîÎìú\"], zip(product_df[\"ÎåÄÎ∂ÑÎ•ò\"], product_df[\"ÏÜåÎ∂ÑÎ•ò\"])))\n",
    "\n",
    "    # ÌïúÍ∏Ä ‚Üí ÏòÅÏñ¥ Îß§Ìïë\n",
    "    major_map = {\n",
    "        \"ÌïòÏùò\": \"pants\",\n",
    "        \"Î∞îÏßÄ\": \"pants\",\n",
    "    }\n",
    "    sub_map = {\n",
    "        \"Îç∞ÎãòÌå¨Ï∏†\": \"denim-pants\",\n",
    "        \"Ìä∏Î†àÏù¥ÎãùÏ°∞Í±∞Ìå¨Ï∏†\": \"jogger-pants\",\n",
    "        \"ÏΩîÌäºÌå¨Ï∏†\": \"cotton-pants\",\n",
    "        \"ÏäàÌä∏Ìå¨Ï∏†Ïä¨ÎûôÏä§\": \"slacks\",  # 'ÏäàÌä∏ Ìå¨Ï∏†/Ïä¨ÎûôÏä§' Ìï©Ï≥êÏßÑ ÏºÄÏù¥Ïä§\n",
    "        \"ÏäàÌä∏Ïä¨ÎûôÏä§\": \"slacks\",\n",
    "        \"ÏàèÌå¨Ï∏†\": \"short-pants\",\n",
    "        \"Ïπ¥Í≥†Ìå¨Ï∏†\": \"cargo-pants\"\n",
    "    }\n",
    "\n",
    "    records = []\n",
    "    for _, row in captions_df.iterrows():\n",
    "        pid = str(row[\"product_id\"]).lower()\n",
    "        caption1 = str(row[\"caption1\"])\n",
    "        caption2 = str(row[\"caption2\"])\n",
    "\n",
    "        # category/type Îß§Ìïë\n",
    "        raw_category, raw_type = product_map.get(pid, (\"unknown\",\"unknown\"))\n",
    "        category = major_map.get(raw_category, raw_category)\n",
    "        type_ = sub_map.get(raw_type, raw_type)\n",
    "\n",
    "        # Ïù¥ÎØ∏ÏßÄ Í≤ΩÎ°ú ÌÉêÏÉâ\n",
    "        img_path = None\n",
    "        for ext in [\".jpg\",\".jpeg\",\".png\",\".webp\",\".bmp\",\".jfif\"]:\n",
    "            candidate = os.path.join(IMAGE_DIR, f\"pants_{pid}{ext}\")\n",
    "            if os.path.exists(candidate):\n",
    "                img_path = candidate\n",
    "                break\n",
    "        if not img_path:\n",
    "            continue\n",
    "\n",
    "        records.append({\n",
    "            \"product_id\": pid,\n",
    "            \"category\": category,\n",
    "            \"type\": type_,\n",
    "            \"caption1\": caption1,\n",
    "            \"caption2\": caption2,\n",
    "            \"image_path\": img_path\n",
    "        })\n",
    "\n",
    "    if not records:\n",
    "        raise RuntimeError(\"Ï≤òÎ¶¨Ìï† Î†àÏΩîÎìúÍ∞Ä ÏóÜÏäµÎãàÎã§.\")\n",
    "\n",
    "    # Î∞∞Ïπò ÌÅ¨Í∏∞ Í≤∞Ï†ï\n",
    "    start_bs = 512 if device in (\"cuda\",\"mps\") else 256\n",
    "    best_bs = determine_best_batch_size(start=start_bs)\n",
    "    print(f\">>> Using batch_size={best_bs} on {device}\")\n",
    "\n",
    "    embedded = embed_in_batches(records, batch_size=best_bs)\n",
    "\n",
    "    # Ï†ÄÏû• (ÏµúÏ¢Ö Ïª¨Îüº: id, category, type, information, text, multi)\n",
    "    df = pd.DataFrame(embedded, columns=[\"id\",\"category\",\"type\",\"information\",\"text\",\"multi\"])\n",
    "    df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "    print(f\"‚úÖ ÏûÑÎ≤†Îî© Ï†ÄÏû• ÏôÑÎ£å: {OUTPUT_CSV}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5efbb12",
   "metadata": {},
   "source": [
    "## skirt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e5de2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Top Embedding Pipeline (caption1+caption2 ÌèâÍ∑† text ÏûÑÎ≤†Îî© + multi = text+image 0.5:0.5)\n",
    "# CSV Ï†ÄÏû• Ïãú information(ÏõêÎ≥∏ Ï∫°ÏÖò) Ïª¨ÎüºÎßå ÎÇ®ÍπÄ\n",
    "\n",
    "import os, io, json, torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from fashion_clip.fashion_clip import FashionCLIP\n",
    "\n",
    "# ==============================\n",
    "# 0) ÏÑ§Ï†ï\n",
    "# ==============================\n",
    "IMAGE_DIR = \"crop_29cm\"                       # cropÎêú Ïù¥ÎØ∏ÏßÄ Ìè¥Îçî\n",
    "CAPTIONS_CSV = \"29cm/skirt_caption_split.csv\"   # caption1, caption2 Ìè¨Ìï®Îêú CSV\n",
    "PRODUCT_INFO_CSV = \"29cm/29cm_1000.csv\"       # [ÏÉÅÌíàÏΩîÎìú, ÎåÄÎ∂ÑÎ•ò, ÏÜåÎ∂ÑÎ•ò]\n",
    "OUTPUT_CSV = \"29cm/skirt_embeddings_real_final.csv\"  # ÏµúÏ¢Ö ÏÇ∞Ï∂úÎ¨º\n",
    "\n",
    "# ÎîîÎ∞îÏù¥Ïä§ ÏÑ†ÌÉù\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "# Î™®Îç∏ Î°úÎìú\n",
    "fclip = FashionCLIP(\"fashion-clip\")\n",
    "try:\n",
    "    fclip.to(device)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ==============================\n",
    "# 1) Ïú†Ìã∏ Ìï®Ïàò\n",
    "# ==============================\n",
    "def l2_normalize(x: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=np.float32)\n",
    "    if x.ndim == 1:\n",
    "        x = x[None, :]\n",
    "    n = np.linalg.norm(x, axis=1, keepdims=True)\n",
    "    n = np.maximum(n, eps)\n",
    "    return (x / n).astype(np.float32)\n",
    "\n",
    "def _to_numpy_2d(x) -> np.ndarray:\n",
    "    if isinstance(x, np.ndarray):\n",
    "        arr = x\n",
    "    elif torch.is_tensor(x):\n",
    "        arr = x.detach().cpu().numpy()\n",
    "    else:\n",
    "        arr = np.asarray(x)\n",
    "    if arr.ndim == 1:\n",
    "        arr = arr[None, :]\n",
    "    return arr.astype(np.float32)\n",
    "\n",
    "def torch_empty_cache():\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "    elif device == \"mps\":\n",
    "        try:\n",
    "            torch.mps.empty_cache()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def determine_best_batch_size(start: int = 512) -> int:\n",
    "    \"\"\"OOM ÌîºÌïòÎ©¥ÏÑú Í∞ÄÏû• ÌÅ∞ batch_size ÏÑ†ÌÉù\"\"\"\n",
    "    if start >= 512:\n",
    "        candidates = [start,384,256,192,128,96,64,48,32,24,16,12,8,4,2,1]\n",
    "    elif start >= 256:\n",
    "        candidates = [start,192,128,96,64,48,32,24,16,12,8,4,2,1]\n",
    "    else:\n",
    "        candidates = [start,48,32,24,16,12,8,4,2,1]\n",
    "    for bs in candidates:\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                _ = fclip.encode_text([\"warmup\"] * bs, batch_size=bs)\n",
    "                dummy = Image.new(\"RGB\", (224,224), (255,255,255))\n",
    "                _ = fclip.encode_images([dummy] * bs, batch_size=bs)\n",
    "            print(f\">>> OK batch_size={bs}\")\n",
    "            return bs\n",
    "        except Exception as e:\n",
    "            print(f\"OOM/Fail at batch_size={bs} ‚Üí {e}\")\n",
    "            torch_empty_cache()\n",
    "    return 1\n",
    "\n",
    "# ==============================\n",
    "# 2) ÏûÑÎ≤†Îî© Ìï®Ïàò\n",
    "# ==============================\n",
    "def embed_in_batches(records, batch_size: int):\n",
    "    out = []\n",
    "    n = len(records)\n",
    "    total_batches = (n + batch_size - 1) // batch_size\n",
    "\n",
    "    for bi in range(total_batches):\n",
    "        s, e = bi*batch_size, min((bi+1)*batch_size, n)\n",
    "        chunk = records[s:e]\n",
    "\n",
    "        texts1 = [r[\"caption1\"] for r in chunk]\n",
    "        texts2 = [r[\"caption2\"] for r in chunk]\n",
    "        combined_texts = [r[\"caption1\"] + \" | \" + r[\"caption2\"] for r in chunk]\n",
    "        images = [Image.open(r[\"image_path\"]).convert(\"RGB\") for r in chunk]\n",
    "\n",
    "        try:\n",
    "            with torch.no_grad(), torch.autocast(device_type='cuda', dtype=torch.float16, enabled=(device=='cuda')):\n",
    "                # caption1, caption2 ÏûÑÎ≤†Îî©\n",
    "                t1_emb = fclip.encode_text(texts1, batch_size=len(texts1))\n",
    "                t2_emb = fclip.encode_text(texts2, batch_size=len(texts2))\n",
    "                # combined_text ÏûÑÎ≤†Îî© (information Ïª¨ÎüºÏö©)\n",
    "                t_comb_emb = fclip.encode_text(combined_texts, batch_size=len(combined_texts))\n",
    "                # Ïù¥ÎØ∏ÏßÄ ÏûÑÎ≤†Îî©\n",
    "                v_emb = fclip.encode_images(images, batch_size=len(images))\n",
    "        finally:\n",
    "            for img in images:\n",
    "                try: img.close()\n",
    "                except: pass\n",
    "\n",
    "        t1_np = _to_numpy_2d(t1_emb)\n",
    "        t2_np = _to_numpy_2d(t2_emb)\n",
    "        t_comb_np = _to_numpy_2d(t_comb_emb)\n",
    "        v_np  = _to_numpy_2d(v_emb)\n",
    "\n",
    "        # text-only = caption1 + caption2 ÌèâÍ∑†\n",
    "        text_only = l2_normalize((t1_np + t2_np) / 2)\n",
    "\n",
    "        # multi = text + image (0.5:0.5)\n",
    "        multi = l2_normalize(0.5*text_only + 0.5*l2_normalize(v_np))\n",
    "\n",
    "        # Í≤∞Í≥º Ï†ÄÏû• (caption1, caption2 ÎåÄÏã† informationÎßå Ï†ÄÏû•)\n",
    "        for r, info, t_vec, m_vec in zip(chunk, combined_texts, text_only, multi):\n",
    "            out.append({\n",
    "                \"id\": r[\"product_id\"],\n",
    "                \"category\": r[\"category\"],\n",
    "                \"type\": r[\"type\"],\n",
    "                \"information\": info,\n",
    "                \"text\": json.dumps(t_vec.tolist()),\n",
    "                \"multi\": json.dumps(m_vec.tolist())\n",
    "            })\n",
    "\n",
    "        torch_empty_cache()\n",
    "        print(f\"[EMBED] batch {bi+1}/{total_batches} ‚Üí {e}/{n}\")\n",
    "\n",
    "    return out\n",
    "\n",
    "# ==============================\n",
    "# 3) Î©îÏù∏\n",
    "# ==============================\n",
    "def main():\n",
    "    if not os.path.exists(CAPTIONS_CSV):\n",
    "        raise FileNotFoundError(f\"Ï∫°ÏÖò CSVÍ∞Ä ÏóÜÏäµÎãàÎã§: {CAPTIONS_CSV}\")\n",
    "    if not os.path.exists(PRODUCT_INFO_CSV):\n",
    "        raise FileNotFoundError(f\"ÏÉÅÌíàÏ†ïÎ≥¥ CSVÍ∞Ä ÏóÜÏäµÎãàÎã§: {PRODUCT_INFO_CSV}\")\n",
    "\n",
    "    captions_df = pd.read_csv(CAPTIONS_CSV)\n",
    "    product_df = pd.read_csv(PRODUCT_INFO_CSV, dtype=str).fillna(\"\")\n",
    "    product_df = product_df.apply(lambda col: col.str.strip().str.lower())\n",
    "\n",
    "    product_map = dict(zip(product_df[\"ÏÉÅÌíàÏΩîÎìú\"], zip(product_df[\"ÎåÄÎ∂ÑÎ•ò\"], product_df[\"ÏÜåÎ∂ÑÎ•ò\"])))\n",
    "\n",
    "    # ÌïúÍ∏Ä ‚Üí ÏòÅÏñ¥ Îß§Ìïë\n",
    "    major_map = {\"Ïä§Ïª§Ìä∏\": \"skirt\"}\n",
    "    sub_map = {\n",
    "        \"ÎØ∏ÎãàÏä§Ïª§Ìä∏\": \"miniskirt\",\n",
    "        \"ÎØ∏ÎîîÏä§Ïª§Ìä∏\": \"midiskirt\",\n",
    "        \"Î°±Ïä§Ïª§Ìä∏\": \"longskirt\",\n",
    "    }\n",
    "\n",
    "    records = []\n",
    "    for _, row in captions_df.iterrows():\n",
    "        pid = str(row[\"product_id\"]).lower()\n",
    "        caption1 = str(row[\"caption1\"])\n",
    "        caption2 = str(row[\"caption2\"])\n",
    "\n",
    "        # category/type Îß§Ìïë\n",
    "        raw_category, raw_type = product_map.get(pid, (\"unknown\",\"unknown\"))\n",
    "        category = major_map.get(raw_category, raw_category)\n",
    "        type_ = sub_map.get(raw_type, raw_type)\n",
    "\n",
    "        # Ïù¥ÎØ∏ÏßÄ Í≤ΩÎ°ú ÌÉêÏÉâ\n",
    "        img_path = None\n",
    "        for ext in [\".jpg\",\".jpeg\",\".png\",\".webp\",\".bmp\",\".jfif\"]:\n",
    "            candidate = os.path.join(IMAGE_DIR, f\"skirt_{pid}{ext}\")\n",
    "            if os.path.exists(candidate):\n",
    "                img_path = candidate\n",
    "                break\n",
    "        if not img_path:\n",
    "            continue\n",
    "\n",
    "        records.append({\n",
    "            \"product_id\": pid,\n",
    "            \"category\": category,\n",
    "            \"type\": type_,\n",
    "            \"caption1\": caption1,\n",
    "            \"caption2\": caption2,\n",
    "            \"image_path\": img_path\n",
    "        })\n",
    "\n",
    "    if not records:\n",
    "        raise RuntimeError(\"Ï≤òÎ¶¨Ìï† Î†àÏΩîÎìúÍ∞Ä ÏóÜÏäµÎãàÎã§.\")\n",
    "\n",
    "    # Î∞∞Ïπò ÌÅ¨Í∏∞ Í≤∞Ï†ï\n",
    "    start_bs = 512 if device in (\"cuda\",\"mps\") else 256\n",
    "    best_bs = determine_best_batch_size(start=start_bs)\n",
    "    print(f\">>> Using batch_size={best_bs} on {device}\")\n",
    "\n",
    "    embedded = embed_in_batches(records, batch_size=best_bs)\n",
    "\n",
    "    # Ï†ÄÏû• (ÏµúÏ¢Ö Ïª¨Îüº: id, category, type, information, text, multi)\n",
    "    df = pd.DataFrame(embedded, columns=[\"id\",\"category\",\"type\",\"information\",\"text\",\"multi\"])\n",
    "    df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "    print(f\"‚úÖ ÏûÑÎ≤†Îî© Ï†ÄÏû• ÏôÑÎ£å: {OUTPUT_CSV}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412ca308",
   "metadata": {},
   "source": [
    "## dress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa21803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Top Embedding Pipeline (caption1+caption2 ÌèâÍ∑† text ÏûÑÎ≤†Îî© + multi = text+image 0.5:0.5)\n",
    "# CSV Ï†ÄÏû• Ïãú information(ÏõêÎ≥∏ Ï∫°ÏÖò) Ïª¨ÎüºÎßå ÎÇ®ÍπÄ\n",
    "\n",
    "import os, io, json, torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from fashion_clip.fashion_clip import FashionCLIP\n",
    "\n",
    "# ==============================\n",
    "# 0) ÏÑ§Ï†ï\n",
    "# ==============================\n",
    "IMAGE_DIR = \"crop_29cm\"                       # cropÎêú Ïù¥ÎØ∏ÏßÄ Ìè¥Îçî\n",
    "CAPTIONS_CSV = \"29cm/dress_caption_split.csv\"   # caption1, caption2 Ìè¨Ìï®Îêú CSV\n",
    "PRODUCT_INFO_CSV = \"29cm/29cm_1000.csv\"       # [ÏÉÅÌíàÏΩîÎìú, ÎåÄÎ∂ÑÎ•ò, ÏÜåÎ∂ÑÎ•ò]\n",
    "OUTPUT_CSV = \"29cm/dress_embeddings_real_final.csv\"  # ÏµúÏ¢Ö ÏÇ∞Ï∂úÎ¨º\n",
    "\n",
    "# ÎîîÎ∞îÏù¥Ïä§ ÏÑ†ÌÉù\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "# Î™®Îç∏ Î°úÎìú\n",
    "fclip = FashionCLIP(\"fashion-clip\")\n",
    "try:\n",
    "    fclip.to(device)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ==============================\n",
    "# 1) Ïú†Ìã∏ Ìï®Ïàò\n",
    "# ==============================\n",
    "def l2_normalize(x: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=np.float32)\n",
    "    if x.ndim == 1:\n",
    "        x = x[None, :]\n",
    "    n = np.linalg.norm(x, axis=1, keepdims=True)\n",
    "    n = np.maximum(n, eps)\n",
    "    return (x / n).astype(np.float32)\n",
    "\n",
    "def _to_numpy_2d(x) -> np.ndarray:\n",
    "    if isinstance(x, np.ndarray):\n",
    "        arr = x\n",
    "    elif torch.is_tensor(x):\n",
    "        arr = x.detach().cpu().numpy()\n",
    "    else:\n",
    "        arr = np.asarray(x)\n",
    "    if arr.ndim == 1:\n",
    "        arr = arr[None, :]\n",
    "    return arr.astype(np.float32)\n",
    "\n",
    "def torch_empty_cache():\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "    elif device == \"mps\":\n",
    "        try:\n",
    "            torch.mps.empty_cache()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def determine_best_batch_size(start: int = 512) -> int:\n",
    "    \"\"\"OOM ÌîºÌïòÎ©¥ÏÑú Í∞ÄÏû• ÌÅ∞ batch_size ÏÑ†ÌÉù\"\"\"\n",
    "    if start >= 512:\n",
    "        candidates = [start,384,256,192,128,96,64,48,32,24,16,12,8,4,2,1]\n",
    "    elif start >= 256:\n",
    "        candidates = [start,192,128,96,64,48,32,24,16,12,8,4,2,1]\n",
    "    else:\n",
    "        candidates = [start,48,32,24,16,12,8,4,2,1]\n",
    "    for bs in candidates:\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                _ = fclip.encode_text([\"warmup\"] * bs, batch_size=bs)\n",
    "                dummy = Image.new(\"RGB\", (224,224), (255,255,255))\n",
    "                _ = fclip.encode_images([dummy] * bs, batch_size=bs)\n",
    "            print(f\">>> OK batch_size={bs}\")\n",
    "            return bs\n",
    "        except Exception as e:\n",
    "            print(f\"OOM/Fail at batch_size={bs} ‚Üí {e}\")\n",
    "            torch_empty_cache()\n",
    "    return 1\n",
    "\n",
    "# ==============================\n",
    "# 2) ÏûÑÎ≤†Îî© Ìï®Ïàò\n",
    "# ==============================\n",
    "def embed_in_batches(records, batch_size: int):\n",
    "    out = []\n",
    "    n = len(records)\n",
    "    total_batches = (n + batch_size - 1) // batch_size\n",
    "\n",
    "    for bi in range(total_batches):\n",
    "        s, e = bi*batch_size, min((bi+1)*batch_size, n)\n",
    "        chunk = records[s:e]\n",
    "\n",
    "        texts1 = [r[\"caption1\"] for r in chunk]\n",
    "        texts2 = [r[\"caption2\"] for r in chunk]\n",
    "        combined_texts = [r[\"caption1\"] + \" | \" + r[\"caption2\"] for r in chunk]\n",
    "        images = [Image.open(r[\"image_path\"]).convert(\"RGB\") for r in chunk]\n",
    "\n",
    "        try:\n",
    "            with torch.no_grad(), torch.autocast(device_type='cuda', dtype=torch.float16, enabled=(device=='cuda')):\n",
    "                # caption1, caption2 ÏûÑÎ≤†Îî©\n",
    "                t1_emb = fclip.encode_text(texts1, batch_size=len(texts1))\n",
    "                t2_emb = fclip.encode_text(texts2, batch_size=len(texts2))\n",
    "                # combined_text ÏûÑÎ≤†Îî© (information Ïª¨ÎüºÏö©)\n",
    "                t_comb_emb = fclip.encode_text(combined_texts, batch_size=len(combined_texts))\n",
    "                # Ïù¥ÎØ∏ÏßÄ ÏûÑÎ≤†Îî©\n",
    "                v_emb = fclip.encode_images(images, batch_size=len(images))\n",
    "        finally:\n",
    "            for img in images:\n",
    "                try: img.close()\n",
    "                except: pass\n",
    "\n",
    "        t1_np = _to_numpy_2d(t1_emb)\n",
    "        t2_np = _to_numpy_2d(t2_emb)\n",
    "        t_comb_np = _to_numpy_2d(t_comb_emb)\n",
    "        v_np  = _to_numpy_2d(v_emb)\n",
    "\n",
    "        # text-only = caption1 + caption2 ÌèâÍ∑†\n",
    "        text_only = l2_normalize((t1_np + t2_np) / 2)\n",
    "\n",
    "        # multi = text + image (0.5:0.5)\n",
    "        multi = l2_normalize(0.5*text_only + 0.5*l2_normalize(v_np))\n",
    "\n",
    "        # Í≤∞Í≥º Ï†ÄÏû• (caption1, caption2 ÎåÄÏã† informationÎßå Ï†ÄÏû•)\n",
    "        for r, info, t_vec, m_vec in zip(chunk, combined_texts, text_only, multi):\n",
    "            out.append({\n",
    "                \"id\": r[\"product_id\"],\n",
    "                \"category\": r[\"category\"],\n",
    "                \"type\": r[\"type\"],\n",
    "                \"information\": info,\n",
    "                \"text\": json.dumps(t_vec.tolist()),\n",
    "                \"multi\": json.dumps(m_vec.tolist())\n",
    "            })\n",
    "\n",
    "        torch_empty_cache()\n",
    "        print(f\"[EMBED] batch {bi+1}/{total_batches} ‚Üí {e}/{n}\")\n",
    "\n",
    "    return out\n",
    "\n",
    "# ==============================\n",
    "# 3) Î©îÏù∏\n",
    "# ==============================\n",
    "def main():\n",
    "    if not os.path.exists(CAPTIONS_CSV):\n",
    "        raise FileNotFoundError(f\"Ï∫°ÏÖò CSVÍ∞Ä ÏóÜÏäµÎãàÎã§: {CAPTIONS_CSV}\")\n",
    "    if not os.path.exists(PRODUCT_INFO_CSV):\n",
    "        raise FileNotFoundError(f\"ÏÉÅÌíàÏ†ïÎ≥¥ CSVÍ∞Ä ÏóÜÏäµÎãàÎã§: {PRODUCT_INFO_CSV}\")\n",
    "\n",
    "    captions_df = pd.read_csv(CAPTIONS_CSV)\n",
    "    product_df = pd.read_csv(PRODUCT_INFO_CSV, dtype=str).fillna(\"\")\n",
    "    product_df = product_df.apply(lambda col: col.str.strip().str.lower())\n",
    "\n",
    "    product_map = dict(zip(product_df[\"ÏÉÅÌíàÏΩîÎìú\"], zip(product_df[\"ÎåÄÎ∂ÑÎ•ò\"], product_df[\"ÏÜåÎ∂ÑÎ•ò\"])))\n",
    "\n",
    "    # ÌïúÍ∏Ä ‚Üí ÏòÅÏñ¥ Îß§Ìïë\n",
    "    major_map = {\"ÏõêÌîºÏä§\":\"dress\"}\n",
    "    sub_map = {\n",
    "        \"ÎØ∏ÎãàÏõêÌîºÏä§\":\"minidress\",\n",
    "        \"ÎØ∏ÎîîÏõêÌîºÏä§\":\"mididress\",\n",
    "        \"Îß•ÏãúÏõêÌîºÏä§\":\"maxidress\",\n",
    "    }\n",
    "\n",
    "    records = []\n",
    "    for _, row in captions_df.iterrows():\n",
    "        pid = str(row[\"product_id\"]).lower()\n",
    "        caption1 = str(row[\"caption1\"])\n",
    "        caption2 = str(row[\"caption2\"])\n",
    "\n",
    "        # category/type Îß§Ìïë\n",
    "        raw_category, raw_type = product_map.get(pid, (\"unknown\",\"unknown\"))\n",
    "        category = major_map.get(raw_category, raw_category)\n",
    "        type_ = sub_map.get(raw_type, raw_type)\n",
    "\n",
    "        # Ïù¥ÎØ∏ÏßÄ Í≤ΩÎ°ú ÌÉêÏÉâ\n",
    "        img_path = None\n",
    "        for ext in [\".jpg\",\".jpeg\",\".png\",\".webp\",\".bmp\",\".jfif\"]:\n",
    "            candidate = os.path.join(IMAGE_DIR, f\"dress_{pid}{ext}\")\n",
    "            if os.path.exists(candidate):\n",
    "                img_path = candidate\n",
    "                break\n",
    "        if not img_path:\n",
    "            continue\n",
    "\n",
    "        records.append({\n",
    "            \"product_id\": pid,\n",
    "            \"category\": category,\n",
    "            \"type\": type_,\n",
    "            \"caption1\": caption1,\n",
    "            \"caption2\": caption2,\n",
    "            \"image_path\": img_path\n",
    "        })\n",
    "\n",
    "    if not records:\n",
    "        raise RuntimeError(\"Ï≤òÎ¶¨Ìï† Î†àÏΩîÎìúÍ∞Ä ÏóÜÏäµÎãàÎã§.\")\n",
    "\n",
    "    # Î∞∞Ïπò ÌÅ¨Í∏∞ Í≤∞Ï†ï\n",
    "    start_bs = 512 if device in (\"cuda\",\"mps\") else 256\n",
    "    best_bs = determine_best_batch_size(start=start_bs)\n",
    "    print(f\">>> Using batch_size={best_bs} on {device}\")\n",
    "\n",
    "    embedded = embed_in_batches(records, batch_size=best_bs)\n",
    "\n",
    "    # Ï†ÄÏû• (ÏµúÏ¢Ö Ïª¨Îüº: id, category, type, information, text, multi)\n",
    "    df = pd.DataFrame(embedded, columns=[\"id\",\"category\",\"type\",\"information\",\"text\",\"multi\"])\n",
    "    df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
    "    print(f\"‚úÖ ÏûÑÎ≤†Îî© Ï†ÄÏû• ÏôÑÎ£å: {OUTPUT_CSV}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93d1b99",
   "metadata": {},
   "source": [
    "# Qdrant upload : Qdrant Ïóê Îç∞Ïù¥ÌÑ∞ ÏÇΩÏûÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549c00e8",
   "metadata": {},
   "source": [
    "## top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b13c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import os\n",
    "from qdrant_client import QdrantClient, models\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 0. .env Î°úÎìú\n",
    "load_dotenv()\n",
    "\n",
    "# 1. Îç∞Ïù¥ÌÑ∞ Î∂àÎü¨Ïò§Í∏∞\n",
    "df = pd.read_csv(\"29cm/top_embeddings_real_final.csv\")\n",
    "\n",
    "# 2. ÌôòÍ≤ΩÎ≥ÄÏàòÏóêÏÑú QDRANT_API_KEY Î∂àÎü¨Ïò§Í∏∞\n",
    "QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")\n",
    "\n",
    "# 3. ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Ïó∞Í≤∞\n",
    "client = QdrantClient(\n",
    "    url=\"http://43.201.185.192:6333\",\n",
    "    api_key=QDRANT_API_KEY,\n",
    "    prefer_grpc=False,\n",
    "    timeout=120.0\n",
    ")\n",
    "\n",
    "# 4. Ïª¨Î†âÏÖò Ïù¥Î¶Ñ\n",
    "COLLECTION_NAME = \"ivlle\"\n",
    "\n",
    "# 5. Î∞∞Ïπò ÏóÖÏÑúÌä∏\n",
    "BATCH = 500\n",
    "total = len(df)\n",
    "\n",
    "for i in range(0, total, BATCH):\n",
    "    sl = df.iloc[i:i+BATCH]\n",
    "    points = []\n",
    "    for _, row in sl.iterrows():\n",
    "        payload = {\n",
    "            \"category\": row[\"category\"],\n",
    "            \"type\": row[\"type\"],\n",
    "            \"information\": row[\"information\"],\n",
    "        }\n",
    "        vec_text = ast.literal_eval(row[\"text\"])   # Î¨∏ÏûêÏó¥ ‚Üí Î¶¨Ïä§Ìä∏ Î≥ÄÌôò\n",
    "        vec_multi = ast.literal_eval(row[\"multi\"])\n",
    "        \n",
    "        points.append(\n",
    "            models.PointStruct(\n",
    "                id=int(row[\"id\"]),\n",
    "                vector={\n",
    "                    \"text\": vec_text,\n",
    "                    \"multi\": vec_multi,\n",
    "                },\n",
    "                payload=payload\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    client.upsert(collection_name=COLLECTION_NAME, points=points)\n",
    "    print(f\"‚úÖ Uploaded {i+len(sl)}/{total}\")\n",
    "\n",
    "print(\"üéâ Î™®Îì† Îç∞Ïù¥ÌÑ∞ ÏóÖÏÑúÌä∏ ÏôÑÎ£å\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747522f8",
   "metadata": {},
   "source": [
    "## pants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d519b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import os\n",
    "from qdrant_client import QdrantClient, models\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 0. .env Î°úÎìú\n",
    "load_dotenv()\n",
    "\n",
    "# 1. Îç∞Ïù¥ÌÑ∞ Î∂àÎü¨Ïò§Í∏∞\n",
    "df = pd.read_csv(\"29cm/pants_embeddings_real_final.csv\")\n",
    "\n",
    "# 2. ÌôòÍ≤ΩÎ≥ÄÏàòÏóêÏÑú QDRANT_API_KEY Î∂àÎü¨Ïò§Í∏∞\n",
    "QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")\n",
    "\n",
    "# 3. ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Ïó∞Í≤∞\n",
    "client = QdrantClient(\n",
    "    url=\"http://43.201.185.192:6333\",\n",
    "    api_key=QDRANT_API_KEY,\n",
    "    prefer_grpc=False,\n",
    "    timeout=120.0\n",
    ")\n",
    "\n",
    "# 4. Ïª¨Î†âÏÖò Ïù¥Î¶Ñ\n",
    "COLLECTION_NAME = \"ivlle\"\n",
    "\n",
    "# 5. Î∞∞Ïπò ÏóÖÏÑúÌä∏\n",
    "BATCH = 500\n",
    "total = len(df)\n",
    "\n",
    "for i in range(0, total, BATCH):\n",
    "    sl = df.iloc[i:i+BATCH]\n",
    "    points = []\n",
    "    for _, row in sl.iterrows():\n",
    "        payload = {\n",
    "            \"category\": row[\"category\"],\n",
    "            \"type\": row[\"type\"],\n",
    "            \"information\": row[\"information\"],\n",
    "        }\n",
    "        vec_text = ast.literal_eval(row[\"text\"])   # Î¨∏ÏûêÏó¥ ‚Üí Î¶¨Ïä§Ìä∏ Î≥ÄÌôò\n",
    "        vec_multi = ast.literal_eval(row[\"multi\"])\n",
    "        \n",
    "        points.append(\n",
    "            models.PointStruct(\n",
    "                id=int(row[\"id\"]),\n",
    "                vector={\n",
    "                    \"text\": vec_text,\n",
    "                    \"multi\": vec_multi,\n",
    "                },\n",
    "                payload=payload\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    client.upsert(collection_name=COLLECTION_NAME, points=points)\n",
    "    print(f\"‚úÖ Uploaded {i+len(sl)}/{total}\")\n",
    "\n",
    "print(\"üéâ Î™®Îì† Îç∞Ïù¥ÌÑ∞ ÏóÖÏÑúÌä∏ ÏôÑÎ£å\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a204da6",
   "metadata": {},
   "source": [
    "## skirt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68d15b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import os\n",
    "from qdrant_client import QdrantClient, models\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 0. .env Î°úÎìú\n",
    "load_dotenv()\n",
    "\n",
    "# 1. Îç∞Ïù¥ÌÑ∞ Î∂àÎü¨Ïò§Í∏∞\n",
    "df = pd.read_csv(\"29cm/skirt_embeddings_real_final.csv\")\n",
    "\n",
    "# 2. ÌôòÍ≤ΩÎ≥ÄÏàòÏóêÏÑú QDRANT_API_KEY Î∂àÎü¨Ïò§Í∏∞\n",
    "QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")\n",
    "\n",
    "# 3. ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Ïó∞Í≤∞\n",
    "client = QdrantClient(\n",
    "    url=\"http://43.201.185.192:6333\",\n",
    "    api_key=QDRANT_API_KEY,\n",
    "    prefer_grpc=False,\n",
    "    timeout=120.0\n",
    ")\n",
    "\n",
    "# 4. Ïª¨Î†âÏÖò Ïù¥Î¶Ñ\n",
    "COLLECTION_NAME = \"ivlle\"\n",
    "\n",
    "# 5. Î∞∞Ïπò ÏóÖÏÑúÌä∏\n",
    "BATCH = 500\n",
    "total = len(df)\n",
    "\n",
    "for i in range(0, total, BATCH):\n",
    "    sl = df.iloc[i:i+BATCH]\n",
    "    points = []\n",
    "    for _, row in sl.iterrows():\n",
    "        payload = {\n",
    "            \"category\": row[\"category\"],\n",
    "            \"type\": row[\"type\"],\n",
    "            \"information\": row[\"information\"],\n",
    "        }\n",
    "        vec_text = ast.literal_eval(row[\"text\"])   # Î¨∏ÏûêÏó¥ ‚Üí Î¶¨Ïä§Ìä∏ Î≥ÄÌôò\n",
    "        vec_multi = ast.literal_eval(row[\"multi\"])\n",
    "        \n",
    "        points.append(\n",
    "            models.PointStruct(\n",
    "                id=int(row[\"id\"]),\n",
    "                vector={\n",
    "                    \"text\": vec_text,\n",
    "                    \"multi\": vec_multi,\n",
    "                },\n",
    "                payload=payload\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    client.upsert(collection_name=COLLECTION_NAME, points=points)\n",
    "    print(f\"‚úÖ Uploaded {i+len(sl)}/{total}\")\n",
    "\n",
    "print(\"üéâ Î™®Îì† Îç∞Ïù¥ÌÑ∞ ÏóÖÏÑúÌä∏ ÏôÑÎ£å\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05cb398",
   "metadata": {},
   "source": [
    "## dress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbd4b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import os\n",
    "from qdrant_client import QdrantClient, models\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 0. .env Î°úÎìú\n",
    "load_dotenv()\n",
    "\n",
    "# 1. Îç∞Ïù¥ÌÑ∞ Î∂àÎü¨Ïò§Í∏∞\n",
    "df = pd.read_csv(\"29cm/dress_embeddings_real_final.csv\")\n",
    "\n",
    "# 2. ÌôòÍ≤ΩÎ≥ÄÏàòÏóêÏÑú QDRANT_API_KEY Î∂àÎü¨Ïò§Í∏∞\n",
    "QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")\n",
    "\n",
    "# 3. ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Ïó∞Í≤∞\n",
    "client = QdrantClient(\n",
    "    url=\"http://43.201.185.192:6333\",\n",
    "    api_key=QDRANT_API_KEY,\n",
    "    prefer_grpc=False,\n",
    "    timeout=120.0\n",
    ")\n",
    "\n",
    "# 4. Ïª¨Î†âÏÖò Ïù¥Î¶Ñ\n",
    "COLLECTION_NAME = \"ivlle\"\n",
    "\n",
    "# 5. Î∞∞Ïπò ÏóÖÏÑúÌä∏\n",
    "BATCH = 500\n",
    "total = len(df)\n",
    "\n",
    "for i in range(0, total, BATCH):\n",
    "    sl = df.iloc[i:i+BATCH]\n",
    "    points = []\n",
    "    for _, row in sl.iterrows():\n",
    "        payload = {\n",
    "            \"category\": row[\"category\"],\n",
    "            \"type\": row[\"type\"],\n",
    "            \"information\": row[\"information\"],\n",
    "        }\n",
    "        vec_text = ast.literal_eval(row[\"text\"])   # Î¨∏ÏûêÏó¥ ‚Üí Î¶¨Ïä§Ìä∏ Î≥ÄÌôò\n",
    "        vec_multi = ast.literal_eval(row[\"multi\"])\n",
    "        \n",
    "        points.append(\n",
    "            models.PointStruct(\n",
    "                id=int(row[\"id\"]),\n",
    "                vector={\n",
    "                    \"text\": vec_text,\n",
    "                    \"multi\": vec_multi,\n",
    "                },\n",
    "                payload=payload\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    client.upsert(collection_name=COLLECTION_NAME, points=points)\n",
    "    print(f\"‚úÖ Uploaded {i+len(sl)}/{total}\")\n",
    "\n",
    "print(\"üéâ Î™®Îì† Îç∞Ïù¥ÌÑ∞ ÏóÖÏÑúÌä∏ ÏôÑÎ£å\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
