from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.dummy import DummyOperator
from airflow.operators.trigger_dagrun import TriggerDagRunOperator
from airflow.models import Variable
import os
import time
import re
import csv
import boto3
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from selenium_stealth import stealth

# DAG ê¸°ë³¸ ì„¤ì •
default_args = {
    'owner': 'data_team',
    'depends_on_past': False,
    'start_date': datetime(2023, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=2),
    'catchup': False
}

# DAG ì •ì˜
dag = DAG(
    'wconcept_product_crawling',
    default_args=default_args,
    description='Wì»¨ì…‰ ìƒí’ˆ ë§í¬ í¬ë¡¤ë§ DAG',
    schedule_interval=None,  # ìˆ˜ë™ ì‹¤í–‰
    max_active_runs=1,
    tags=['crawling', 'wconcept', 'ecommerce']
)

class S3Uploader:
    """S3 ì—…ë¡œë“œ ìœ í‹¸ë¦¬í‹° (ë§í¬ CSV ì—…ë¡œë“œìš©)"""
    def __init__(self):
        try:
            self.aws_access_key_id = Variable.get("AWS_ACCESS_KEY_ID", default_var=None)
            self.aws_secret_access_key = Variable.get("AWS_SECRET_ACCESS_KEY", default_var=None)
            self.aws_region = Variable.get("AWS_REGION", default_var="ap-northeast-2")
            self.bucket_name = Variable.get("S3_BUCKET_NAME", default_var="ivle-malle")
        except Exception:
            self.aws_access_key_id = None
            self.aws_secret_access_key = None
            self.aws_region = "ap-northeast-2"
            self.bucket_name = "ivle-malle"

        if not self.aws_access_key_id or not self.aws_secret_access_key:
            print("âŒ AWS í‚¤ê°€ ì—†ì–´ S3 ì—…ë¡œë“œë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            self.s3_client = None
            return

        try:
            self.s3_client = boto3.client(
                's3',
                aws_access_key_id=self.aws_access_key_id,
                aws_secret_access_key=self.aws_secret_access_key,
                region_name=self.aws_region
            )
            print(f"âœ… S3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”: {self.bucket_name}")
        except Exception as e:
            print(f"âŒ S3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.s3_client = None

def setup_chrome_driver():
    """Chrome ë“œë¼ì´ë²„ ì„¤ì •"""
    options = webdriver.ChromeOptions()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--disable-gpu')
    options.add_argument('--window-size=1920,1080')
    options.add_argument('--disable-blink-features=AutomationControlled')
    options.add_argument('--disable-extensions')
    options.add_argument('--disable-plugins')
    # options.add_argument('--disable-images')  # ì´ë¯¸ì§€ ë¡œë”© í™œì„±í™” (ìƒí’ˆ ê°ì§€ìš©)
    options.add_experimental_option('excludeSwitches', ['enable-logging'])
    options.add_experimental_option('useAutomationExtension', False)
    
    # íƒ€ì„ì•„ì›ƒ ì„¤ì •
    options.add_argument('--page-load-strategy=eager')  # DOM ë¡œë”© ì™„ë£Œì‹œ ë°”ë¡œ ë‹¤ìŒ ë‹¨ê³„
    
    driver = webdriver.Chrome(options=options)
    driver.set_page_load_timeout(30)  # í˜ì´ì§€ ë¡œë”© íƒ€ì„ì•„ì›ƒ 30ì´ˆ
    
    # ë´‡ ê°ì§€ ìš°íšŒ ì„¤ì •
    stealth(driver,
            languages=["ko-KR", "ko"],
            vendor="Google Inc.",
            platform="Win32",
            webgl_vendor="Intel Inc.",
            renderer="Intel Iris OpenGL Engine",
            fix_hairline=True,
            )
    
    return driver

def wait_for_products_to_load(driver, max_wait=20):
    """ìƒí’ˆë“¤ì´ ì™„ì „íˆ ë¡œë”©ë  ë•Œê¹Œì§€ ëŒ€ê¸°"""
    print("ğŸ”„ ìƒí’ˆ ë¡œë”© ëŒ€ê¸° ì¤‘...")
    
    # Wì»¨ì…‰ ì‚¬ì´íŠ¸ì— íŠ¹í™”ëœ ì„ íƒìë“¤
    selectors_to_try = [
        "div.product-item",
        ".product-card", 
        ".item",
        ".goods-item",
        ".product",
        "a[href*='/Product/']",
        "[data-product-id]",
        "[data-goods-id]",
        ".goods-list .item",  # Wì»¨ì…‰ íŠ¹í™”
        ".product-list .item",  # Wì»¨ì…‰ íŠ¹í™”
        ".goods-list li",  # Wì»¨ì…‰ íŠ¹í™”
        ".product-list li",  # Wì»¨ì…‰ íŠ¹í™”
        "[class*='product']",  # productê°€ í¬í•¨ëœ í´ë˜ìŠ¤
        "[class*='goods']",  # goodsê°€ í¬í•¨ëœ í´ë˜ìŠ¤
        "[class*='item']"  # itemì´ í¬í•¨ëœ í´ë˜ìŠ¤
    ]
    
    for selector in selectors_to_try:
        try:
            # ìµœì†Œ 5ê°œ ì´ìƒì˜ ìƒí’ˆì´ ë¡œë”©ë  ë•Œê¹Œì§€ ëŒ€ê¸° (ê¸°ì¤€ ë‚®ì¶¤)
            WebDriverWait(driver, max_wait).until(
                lambda d: len(d.find_elements(By.CSS_SELECTOR, selector)) >= 5
            )
            count = len(driver.find_elements(By.CSS_SELECTOR, selector))
            print(f"âœ… '{selector}' ì„ íƒìë¡œ ìƒí’ˆ ë¡œë”© ì™„ë£Œ ({count}ê°œ)")
            return selector
        except:
            continue
    
    # ë§ˆì§€ë§‰ ì‹œë„: ë‹¨ìˆœíˆ ìƒí’ˆ ìš”ì†Œê°€ ìˆëŠ”ì§€ë§Œ í™•ì¸
    for selector in selectors_to_try:
        try:
            WebDriverWait(driver, 5).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, selector))
            )
            count = len(driver.find_elements(By.CSS_SELECTOR, selector))
            print(f"âš ï¸ '{selector}' ì„ íƒìë¡œ ê¸°ë³¸ ë¡œë”© ì™„ë£Œ ({count}ê°œ)")
            return selector
        except:
            continue
    
    print("âŒ ìƒí’ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    return None

def scroll_to_load_all_products(driver, expected_count=60):
    """ëª¨ë“  ìƒí’ˆì´ ë¡œë”©ë  ë•Œê¹Œì§€ ìŠ¤í¬ë¡¤"""
    print(f"ğŸ”„ ëª¨ë“  ìƒí’ˆ ë¡œë”©ì„ ìœ„í•œ ìŠ¤í¬ë¡¤ ì‹œì‘ (ì˜ˆìƒ: {expected_count}ê°œ)")
    
    scroll_pause_time = 3  # ëŒ€ê¸° ì‹œê°„ ì¦ê°€
    max_scrolls = 100  # ìµœëŒ€ ìŠ¤í¬ë¡¤ íšŸìˆ˜ ëŒ€í­ ì¦ê°€
    last_product_count = 0
    no_change_count = 0
    
    # Wì»¨ì…‰ íŠ¹í™” ì„ íƒìë“¤ (ì •í™•í•œ ìˆœì„œë¡œ)
    initial_selectors = [
        ".goods-list .item",  # Wì»¨ì…‰ íŠ¹í™” (ê°€ì¥ ì •í™•)
        ".product-list .item",  # Wì»¨ì…‰ íŠ¹í™”
        ".goods-list li",  # Wì»¨ì…‰ íŠ¹í™”
        ".product-list li",  # Wì»¨ì…‰ íŠ¹í™”
        "a[href*='/Product/']",  # ìƒí’ˆ ë§í¬ê°€ ìˆëŠ” ìš”ì†Œ
        "[data-product-id]",  # ìƒí’ˆ IDê°€ ìˆëŠ” ìš”ì†Œ
        "[data-goods-id]",  # ìƒí’ˆ IDê°€ ìˆëŠ” ìš”ì†Œ
        "div.product-item",
        ".product-card", 
        ".goods-item",
        ".product",
        "[class*='product']",  # productê°€ í¬í•¨ëœ í´ë˜ìŠ¤
        "[class*='goods']",  # goodsê°€ í¬í•¨ëœ í´ë˜ìŠ¤
        "[class*='item']"  # itemì´ í¬í•¨ëœ í´ë˜ìŠ¤
    ]
    
    # ì´ˆê¸° ìƒí’ˆ ê°œìˆ˜ í™•ì¸
    for selector in initial_selectors:
        elements = driver.find_elements(By.CSS_SELECTOR, selector)
        if elements:
            last_product_count = len(elements)
            print(f"ğŸ“ ì´ˆê¸° {selector}: {last_product_count}ê°œ")
            break
    
    # 1ë‹¨ê³„: ì„¸ë°€í•œ ìŠ¤í¬ë¡¤ (6ê°œì”© 10ì¤„ = 60ê°œë¥¼ ìœ„í•´)
    print("ğŸ”„ 1ë‹¨ê³„: ì„¸ë°€í•œ ìŠ¤í¬ë¡¤ ì‹œì‘...")
    for i in range(max_scrolls):
        # í˜„ì¬ í˜ì´ì§€ ë†’ì´ í™•ì¸
        current_height = driver.execute_script("return document.body.scrollHeight;")
        viewport_height = driver.execute_script("return window.innerHeight;")
        
        # ë” ì„¸ë°€í•œ ìŠ¤í¬ë¡¤ (í•œ ì¤„ì”© ë¡œë”©ë˜ë„ë¡)
        scroll_step = viewport_height * 0.3  # ìŠ¤í¬ë¡¤ ê°„ê²©ì„ ë” ì¤„ì„
        target_scroll = scroll_step * (i + 1)
        
        # ìŠ¤í¬ë¡¤ ì‹¤í–‰
        driver.execute_script(f"window.scrollTo(0, {target_scroll});")
        print(f"ğŸ“œ ìŠ¤í¬ë¡¤ {i+1}/{max_scrolls}: {target_scroll}px")
        time.sleep(scroll_pause_time)
        
        # ìƒˆë¡œìš´ ìƒí’ˆ ê°œìˆ˜ í™•ì¸
        current_count = 0
        for selector in initial_selectors:
            elements = driver.find_elements(By.CSS_SELECTOR, selector)
            if elements:
                current_count = len(elements)
                break
        
        # ìƒí’ˆ ê°œìˆ˜ ë³€í™” í™•ì¸
        if current_count == last_product_count:
            no_change_count += 1
            print(f"âš ï¸ ìƒí’ˆ ê°œìˆ˜ ë³€í™” ì—†ìŒ ({current_count}ê°œ) - {no_change_count}/10")
            if no_change_count >= 10:  # 10ë²ˆ ì—°ì† ë³€í™” ì—†ìœ¼ë©´ ì¢…ë£Œ
                print(f"âœ… ìƒí’ˆ ê°œìˆ˜ ë³€í™” ì—†ìŒ ({current_count}ê°œ), ìŠ¤í¬ë¡¤ ì¢…ë£Œ")
                break
        else:
            no_change_count = 0
            print(f"ğŸ“ˆ ìƒí’ˆ ê°œìˆ˜ ì¦ê°€: {last_product_count} â†’ {current_count}")
        
        last_product_count = current_count
        
        # ì˜ˆìƒ ê°œìˆ˜ì— ë„ë‹¬í•˜ë©´ ì¢…ë£Œ
        if current_count >= expected_count:
            print(f"âœ… ì˜ˆìƒ ê°œìˆ˜({expected_count}ê°œ) ë„ë‹¬, ìŠ¤í¬ë¡¤ ì¢…ë£Œ")
            break
        
        # í˜ì´ì§€ ëì— ë„ë‹¬í–ˆëŠ”ì§€ í™•ì¸
        new_height = driver.execute_script("return document.body.scrollHeight;")
        if new_height == current_height and target_scroll >= current_height:
            print(f"âœ… í˜ì´ì§€ ëì— ë„ë‹¬, ìŠ¤í¬ë¡¤ ì¢…ë£Œ")
            break
    
    # 2ë‹¨ê³„: ê°•í™”ëœ ìŠ¤í¬ë¡¤ (ë” ì„¸ë°€í•˜ê²Œ)
    print("ğŸ”„ 2ë‹¨ê³„: ê°•í™”ëœ ìŠ¤í¬ë¡¤ ì‹œì‘...")
    for i in range(20):  # íšŸìˆ˜ ì¦ê°€
        # í˜ì´ì§€ ë§¨ ì•„ë˜ê¹Œì§€ ìŠ¤í¬ë¡¤
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(2)
        
        # ìœ„ë¡œ ì¡°ê¸ˆ ì˜¬ë¼ê°€ì„œ ë‹¤ì‹œ ìŠ¤í¬ë¡¤
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight - 500);")
        time.sleep(1)
        
        # ë‹¤ì‹œ ë§¨ ì•„ë˜ë¡œ
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(2)
        
        # ìƒí’ˆ ê°œìˆ˜ ì¬í™•ì¸
        current_count = 0
        for selector in initial_selectors:
            elements = driver.find_elements(By.CSS_SELECTOR, selector)
            if elements:
                current_count = len(elements)
                break
        
        print(f"ğŸ”„ ê°•í™” ìŠ¤í¬ë¡¤ {i+1}/20: {current_count}ê°œ")
        
        if current_count > last_product_count:
            last_product_count = current_count
            no_change_count = 0
        else:
            no_change_count += 1
            if no_change_count >= 5:  # 5ë²ˆ ì—°ì† ë³€í™” ì—†ìœ¼ë©´ ì¢…ë£Œ
                break
    
    # 3ë‹¨ê³„: ë¬´í•œ ìŠ¤í¬ë¡¤ ì‹œë®¬ë ˆì´ì…˜
    print("ğŸ”„ 3ë‹¨ê³„: ë¬´í•œ ìŠ¤í¬ë¡¤ ì‹œë®¬ë ˆì´ì…˜...")
    for i in range(10):  # íšŸìˆ˜ ì¦ê°€
        # í‚¤ë³´ë“œ End í‚¤ ì‹œë®¬ë ˆì´ì…˜
        from selenium.webdriver.common.keys import Keys
        driver.find_element(By.TAG_NAME, "body").send_keys(Keys.END)
        time.sleep(3)
        
        # ë§ˆìš°ìŠ¤ íœ  ìŠ¤í¬ë¡¤ ì‹œë®¬ë ˆì´ì…˜
        driver.execute_script("window.scrollBy(0, 500);")
        time.sleep(2)
        
        # ìƒí’ˆ ê°œìˆ˜ ì¬í™•ì¸
        current_count = 0
        for selector in initial_selectors:
            elements = driver.find_elements(By.CSS_SELECTOR, selector)
            if elements:
                current_count = len(elements)
                break
        
        print(f"ğŸ”„ ë¬´í•œ ìŠ¤í¬ë¡¤ {i+1}/10: {current_count}ê°œ")
        
        if current_count > last_product_count:
            last_product_count = current_count
        else:
            no_change_count += 1
            if no_change_count >= 3:
                break
    
    # 4ë‹¨ê³„: ìµœì¢… ê°•í™” ìŠ¤í¬ë¡¤
    print("ğŸ”„ 4ë‹¨ê³„: ìµœì¢… ê°•í™” ìŠ¤í¬ë¡¤...")
    for i in range(5):
        # ì²œì²œíˆ ë§¨ ì•„ë˜ê¹Œì§€ ìŠ¤í¬ë¡¤
        total_height = driver.execute_script("return document.body.scrollHeight;")
        for j in range(0, total_height, 200):
            driver.execute_script(f"window.scrollTo(0, {j});")
            time.sleep(0.5)
        
        # ë§¨ ì•„ë˜ì—ì„œ ì ì‹œ ëŒ€ê¸°
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(3)
        
        # ìƒí’ˆ ê°œìˆ˜ ì¬í™•ì¸
        current_count = 0
        for selector in initial_selectors:
            elements = driver.find_elements(By.CSS_SELECTOR, selector)
            if elements:
                current_count = len(elements)
                break
        
        print(f"ğŸ”„ ìµœì¢… ê°•í™” ìŠ¤í¬ë¡¤ {i+1}/5: {current_count}ê°œ")
        
        if current_count > last_product_count:
            last_product_count = current_count
        else:
            break
    
    # ìµœì¢… ìƒí’ˆ ê°œìˆ˜ í™•ì¸
    final_count = 0
    for selector in initial_selectors:
        elements = driver.find_elements(By.CSS_SELECTOR, selector)
        if elements:
            final_count = len(elements)
            break
    
    print(f"ğŸ¯ ìµœì¢… ìƒí’ˆ ê°œìˆ˜: {final_count}ê°œ")
    return final_count

def extract_product_links(soup, category_name):
    """BeautifulSoupì„ ì‚¬ìš©í•˜ì—¬ ìƒí’ˆ ë§í¬ ì¶”ì¶œ"""
    all_links = []
    
    # Wì»¨ì…‰ ìƒí’ˆ ë§í¬ ê²€ì¦ í•¨ìˆ˜
    def is_valid_product_link(link):
        """ìœ íš¨í•œ ìƒí’ˆ ë§í¬ì¸ì§€ ê²€ì¦"""
        if not link or not isinstance(link, str):
            return False
        
        # Wì»¨ì…‰ ìƒí’ˆ ë§í¬ íŒ¨í„´ í™•ì¸
        if not link.startswith('https://www.wconcept.co.kr/Product/'):
            return False
        
        # ìƒí’ˆ IDê°€ ìˆ«ìì¸ì§€ í™•ì¸
        product_id = link.split('/Product/')[-1]
        if not product_id.isdigit():
            return False
        
        # ìƒí’ˆ ID ê¸¸ì´ í™•ì¸ (ë³´í†µ 6-10ìë¦¬)
        if len(product_id) < 6 or len(product_id) > 10:
            return False
        
        return True
    
    # ì •í™•í•œ ìƒí’ˆ ë§í¬ ì¶”ì¶œ ë°©ë²•ë“¤
    extraction_methods = [
        # ë°©ë²• 1: ì§ì ‘ì ì¸ ìƒí’ˆ ë§í¬ ì¶”ì¶œ (ê°€ì¥ ì •í™•)
        lambda: [
            link['href'] if link['href'].startswith('http') else "https://www.wconcept.co.kr" + link['href']
            for link in soup.find_all('a', href=True)
            if '/Product/' in link['href'] and is_valid_product_link(
                link['href'] if link['href'].startswith('http') else "https://www.wconcept.co.kr" + link['href']
            )
        ],
        
        # ë°©ë²• 2: Wì»¨ì…‰ íŠ¹í™” ì„ íƒìë¡œ ì¶”ì¶œ
        lambda: [
            "https://www.wconcept.co.kr/Product/" + re.search(r'/(\d+)', link['href']).group(1)
            for link in soup.select(".goods-list a[href*='/Product/'], .product-list a[href*='/Product/']")
            if re.search(r'/(\d+)', link['href']) and is_valid_product_link(
                "https://www.wconcept.co.kr/Product/" + re.search(r'/(\d+)', link['href']).group(1)
            )
        ],
        
        # ë°©ë²• 3: ìƒí’ˆ ì¹´ë“œ ë‚´ë¶€ ë§í¬
        lambda: [
            "https://www.wconcept.co.kr/Product/" + re.search(r'/(\d+)', link['href']).group(1)
            for link in soup.select(".goods-card a[href*='/Product/'], .product-card a[href*='/Product/']")
            if re.search(r'/(\d+)', link['href']) and is_valid_product_link(
                "https://www.wconcept.co.kr/Product/" + re.search(r'/(\d+)', link['href']).group(1)
            )
        ],
        
        # ë°©ë²• 4: data-product-id ì†ì„±ì—ì„œ ì¶”ì¶œ
        lambda: [
            "https://www.wconcept.co.kr/Product/" + item['data-product-id']
            for item in soup.find_all(attrs={'data-product-id': True})
            if item['data-product-id'].isdigit() and is_valid_product_link(
                "https://www.wconcept.co.kr/Product/" + item['data-product-id']
            )
        ],
        
        # ë°©ë²• 5: ì´ë¯¸ì§€ srcì—ì„œ ìƒí’ˆ ID ì¶”ì¶œ (Wì»¨ì…‰ íŒ¨í„´)
        lambda: [
            "https://www.wconcept.co.kr/Product/" + re.search(r'/(\d+)_', img['src']).group(1)
            for img in soup.find_all('img', src=True)
            if re.search(r'/(\d+)_', img.get('src', '')) and is_valid_product_link(
                "https://www.wconcept.co.kr/Product/" + re.search(r'/(\d+)_', img['src']).group(1)
            )
        ]
    ]
    
    for i, method in enumerate(extraction_methods, 1):
        try:
            links = method()
            if links:
                # ì¤‘ë³µ ì œê±°
                unique_method_links = list(set(links))
                print(f"âœ… ë°©ë²• {i}ë¡œ {len(unique_method_links)}ê°œ ìœ íš¨í•œ ë§í¬ ì¶”ì¶œ")
                all_links.extend(unique_method_links)
        except Exception as e:
            print(f"âš ï¸ ë°©ë²• {i} ì‹¤íŒ¨: {e}")
    
    # ìµœì¢… ì¤‘ë³µ ì œê±° ë° ê²€ì¦
    unique_links = []
    seen_ids = set()
    
    for link in all_links:
        if is_valid_product_link(link):
            product_id = link.split('/Product/')[-1]
            if product_id not in seen_ids:
                unique_links.append(link)
                seen_ids.add(product_id)
    
    # ì •ë ¬
    unique_links = sorted(unique_links, key=lambda x: int(x.split('/Product/')[-1]))
    
    print(f"ğŸ¯ '{category_name}'ì—ì„œ ì´ {len(unique_links)}ê°œì˜ ìœ íš¨í•œ ìƒí’ˆ ë§í¬ ì¶”ì¶œ")
    
    # ë””ë²„ê¹…: ì²˜ìŒ 5ê°œ ë§í¬ ì¶œë ¥
    if unique_links:
        print("ğŸ“‹ ì¶”ì¶œëœ ë§í¬ ìƒ˜í”Œ:")
        for i, link in enumerate(unique_links[:5], 1):
            print(f"   {i}. {link}")
    
    return unique_links

def crawl_category_links(category_name, base_url, total_pages, **context):
    """
    ì£¼ì–´ì§„ ì¹´í…Œê³ ë¦¬ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ Wì»¨ì…‰ ìƒí’ˆ ë§í¬ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
    """
    print(f"\n{'='*30}")
    print(f"â–¶â–¶â–¶ '{category_name}' ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì‹œì‘ (ì´ {total_pages} í˜ì´ì§€)")
    print(f"{'='*30}")
    
    driver = setup_chrome_driver()
    all_product_links = []

    try:
        for page_num in range(1, total_pages + 1):
            url = f"{base_url}&page={page_num}" if '?' in base_url else f"{base_url}?page={page_num}"
            
            try:
                print(f"ğŸ“ í˜ì´ì§€ ë¡œë”© ì¤‘: {url}")
                driver.get(url)
                
                # ìƒí’ˆ ë¡œë”© ëŒ€ê¸°
                active_selector = wait_for_products_to_load(driver)
                if not active_selector:
                    print(f"âŒ {page_num}í˜ì´ì§€ì—ì„œ ìƒí’ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                    continue
                
                # ëª¨ë“  ìƒí’ˆì´ ë¡œë”©ë  ë•Œê¹Œì§€ ìŠ¤í¬ë¡¤
                final_count = scroll_to_load_all_products(driver, expected_count=60)
                
                # ìŠ¤í¬ë¡¤ í›„ ì¶”ê°€ ëŒ€ê¸°
                time.sleep(3)
                
                # ì¶”ê°€ ë¡œë”© ëŒ€ê¸° (ë™ì  ì½˜í…ì¸  ì™„ì „ ë¡œë”©)
                print("ğŸ”„ ë™ì  ì½˜í…ì¸  ì¶”ê°€ ë¡œë”© ëŒ€ê¸°...")
                for i in range(10):  # íšŸìˆ˜ ì¦ê°€
                    # ë§¨ ì•„ë˜ë¡œ ìŠ¤í¬ë¡¤
                    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                    time.sleep(2)
                    
                    # ë§¨ ìœ„ë¡œ ìŠ¤í¬ë¡¤
                    driver.execute_script("window.scrollTo(0, 0);")
                    time.sleep(1)
                    
                    # ì¤‘ê°„ ì§€ì ìœ¼ë¡œ ìŠ¤í¬ë¡¤
                    total_height = driver.execute_script("return document.body.scrollHeight;")
                    driver.execute_script(f"window.scrollTo(0, {total_height // 2});")
                    time.sleep(1)
                
                # í˜ì´ì§€ ì†ŒìŠ¤ íŒŒì‹±
                soup = BeautifulSoup(driver.page_source, 'html.parser')
                
                # ìƒí’ˆ ë§í¬ ì¶”ì¶œ
                page_links = extract_product_links(soup, category_name)
                all_product_links.extend(page_links)
                
                print(f"ğŸ“Š ìŠ¤í¬ë¡¤ ê²°ê³¼: {final_count}ê°œ ìƒí’ˆ ê°ì§€, {len(page_links)}ê°œ ë§í¬ ì¶”ì¶œ")
                
                # ë§Œì•½ 60ê°œ ë¯¸ë§Œì´ë©´ ì¶”ê°€ ì‹œë„
                if len(page_links) < 60:
                    print(f"âš ï¸ {len(page_links)}ê°œë§Œ ìˆ˜ì§‘ë¨. ì¶”ê°€ ì‹œë„ ì¤‘...")
                    
                    # ê°•í™”ëœ ì¶”ê°€ ìŠ¤í¬ë¡¤
                    for attempt in range(3):  # 3ë²ˆ ì‹œë„
                        print(f"ğŸ”„ ì¶”ê°€ ì‹œë„ {attempt + 1}/3...")
                        time.sleep(5)
                        
                        # ì²œì²œíˆ ë§¨ ì•„ë˜ê¹Œì§€ ìŠ¤í¬ë¡¤
                        total_height = driver.execute_script("return document.body.scrollHeight;")
                        for j in range(0, total_height, 100):
                            driver.execute_script(f"window.scrollTo(0, {j});")
                            time.sleep(0.3)
                        
                        # ë§¨ ì•„ë˜ì—ì„œ ì ì‹œ ëŒ€ê¸°
                        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                        time.sleep(5)
                        
                        # ë‹¤ì‹œ íŒŒì‹±
                        soup = BeautifulSoup(driver.page_source, 'html.parser')
                        additional_links = extract_product_links(soup, category_name)
                        
                        if len(additional_links) > len(page_links):
                            page_links = additional_links
                            all_product_links = all_product_links[:-len(page_links)] + page_links
                            print(f"âœ… ì¶”ê°€ ì‹œë„ {attempt + 1} ì„±ê³µ: {len(page_links)}ê°œ ë§í¬")
                            break
                        else:
                            print(f"âš ï¸ ì¶”ê°€ ì‹œë„ {attempt + 1} ì‹¤íŒ¨: ì—¬ì „íˆ {len(page_links)}ê°œ")
                    
                    if len(page_links) < 60:
                        print(f"âŒ ìµœì¢… ì‹¤íŒ¨: {len(page_links)}ê°œë§Œ ìˆ˜ì§‘ë¨")
                
                # ìµœì¢… ê²€ì¦: ì‹¤ì œ ìƒí’ˆ ë§í¬ë§Œ í•„í„°ë§
                valid_links = []
                for link in page_links:
                    if (link.startswith('https://www.wconcept.co.kr/Product/') and 
                        link.split('/Product/')[-1].isdigit() and
                        len(link.split('/Product/')[-1]) >= 6):
                        valid_links.append(link)
                
                if len(valid_links) != len(page_links):
                    print(f"ğŸ” ë§í¬ ê²€ì¦: {len(page_links)}ê°œ â†’ {len(valid_links)}ê°œ ìœ íš¨í•œ ë§í¬")
                    page_links = valid_links
                    all_product_links = all_product_links[:-len(page_links)] + page_links
                
                print(f"âœ… '{category_name}' {page_num}/{total_pages} í˜ì´ì§€ í¬ë¡¤ë§ ì™„ë£Œ (ìˆ˜ì§‘: {len(page_links)}ê°œ, ëˆ„ì : {len(all_product_links)}ê°œ)")
                
                # í˜ì´ì§€ ê°„ ê°„ê²©
                time.sleep(3)

            except Exception as e:
                print(f"â—ï¸ '{category_name}' {page_num}í˜ì´ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue

    except Exception as e:
        print(f"âŒ '{category_name}' í¬ë¡¤ë§ ì¤‘ ì „ì²´ ì˜¤ë¥˜: {e}")
        raise
    finally:
        driver.quit()

    # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
    unique_links = sorted(list(set(all_product_links)))
    print(f"\nğŸš€ '{category_name}' ì¹´í…Œê³ ë¦¬ì—ì„œ ì´ {len(unique_links)}ê°œì˜ ê³ ìœ  ë§í¬ ìˆ˜ì§‘ ì™„ë£Œ!")
    
    # XComì„ í†µí•´ ê²°ê³¼ ì „ë‹¬
    context['task_instance'].xcom_push(key=f'{category_name}_links', value=unique_links)
    context['task_instance'].xcom_push(key=f'{category_name}_count', value=len(unique_links))
    
    return unique_links

def save_to_csv_task(category_name, save_path='link', **context):
    """ìˆ˜ì§‘ëœ ë§í¬ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    # XComì—ì„œ ë§í¬ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    links = context['task_instance'].xcom_pull(key=f'{category_name}_links')
    
    if not links:
        print(f"âŒ '{category_name}' ë§í¬ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return False
    
    # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„± (ë‚ ì§œë³„ í´ë” êµ¬ì¡°)
    current_date = datetime.now().strftime('%Y%m%d')  # YYYYMMDD í˜•ì‹
    full_save_path = os.path.join('/home/tjddml/w', save_path, current_date)  # link/ë‚ ì§œ/ í˜•íƒœ
    os.makedirs(full_save_path, exist_ok=True)
    
    # íŒŒì¼ëª… (ì¹´í…Œê³ ë¦¬ëª…ë§Œ)
    file_path = os.path.join(full_save_path, f"{category_name}.csv")
    
    try:
        with open(file_path, "w", newline='', encoding='utf-8-sig') as f:
            writer = csv.writer(f)
            writer.writerow(['id', 'ë§í¬'])
            for link in links:
                # URLì—ì„œ ìƒí’ˆ ID ì¶”ì¶œ
                product_id = link.split('/Product/')[-1]
                writer.writerow([product_id, link])
        
        print(f"ğŸ’¾ [ì €ì¥ ì„±ê³µ] '{file_path}' íŒŒì¼ë¡œ {len(links)}ê°œ ë§í¬ ì €ì¥ ì™„ë£Œ.")
        
        # ì €ì¥ ê²°ê³¼ë¥¼ XComì— ì €ì¥
        context['task_instance'].xcom_push(key=f'{category_name}_file_path', value=file_path)
        return True
        
    except IOError as e:
        print(f"âŒ [ì €ì¥ ì˜¤ë¥˜] '{file_path}' ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise

def summary_task(**context):
    """ì „ì²´ ì‘ì—… ê²°ê³¼ ìš”ì•½"""
    print("\n" + "="*50)
    print("ğŸ“Š í¬ë¡¤ë§ ì‘ì—… ì™„ë£Œ ìš”ì•½")
    print("="*50)
    
    total_links = 0
    for task_config in CRAWLING_TASKS:
        category_name = task_config['name']
        count = context['task_instance'].xcom_pull(key=f'{category_name}_count')
        file_path = context['task_instance'].xcom_pull(key=f'{category_name}_file_path')
        
        if count:
            total_links += count
            print(f"âœ… {category_name}: {count}ê°œ ë§í¬ ìˆ˜ì§‘")
            print(f"   ğŸ“ íŒŒì¼: {file_path}")
        else:
            print(f"âŒ {category_name}: ìˆ˜ì§‘ ì‹¤íŒ¨")
    
    print(f"\nğŸ‰ ì´ {total_links}ê°œì˜ ìƒí’ˆ ë§í¬ê°€ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("="*50)

# í¬ë¡¤ë§ ì‘ì—… ì„¤ì •
CRAWLING_TASKS = [
    {
        'name': 'ì—¬ì_í‹°ì…”ì¸ _ì‹ ìƒí’ˆ',
        'url': 'https://display.wconcept.co.kr/category/women/001003?sort=NEW',
        'pages': 1,
        'save_path': 'link'
    },
    {
        'name': 'ì—¬ì_ìŠ¤ì»¤íŠ¸_ì‹ ìƒí’ˆ',
        'url': 'https://display.wconcept.co.kr/category/women/001007?sort=NEW',
        'pages': 1,
        'save_path': 'link'
    },
    {
        'name': 'ì—¬ì_ë°”ì§€_ì‹ ìƒí’ˆ',
        'url': 'https://display.wconcept.co.kr/category/women/001006?sort=NEW',
        'pages': 1,
        'save_path': 'link'
    },
    {
        'name': 'ë‚¨ì_í‹°ì…”ì¸ _ì‹ ìƒí’ˆ',
        'url': 'https://display.wconcept.co.kr/category/men/001002?sort=NEW',
        'pages': 1,
        'save_path': 'link'
    },
    {
        'name': 'ë‚¨ì_ë°”ì§€_ì‹ ìƒí’ˆ',
        'url': 'https://display.wconcept.co.kr/category/men/001005?sort=NEW',
        'pages': 1,
        'save_path': 'link'
    },
    {
        'name': 'ì—¬ì_ì›í”¼ìŠ¤_ì‹ ìƒí’ˆ',
        'url': 'https://display.wconcept.co.kr/category/women/001005?sort=NEW',  
        'pages': 1,
        'save_path': 'link'
    },
]

# ì‹œì‘ íƒœìŠ¤í¬
start_task = DummyOperator(
    task_id='start_crawling',
    dag=dag
)

# ë™ì ìœ¼ë¡œ í¬ë¡¤ë§ ë° ì €ì¥ íƒœìŠ¤í¬ ìƒì„±
crawling_tasks = []
saving_tasks = []

for task_config in CRAWLING_TASKS:
    category_name = task_config['name']
    
    # í¬ë¡¤ë§ íƒœìŠ¤í¬
    crawl_task = PythonOperator(
        task_id=f'crawl_{category_name}',
        python_callable=crawl_category_links,
        op_kwargs={
            'category_name': category_name,
            'base_url': task_config['url'],
            'total_pages': task_config['pages']
        },
        dag=dag
    )
    
    # CSV ì €ì¥ íƒœìŠ¤í¬
    save_task = PythonOperator(
        task_id=f'save_{category_name}',
        python_callable=save_to_csv_task,
        op_kwargs={
            'category_name': category_name,
            'save_path': task_config['save_path']
        },
        dag=dag
    )
    
    # íƒœìŠ¤í¬ ì˜ì¡´ì„± ì„¤ì •
    start_task >> crawl_task >> save_task
    
    crawling_tasks.append(crawl_task)
    saving_tasks.append(save_task)

# ìš”ì•½ íƒœìŠ¤í¬
summary = PythonOperator(
    task_id='crawling_summary',
    python_callable=summary_task,
    dag=dag
)

# ë§í¬ CSVë¥¼ S3ì— ì—…ë¡œë“œ í›„ ë¡œì»¬ì—ì„œ ì‚­ì œ
def upload_link_csvs_to_s3_and_cleanup(base_path='/home/tjddml/w/link', **context):
    try:
        if not os.path.exists(base_path):
            print(f"â„¹ï¸ ë§í¬ í´ë” ì—†ìŒ: {base_path}")
            return True

        # ìµœì‹  ë‚ ì§œ(YYYYMMDD) í´ë” ì„ íƒ
        date_dirs = [d for d in os.listdir(base_path)
                     if os.path.isdir(os.path.join(base_path, d)) and len(d) == 8 and d.isdigit()]
        if not date_dirs:
            print("â„¹ï¸ ë‚ ì§œ í´ë”ê°€ ì—†ì–´ ì—…ë¡œë“œí•  ë§í¬ CSVê°€ ì—†ìŠµë‹ˆë‹¤.")
            return True

        latest_date = max(date_dirs)
        latest_dir = os.path.join(base_path, latest_date)
        csv_files = [os.path.join(latest_dir, f) for f in os.listdir(latest_dir) if f.lower().endswith('.csv')]

        if not csv_files:
            print(f"â„¹ï¸ ìµœê·¼ í´ë”({latest_dir}) ë‚´ CSVê°€ ì—†ìŠµë‹ˆë‹¤.")
            return True

        uploader = S3Uploader()
        if not uploader.s3_client:
            print("âŒ S3 í´ë¼ì´ì–¸íŠ¸ ì—†ìŒ: ì—…ë¡œë“œ/ì •ë¦¬ ìƒëµ")
            return True

        uploaded = 0
        for file_path in csv_files:
            try:
                filename = os.path.basename(file_path)
                s3_key = f"wconcept_link_csvs/{latest_date}/{filename}"
                print(f"ğŸ”„ ì—…ë¡œë“œ: {file_path} â†’ s3://{uploader.bucket_name}/{s3_key}")
                uploader.s3_client.upload_file(file_path, uploader.bucket_name, s3_key, ExtraArgs={'ContentType': 'text/csv'})
                uploaded += 1
                # ì—…ë¡œë“œ ì„±ê³µ ì‹œ íŒŒì¼ ì‚­ì œ
                try:
                    os.remove(file_path)
                    print(f"ğŸ—‘ï¸ ë¡œì»¬ ì‚­ì œ: {file_path}")
                except Exception as e:
                    print(f"âš ï¸ ë¡œì»¬ ì‚­ì œ ì‹¤íŒ¨: {file_path} - {e}")
            except Exception as e:
                print(f"âŒ ì—…ë¡œë“œ ì‹¤íŒ¨: {file_path} - {e}")

        # í´ë”ê°€ ë¹„ë©´ í´ë” ì‚­ì œ
        try:
            if not os.listdir(latest_dir):
                os.rmdir(latest_dir)
                print(f"ğŸ—‘ï¸ ë¹ˆ ë‚ ì§œ í´ë” ì‚­ì œ: {latest_dir}")
        except Exception as e:
            print(f"âš ï¸ í´ë” ì‚­ì œ ì‹¤íŒ¨: {latest_dir} - {e}")

        print(f"ğŸ“Š ë§í¬ CSV ì—…ë¡œë“œ ì™„ë£Œ: {uploaded}/{len(csv_files)} ì„±ê³µ")
        return True
    except Exception as e:
        print(f"âŒ ë§í¬ CSV ì—…ë¡œë“œ/ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return True

upload_links_and_cleanup_task = PythonOperator(
    task_id='upload_link_csvs_to_s3_and_cleanup',
    python_callable=upload_link_csvs_to_s3_and_cleanup,
    dag=dag
)

# HTML í¬ë¡¤ë§ DAG íŠ¸ë¦¬ê±° íƒœìŠ¤í¬
trigger_html_crawler = TriggerDagRunOperator(
    task_id='trigger_html_crawler',
    trigger_dag_id='wconcept_html_crawler',
    dag=dag
)

# ëª¨ë“  ì €ì¥ íƒœìŠ¤í¬ê°€ ì™„ë£Œëœ í›„ ìš”ì•½ ì‹¤í–‰ â†’ ë§í¬ CSV S3 ì—…ë¡œë“œ ë° ë¡œì»¬ ì •ë¦¬ â†’ HTML í¬ë¡¤ë§ íŠ¸ë¦¬ê±°
saving_tasks >> summary >> upload_links_and_cleanup_task >> trigger_html_crawler 