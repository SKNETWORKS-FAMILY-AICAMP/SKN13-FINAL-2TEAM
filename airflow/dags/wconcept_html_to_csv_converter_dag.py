from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.dummy import DummyOperator
from airflow.operators.trigger_dagrun import TriggerDagRunOperator
from airflow.models import Variable
import os
import pandas as pd
from bs4 import BeautifulSoup
import requests
import re
import boto3
from botocore.exceptions import ClientError

# DAG ê¸°ë³¸ ì„¤ì •
default_args = {
    'owner': 'data_team',
    'depends_on_past': False,
    'start_date': datetime(2023, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=2),
    'catchup': False
}

class S3Uploader:
    """S3ì— CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        # Airflow Variablesì—ì„œ AWS í‚¤ ê°€ì ¸ì˜¤ê¸°
        try:
            self.aws_access_key_id = Variable.get("AWS_ACCESS_KEY_ID", default_var=None)
            self.aws_secret_access_key = Variable.get("AWS_SECRET_ACCESS_KEY", default_var=None)
            self.aws_region = Variable.get("AWS_REGION", default_var="ap-northeast-2")
            self.bucket_name = Variable.get("S3_BUCKET_NAME", default_var="ivle-malle")
        except Exception as e:
            print(f"âŒ Airflow Variables ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.aws_access_key_id = None
            self.aws_secret_access_key = None
            self.aws_region = "ap-northeast-2"
            self.bucket_name = "ivle-malle"
        
        # AWS í‚¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        print(f"ğŸ” AWS ì„¤ì • í™•ì¸:")
        print(f"   AWS_ACCESS_KEY_ID: {'ì„¤ì •ë¨' if self.aws_access_key_id else 'ì„¤ì •ë˜ì§€ ì•ŠìŒ'}")
        print(f"   AWS_SECRET_ACCESS_KEY: {'ì„¤ì •ë¨' if self.aws_secret_access_key else 'ì„¤ì •ë˜ì§€ ì•ŠìŒ'}")
        print(f"   AWS_REGION: {self.aws_region}")
        print(f"   S3_BUCKET_NAME: {self.bucket_name}")
        
        # AWS í‚¤ê°€ ì—†ìœ¼ë©´ S3 í´ë¼ì´ì–¸íŠ¸ë¥¼ Noneìœ¼ë¡œ ì„¤ì •
        if not self.aws_access_key_id or not self.aws_secret_access_key:
            print("âŒ AWS í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
            print("   Airflow UI â†’ Admin â†’ Variablesì—ì„œ ë‹¤ìŒ ë³€ìˆ˜ë“¤ì„ ì„¤ì •í•´ì£¼ì„¸ìš”:")
            print("   - AWS_ACCESS_KEY_ID")
            print("   - AWS_SECRET_ACCESS_KEY")
            print("   - AWS_REGION (ì„ íƒì‚¬í•­)")
            print("   - S3_BUCKET_NAME (ì„ íƒì‚¬í•­)")
            self.s3_client = None
            return
        
        # S3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        try:
            self.s3_client = boto3.client(
                's3',
                aws_access_key_id=self.aws_access_key_id,
                aws_secret_access_key=self.aws_secret_access_key,
                region_name=self.aws_region
            )
            print(f"âœ… S3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ: {self.bucket_name}")
        except Exception as e:
            print(f"âŒ S3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.s3_client = None

# DAG ì •ì˜
dag = DAG(
    'wconcept_html_to_csv_converter',
    default_args=default_args,
    description='Wì»¨ì…‰ HTML íŒŒì¼ë“¤ì„ CSVë¡œ ë³€í™˜í•˜ëŠ” DAG (ì œí’ˆì •ë³´ + ë¦¬ë·° í†µí•©)',
    schedule_interval=None,  # ìˆ˜ë™ ì‹¤í–‰ ë˜ëŠ” íŠ¸ë¦¬ê±°ë¡œë§Œ ì‹¤í–‰
    max_active_runs=1,
    tags=['conversion', 'wconcept', 'html', 'csv', 'ecommerce']
)

def find_html_files(base_html_dir='/home/tjddml/w/html', **context):
    """HTML íŒŒì¼ë“¤ì„ ì°¾ì•„ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    print(f"\n{'='*50}")
    print(f"ğŸ” HTML íŒŒì¼ ê²€ìƒ‰ ì‹œì‘: {base_html_dir}")
    print(f"{'='*50}")
    
    try:
        if not os.path.exists(base_html_dir):
            print(f"âŒ HTML ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {base_html_dir}")
            context['task_instance'].xcom_push(key='html_files', value=[])
            return []
        
        # ëª¨ë“  í•˜ìœ„ ë””ë ‰í† ë¦¬ì—ì„œ HTML íŒŒì¼ë“¤ì„ ì°¾ê¸°
        html_files = []
        for root, dirs, files in os.walk(base_html_dir):
            for file in files:
                if file.endswith(".html"):
                    html_files.append(os.path.join(root, file))
        
        print(f"âœ… ë°œê²¬ëœ HTML íŒŒì¼: {len(html_files)}ê°œ")
        for file in html_files[:5]:  # ì²˜ìŒ 5ê°œë§Œ ì¶œë ¥
            print(f"   ğŸ“„ {os.path.basename(file)}")
        if len(html_files) > 5:
            print(f"   ... ì™¸ {len(html_files) - 5}ê°œ")
        
        context['task_instance'].xcom_push(key='html_files', value=html_files)
        context['task_instance'].xcom_push(key='base_html_dir', value=base_html_dir)
        
        return html_files
        
    except Exception as e:
        print(f"âŒ HTML íŒŒì¼ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
        context['task_instance'].xcom_push(key='html_files', value=[])
        return []

def extract_category_from_path(file_path):
    """íŒŒì¼ ê²½ë¡œì—ì„œ ëŒ€ë¶„ë¥˜ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    try:
        # íŒŒì¼ ê²½ë¡œì—ì„œ ë””ë ‰í† ë¦¬ êµ¬ì¡° ë¶„ì„
        path_parts = file_path.split('/')
        
        # ë””ë ‰í† ë¦¬ ì´ë¦„ì„ ëŒ€ë¶„ë¥˜ë¡œ ë§¤í•‘ (ìƒì˜/ë°”ì§€/ì›í”¼ìŠ¤/ìŠ¤ì»¤íŠ¸ë§Œ)
        category_mapping = {
            'ìƒì˜': ['ìƒì˜', 'í‹°ì…”ì¸ ', 'ì…”ì¸ ', 'ë¸”ë¼ìš°ìŠ¤', 'ë‹ˆíŠ¸', 'ìŠ¤ì›¨í„°', 'í›„ë“œ', 'ìƒì˜ë¥˜'],
            'ë°”ì§€': ['ë°”ì§€', 'íŒ¬ì¸ ', 'ë°ë‹˜', 'ìŠ¬ë™ìŠ¤', 'ì¡°ê±°', 'ë°”ì§€ë¥˜'],
            'ì›í”¼ìŠ¤': ['ì›í”¼ìŠ¤', 'ë“œë ˆìŠ¤', 'ì›í”¼ìŠ¤ë¥˜'],
            'ìŠ¤ì»¤íŠ¸': ['ìŠ¤ì»¤íŠ¸', 'ì¹˜ë§ˆ', 'ìŠ¤ì»¤íŠ¸ë¥˜']
        }
        
        # ê²½ë¡œì˜ ê° ë¶€ë¶„ì„ í™•ì¸í•˜ì—¬ ëŒ€ë¶„ë¥˜ ë§¤í•‘
        for part in path_parts:
            part_lower = part.lower()
            for category, keywords in category_mapping.items():
                for keyword in keywords:
                    if keyword in part_lower:
                        return category
        
        return None
        
    except Exception as e:
        print(f"âš ï¸ ê²½ë¡œì—ì„œ ëŒ€ë¶„ë¥˜ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return None

def parse_html_file(file_path):
    """ë‹¨ì¼ HTML íŒŒì¼ì„ íŒŒì‹±í•˜ì—¬ ìƒí’ˆ ì •ë³´ì™€ ë¦¬ë·°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            soup = BeautifulSoup(f, "html.parser")
            full_text = soup.get_text(separator="\n")

        product_id = os.path.basename(file_path).replace(".html", "")
        
        # === ì œí’ˆ ì •ë³´ ì¶”ì¶œ ===
        # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
        product_name = soup.title.text.strip().replace('[', '').replace(']', '') if soup.title else None
        canonical = soup.find("link", rel="canonical")
        product_link = canonical["href"] if canonical else None

        # ë¸Œëœë“œ ì •ë³´ ì¶”ì¶œ
        brand = seller = None
        brand_script = soup.find("script", string=lambda s: s and "ep_page_brandNM" in s)
        if brand_script:
            for line in brand_script.string.splitlines():
                if "ep_page_brandNM" in line:
                    brand = line.split("=")[-1].strip().strip('"+; ')
                    seller = brand

        # ê°€ê²© ì •ë³´ ì¶”ì¶œ
        price_wrap = soup.find("div", class_="price_wrap")
        normal_price = discount_price = None
        if price_wrap:
            # ë””ë²„ê¹…: ê°€ê²© ì •ë³´ HTML êµ¬ì¡° ì¶œë ¥ (ì²˜ìŒ 5ê°œ íŒŒì¼ë§Œ)
            if product_id in ['debug_sample']:  # ì‹¤ì œ ë””ë²„ê¹…ì´ í•„ìš”í•  ë•Œ ì‚¬ìš©
                print(f"ğŸ” ê°€ê²© ì •ë³´ HTML êµ¬ì¡° ({product_id}):")
                print(price_wrap.prettify()[:1000])  # ì²˜ìŒ 1000ìë§Œ ì¶œë ¥
            
            # ëª¨ë“  dd íƒœê·¸ë¥¼ ì°¾ì•„ì„œ ê°€ê²© ì •ë³´ ì¶”ì¶œ
            dd_tags = price_wrap.find_all("dd")
            
            for dd_tag in dd_tags:
                # dd íƒœê·¸ ë‚´ë¶€ì˜ em íƒœê·¸ ì°¾ê¸°
                em_tag = dd_tag.find("em")
                if em_tag:
                    price_text = em_tag.text.strip().replace(",", "")
                    
                    # dd íƒœê·¸ì˜ í´ë˜ìŠ¤ì— ë”°ë¼ êµ¬ë¶„
                    dd_class = dd_tag.get("class", [])
                    
                    if "normal" in dd_class:
                        normal_price = price_text
                    elif "cupon" in dd_class:
                        discount_price = price_text
                    else:
                        # í´ë˜ìŠ¤ê°€ ì—†ëŠ” ê²½ìš°, ì²« ë²ˆì§¸ dd íƒœê·¸ë¥¼ ì •ìƒê°€ë¡œ ê°„ì£¼
                        if normal_price is None:
                            normal_price = price_text
                        elif discount_price is None:
                            discount_price = price_text
            
            # em íƒœê·¸ê°€ ì—†ëŠ” ê²½ìš°ë¥¼ ìœ„í•œ ëŒ€ì²´ ë°©ë²•
            if normal_price is None:
                # ì •ìƒê°€ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš°, ëª¨ë“  dd íƒœê·¸ì—ì„œ ìˆ«ì ì¶”ì¶œ ì‹œë„
                for dd_tag in dd_tags:
                    dd_class = dd_tag.get("class", [])
                    if "normal" in dd_class or (not dd_class and normal_price is None):
                        text = dd_tag.get_text(strip=True)
                        price_match = re.search(r'[\d,]+', text)
                        if price_match:
                            normal_price = price_match.group().replace(",", "")
                            break
            
            if discount_price is None:
                # í• ì¸ê°€ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš°, cupon í´ë˜ìŠ¤ê°€ ìˆëŠ” dd íƒœê·¸ì—ì„œ ìˆ«ì ì¶”ì¶œ ì‹œë„
                for dd_tag in dd_tags:
                    dd_class = dd_tag.get("class", [])
                    if "cupon" in dd_class:
                        text = dd_tag.get_text(strip=True)
                        price_match = re.search(r'[\d,]+', text)
                        if price_match:
                            discount_price = price_match.group().replace(",", "")
                            break

        # ìƒí’ˆ ID ì¶”ì¶œ
        product_id_from_script = None
        id_script = soup.find("script", string=lambda s: s and "prodid" in s)
        if id_script:
            match = re.search(r"['\"]prodid['\"]\s*:\s*['\"]?(\d+)", id_script.string)
            if match:
                product_id_from_script = match.group(1)

        # ì„±ë³„ ì •ë³´ ì¶”ì¶œ
        gender_tag = soup.select_one("#prdLocaiton li:nth-of-type(2) a")
        gender = gender_tag.get_text(strip=True) if gender_tag else None

        # ì¹´í…Œê³ ë¦¬ ì •ë³´ ì¶”ì¶œ
        main_category = soup.select_one("#cateDepth3 > button")
        sub_category = soup.select_one("#btn4depth")
        product_main_category = main_category.get_text(strip=True) if main_category else None
        product_sub_category = sub_category.get_text(strip=True) if sub_category else None
        
        # ë””ë ‰í† ë¦¬ ì´ë¦„ì—ì„œ ëŒ€ë¶„ë¥˜ ì¶”ì¶œ (íŒŒì¼ ê²½ë¡œ ê¸°ë°˜)
        directory_category = extract_category_from_path(file_path)
        
        # ë””ë ‰í† ë¦¬ì—ì„œ ì¶”ì¶œí•œ ëŒ€ë¶„ë¥˜ê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
        if directory_category:
            product_main_category = directory_category

        # ì¢‹ì•„ìš” ìˆ˜ ì¶”ì¶œ
        like_tag = soup.find(id="myHeartAllCount")
        like_count = like_tag.get_text(strip=True) if like_tag else None

        # ì´ë¯¸ì§€ URL ì¶”ì¶œ (ë‹¤ìš´ë¡œë“œ ì—†ì´ URLë§Œ ì €ì¥)
        image_url = None
        og_image = soup.find("meta", property="og:image")
        if og_image:
            image_url = og_image["content"]

        # ì‚¬ì´ì¦ˆ ì •ë³´ ì¶”ì¶œ
        size_info = { "ì‚¬ì´ì¦ˆ": None, "ì´ì¥": None, "ì–´ê¹¨ ë„ˆë¹„": None, "ê°€ìŠ´ ë‹¨ë©´": None, "ì†Œë§¤ ê¸¸ì´": None }
        product_material = product_color = product_care = product_code = product_dimension = None

        tables = soup.find_all("table")
        for table in tables:
            for row in table.find_all("tr"):
                cells = row.find_all(["td", "th"])
                if len(cells) >= 2:
                    key = cells[0].text.strip()
                    value = cells[1].text.strip()
                    for k in size_info:
                        if k in key and not size_info[k]:
                            size_info[k] = value
                    if "ìƒí’ˆì½”ë“œ" in key and not product_code:
                        product_code = value
                    elif ("ì¹˜ìˆ˜" in key or "í¬ê¸°" in key) and not product_dimension:
                        product_dimension = value

        # ìƒí’ˆ ìƒì„¸ ì •ë³´ ì¶”ì¶œ
        material_tag = soup.select_one("#container > div > div > div:nth-of-type(1) > div:nth-of-type(2) > dl > dd > ul > li:nth-of-type(2)")
        if material_tag:
            product_material = material_tag.get_text(strip=True)

        color_tag = soup.select_one("#container > div > div > div:nth-of-type(3) > div:nth-of-type(4) > table > tbody > tr:nth-of-type(2) > td")
        if color_tag:
            product_color = color_tag.get_text(strip=True)

        care_tag = soup.select_one("#container > div > div > div:nth-of-type(3) > div:nth-of-type(4) > table > tbody > tr:nth-of-type(6) > td")
        if care_tag:
            product_care = care_tag.get_text(strip=True)

        # === ë¦¬ë·° ì •ë³´ ì¶”ì¶œ ===
        def get_text(sel):
            tag = soup.select_one(sel)
            return tag.get_text(strip=True) if tag else None

        ë¦¬ë·°ê°œìˆ˜ = get_text("#reviewCnt3")
        ë¦¬ë·°í‰ê·  = get_text("#container div div div:nth-of-type(4) > div:nth-of-type(1) > div > div:nth-of-type(1) div div:nth-of-type(2) > strong")

        # ë¦¬ë·° row ë°˜ë³µ
        reviews = []
        rows = soup.select("#reviewList table tbody tr")
        for row in rows:
            def get_text_from(tag, sel):
                el = tag.select_one(sel)
                return el.get_text(strip=True) if el else None

            def get_attr_from(tag, sel, attr):
                el = tag.select_one(sel)
                return el.get(attr) if el and el.has_attr(attr) else None

            def get_date_from_row(tag):
                p_tag = tag.select_one("td:nth-of-type(2) > div:nth-of-type(1) > p")
                if p_tag:
                    span = p_tag.select_one("span")
                    if span:
                        return span.get_text(strip=True)
                return None

            def get_writer_info(tag):
                """ì‘ì„±ì ì •ë³´ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ì¶”ì¶œ"""
                writer_name = get_text_from(tag, "td:nth-of-type(2) div:nth-of-type(1) p em")
                size_info = get_text_from(tag, "td:nth-of-type(2) div:nth-of-type(1) div div p:nth-of-type(2) span")
                
                info_parts = []
                if writer_name:
                    info_parts.append(f"ì‘ì„±ì: {writer_name}")
                if size_info:
                    info_parts.append(f"ì‚¬ì´ì¦ˆ: {size_info}")
                    
                return " | ".join(info_parts) if info_parts else None

            ë¦¬ë·° = {
                "ìƒí’ˆì½”ë“œ": product_id,
                "ì‘ì„±ìëª…": get_text_from(row, "td:nth-of-type(2) div:nth-of-type(1) p em"),
                "ë¦¬ë·°ê°¯ìˆ˜": ë¦¬ë·°ê°œìˆ˜,
                "í‰ê· í‰ì ": ë¦¬ë·°í‰ê· ,
                "ì‘ì„±ë‚ ì§œ": get_date_from_row(row),
                "êµ¬ë§¤ì˜µì…˜": get_text_from(row, "td:nth-of-type(2) div:nth-of-type(1) div div p span"),
                "ë¦¬ë·°ë‚´ìš©": get_text_from(row, "td:nth-of-type(2) div:nth-of-type(2) p"),
                "ë¦¬ë·° ì´ë¯¸ì§€ url": get_attr_from(row, "td:nth-of-type(2) ul li img", "src"),
                "ì‘ì„±ì ì •ë³´": get_writer_info(row)
            }

            # ë‚´ìš©ì´ ì¡´ì¬í•˜ëŠ” rowë§Œ ì¶”ê°€
            if ë¦¬ë·°["ë¦¬ë·°ë‚´ìš©"] or ë¦¬ë·°["ì‘ì„±ìëª…"]:
                reviews.append(ë¦¬ë·°)

        # ì œí’ˆ ì •ë³´ ë°˜í™˜
        product_info = {
            "ìƒí’ˆì½”ë“œ": product_id,
            "ìƒí’ˆëª…": product_name,
            "ë¸Œëœë“œëª…": brand,
            "ëŒ€ë¶„ë¥˜": product_main_category,
            "ì†Œë¶„ë¥˜": product_sub_category,
            "ì›ê°€": normal_price,
            "í• ì¸ê°€": discount_price,
            "ì„±ë³„": gender,
            "ì´ë¯¸ì§€URL": image_url,
            "ì†Œì¬": product_material,
            "ìƒ‰ìƒ": product_color,
            "ì¢‹ì•„ìš”ìˆ˜": like_count,
            "ìƒí’ˆë§í¬": product_link
        }

        return {
            "product_info": product_info,
            "reviews": reviews
        }
        
    except Exception as e:
        print(f"âŒ HTML íŒŒì¼ íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ({os.path.basename(file_path)}): {e}")
        return None

def convert_html_to_csv(base_html_dir='/home/tjddml/w/html', **context):
    """HTML íŒŒì¼ë“¤ì„ CSVë¡œ ë³€í™˜í•©ë‹ˆë‹¤ (ì œí’ˆì •ë³´ + ë¦¬ë·° í†µí•©)."""
    print(f"\n{'='*50}")
    print(f"ğŸ”„ HTML to CSV ë³€í™˜ ì‹œì‘: {base_html_dir}")
    print(f"{'='*50}")
    
    try:
        # ì¶œë ¥ ê²½ë¡œ ì„¤ì • (HTML ë””ë ‰í† ë¦¬ì™€ ê°™ì€ ë ˆë²¨ì— ì €ì¥)
        output_dir = os.path.dirname(base_html_dir)
        products_csv_path = os.path.join(output_dir, "parsed_products.csv")
        reviews_csv_path = os.path.join(output_dir, "reviews_extracted.csv")
        
        # HTML íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        html_files = context['task_instance'].xcom_pull(task_ids='find_html_files', key='html_files')
        
        if not html_files:
            print("âŒ ì²˜ë¦¬í•  HTML íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        print(f"ğŸ“Š ì´ {len(html_files)}ê°œ HTML íŒŒì¼ ì²˜ë¦¬ ì‹œì‘...")
        
        products_results = []
        reviews_results = []
        success_count = 0
        error_count = 0
        
        for idx, file_path in enumerate(html_files, 1):
            try:
                result = parse_html_file(file_path)
                if result:
                    # ì œí’ˆ ì •ë³´ ì¶”ê°€
                    if result["product_info"]["ìƒí’ˆëª…"]:  # ìƒí’ˆëª…ì´ ìˆëŠ” ê²½ìš°ë§Œ ì¶”ê°€
                        products_results.append(result["product_info"])
                    
                    # ë¦¬ë·° ì •ë³´ ì¶”ê°€
                    reviews_results.extend(result["reviews"])
                    
                    success_count += 1
                else:
                    error_count += 1
                    
                # 1000ê°œ ë‹¨ìœ„ë¡œ ì§„í–‰ ìƒí™© ì¶œë ¥
                if idx % 1000 == 0:
                    print(f"   ğŸ“ˆ {idx}ê°œ ì²˜ë¦¬ ì™„ë£Œ (ì„±ê³µ: {success_count}, ì‹¤íŒ¨: {error_count})")
                    
            except Exception as e:
                print(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ({os.path.basename(file_path)}): {e}")
                error_count += 1
        
        # CSV ì €ì¥
        if products_results or reviews_results:
            # ì œí’ˆ ì •ë³´ CSV ì €ì¥
            if products_results:
                df_products = pd.DataFrame(products_results)
                df_products.to_csv(products_csv_path, index=False, encoding="utf-8-sig")
                print(f"ğŸ“¦ ì œí’ˆ ì •ë³´ CSV ì €ì¥ ì™„ë£Œ: {len(products_results)}ê°œ")
            
            # ë¦¬ë·° ì •ë³´ CSV ì €ì¥
            if reviews_results:
                df_reviews = pd.DataFrame(reviews_results)
                df_reviews.to_csv(reviews_csv_path, index=False, encoding="utf-8-sig")
                print(f"ğŸ“ ë¦¬ë·° ì •ë³´ CSV ì €ì¥ ì™„ë£Œ: {len(reviews_results)}ê°œ")
            
            print(f"\nâœ… CSV ë³€í™˜ ì™„ë£Œ!")
            print(f"ğŸ“Š ì´ ì²˜ë¦¬: {len(html_files)}ê°œ")
            print(f"âœ… ì„±ê³µ: {success_count}ê°œ")
            print(f"âŒ ì‹¤íŒ¨: {error_count}ê°œ")
            print(f"ğŸ“ˆ ì„±ê³µë¥ : {(success_count/len(html_files)*100):.1f}%")
            print(f"ğŸ“¦ ì œí’ˆ ì •ë³´: {len(products_results)}ê°œ")
            print(f"ğŸ“ ë¦¬ë·° ì •ë³´: {len(reviews_results)}ê°œ")
            print(f"ğŸ“ ì œí’ˆ CSV ê²½ë¡œ: {products_csv_path}")
            print(f"ğŸ“ ë¦¬ë·° CSV ê²½ë¡œ: {reviews_csv_path}")
            
            # ê²°ê³¼ë¥¼ XComì— ì €ì¥
            context['task_instance'].xcom_push(key='products_csv_path', value=products_csv_path)
            context['task_instance'].xcom_push(key='reviews_csv_path', value=reviews_csv_path)
            context['task_instance'].xcom_push(key='total_processed', value=len(html_files))
            context['task_instance'].xcom_push(key='success_count', value=success_count)
            context['task_instance'].xcom_push(key='error_count', value=error_count)
            context['task_instance'].xcom_push(key='total_products', value=len(products_results))
            context['task_instance'].xcom_push(key='total_reviews', value=len(reviews_results))
            
            return True
        else:
            print("âŒ ì €ì¥í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
            
    except Exception as e:
        print(f"âŒ CSV ë³€í™˜ ì¤‘ ì˜¤ë¥˜: {e}")
        return False

def conversion_summary(**context):
    """ë³€í™˜ ì‘ì—… ìš”ì•½ì„ ì¶œë ¥í•©ë‹ˆë‹¤."""
    print(f"\n{'='*50}")
    print(f"ğŸ“‹ HTML to CSV ë³€í™˜ ì‘ì—… ìš”ì•½")
    print(f"{'='*50}")
    
    try:
        products_csv_path = context['task_instance'].xcom_pull(task_ids='convert_html_to_csv', key='products_csv_path')
        reviews_csv_path = context['task_instance'].xcom_pull(task_ids='convert_html_to_csv', key='reviews_csv_path')
        total_processed = context['task_instance'].xcom_pull(task_ids='convert_html_to_csv', key='total_processed')
        success_count = context['task_instance'].xcom_pull(task_ids='convert_html_to_csv', key='success_count')
        error_count = context['task_instance'].xcom_pull(task_ids='convert_html_to_csv', key='error_count')
        total_products = context['task_instance'].xcom_pull(task_ids='convert_html_to_csv', key='total_products')
        total_reviews = context['task_instance'].xcom_pull(task_ids='convert_html_to_csv', key='total_reviews')
        s3_upload_results = context['task_instance'].xcom_pull(task_ids='upload_to_s3', key='s3_upload_results')
        
        if total_processed is not None:
            print(f"ğŸ‰ HTML to CSV ë³€í™˜ ì™„ë£Œ!")
            print(f"ğŸ“Š ì´ ì²˜ë¦¬ëœ íŒŒì¼: {total_processed}ê°œ")
            print(f"âœ… ì„±ê³µ: {success_count}ê°œ")
            print(f"âŒ ì‹¤íŒ¨: {error_count}ê°œ")
            print(f"ğŸ“¦ ì¶”ì¶œëœ ì œí’ˆ: {total_products}ê°œ")
            print(f"ğŸ“ ì¶”ì¶œëœ ë¦¬ë·°: {total_reviews}ê°œ")
            if products_csv_path:
                print(f"ğŸ“ ì œí’ˆ CSV íŒŒì¼ ê²½ë¡œ: {products_csv_path}")
            if reviews_csv_path:
                print(f"ğŸ“ ë¦¬ë·° CSV íŒŒì¼ ê²½ë¡œ: {reviews_csv_path}")
            
            if s3_upload_results:
                print(f"\nâ˜ï¸ S3 ì—…ë¡œë“œ ê²°ê³¼:")
                for key, path in s3_upload_results.items():
                    if key == 'html_files':
                        print(f"   {key}: {path} (HTML íŒŒì¼ë“¤)")
                    else:
                        print(f"   {key}: {path}")
        else:
            print("âŒ ë³€í™˜ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        print("="*50)
        return True
        
    except Exception as e:
        print(f"âŒ ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        return True

def upload_to_s3_task(**context):
    """CSV íŒŒì¼ë“¤ê³¼ HTML íŒŒì¼ë“¤ì„ S3ì— ì—…ë¡œë“œ"""
    products_csv_path = context['task_instance'].xcom_pull(task_ids='convert_html_to_csv', key='products_csv_path')
    reviews_csv_path = context['task_instance'].xcom_pull(task_ids='convert_html_to_csv', key='reviews_csv_path')
    total_products = context['task_instance'].xcom_pull(task_ids='convert_html_to_csv', key='total_products')
    total_reviews = context['task_instance'].xcom_pull(task_ids='convert_html_to_csv', key='total_reviews')
    base_html_dir = context['task_instance'].xcom_pull(task_ids='find_html_files', key='base_html_dir')
    
    if not products_csv_path and not reviews_csv_path:
        print("âŒ ì—…ë¡œë“œí•  CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return False
    
    # S3 ì—…ë¡œë” ì´ˆê¸°í™”
    s3_uploader = S3Uploader()
    
    if not s3_uploader.s3_client:
        print("âŒ S3 í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return False
    
    today = datetime.now().strftime('%Y-%m-%d')
    upload_results = {}
    
    try:
        # ì œí’ˆ ì •ë³´ CSV ì—…ë¡œë“œ
        if products_csv_path and os.path.exists(products_csv_path):
            filename = os.path.basename(products_csv_path)
            s3_key = f"wconcept_csvs/{today}/{filename}"
            
            print(f"ğŸ”„ ì œí’ˆ ì •ë³´ CSV ì—…ë¡œë“œ ì¤‘: {products_csv_path} â†’ s3://{s3_uploader.bucket_name}/{s3_key}")
            
            s3_uploader.s3_client.upload_file(
                products_csv_path,
                s3_uploader.bucket_name,
                s3_key,
                ExtraArgs={'ContentType': 'text/csv'}
            )
            
            upload_results['product_csv'] = f"s3://{s3_uploader.bucket_name}/{s3_key}"
            print(f"âœ… ì œí’ˆ ì •ë³´ CSV S3 ì—…ë¡œë“œ ì™„ë£Œ: {total_products}ê°œ ì œí’ˆ")
        
        # ë¦¬ë·° ì •ë³´ CSV ì—…ë¡œë“œ
        if reviews_csv_path and os.path.exists(reviews_csv_path):
            filename = os.path.basename(reviews_csv_path)
            s3_key = f"wconcept_csvs/{today}/{filename}"
            
            print(f"ğŸ”„ ë¦¬ë·° ì •ë³´ CSV ì—…ë¡œë“œ ì¤‘: {reviews_csv_path} â†’ s3://{s3_uploader.bucket_name}/{s3_key}")
            
            s3_uploader.s3_client.upload_file(
                reviews_csv_path,
                s3_uploader.bucket_name,
                s3_key,
                ExtraArgs={'ContentType': 'text/csv'}
            )
            
            upload_results['review_csv'] = f"s3://{s3_uploader.bucket_name}/{s3_key}"
            print(f"âœ… ë¦¬ë·° ì •ë³´ CSV S3 ì—…ë¡œë“œ ì™„ë£Œ: {total_reviews}ê°œ ë¦¬ë·°")
        
        # HTML íŒŒì¼ë“¤ S3 ì—…ë¡œë“œ
        if base_html_dir and os.path.exists(base_html_dir):
            print(f"\nğŸ”„ HTML íŒŒì¼ë“¤ S3 ì—…ë¡œë“œ ì‹œì‘: {base_html_dir}")
            html_upload_count = 0
            
            for root, dirs, files in os.walk(base_html_dir):
                for file in files:
                    if file.endswith('.html'):
                        local_file_path = os.path.join(root, file)
                        
                        # ìƒëŒ€ ê²½ë¡œ ê³„ì‚° (base_html_dir ê¸°ì¤€)
                        relative_path = os.path.relpath(local_file_path, base_html_dir)
                        s3_key = f"wconcept_htmls/{today}/{relative_path}"
                        
                        try:
                            s3_uploader.s3_client.upload_file(
                                local_file_path,
                                s3_uploader.bucket_name,
                                s3_key,
                                ExtraArgs={'ContentType': 'text/html'}
                            )
                            html_upload_count += 1
                            
                            if html_upload_count % 100 == 0:
                                print(f"   ğŸ“ˆ {html_upload_count}ê°œ HTML íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ")
                                
                        except Exception as e:
                            print(f"   âŒ HTML íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨: {file} - {e}")
            
            upload_results['html_files'] = f"s3://{s3_uploader.bucket_name}/wconcept_htmls/{today}/"
            print(f"âœ… HTML íŒŒì¼ë“¤ S3 ì—…ë¡œë“œ ì™„ë£Œ: {html_upload_count}ê°œ íŒŒì¼")
        
        # ì—…ë¡œë“œ ê²°ê³¼ë¥¼ XComì— ì €ì¥
        context['task_instance'].xcom_push(key='s3_upload_results', value=upload_results)
        return True
        
    except Exception as e:
        print(f"âŒ S3 ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False

def cleanup_html_files_task(**context):
    """S3 ì—…ë¡œë“œ ì„±ê³µ í›„ ë¡œì»¬ HTML/CSV íŒŒì¼ ì •ë¦¬"""
    base_html_dir = context['task_instance'].xcom_pull(task_ids='find_html_files', key='base_html_dir')
    s3_upload_results = context['task_instance'].xcom_pull(task_ids='upload_to_s3', key='s3_upload_results')
    products_csv_path = context['task_instance'].xcom_pull(task_ids='convert_html_to_csv', key='products_csv_path')
    reviews_csv_path = context['task_instance'].xcom_pull(task_ids='convert_html_to_csv', key='reviews_csv_path')

    # S3 ì—…ë¡œë“œê°€ ì„±ê³µí•œ ê²½ìš°ì—ë§Œ ì •ë¦¬
    if not s3_upload_results:
        print("âŒ S3 ì—…ë¡œë“œê°€ ì„±ê³µí•˜ì§€ ì•Šì•„ ë¡œì»¬ íŒŒì¼ì„ ì •ë¦¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return False

    # 1) HTML íŒŒì¼ ë° ë””ë ‰í† ë¦¬ ì •ë¦¬
    if not base_html_dir or not os.path.exists(base_html_dir):
        print(f"âš ï¸ HTML ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {base_html_dir}")
    else:
        print(f"\nğŸ—‘ï¸ HTML íŒŒì¼ë“¤ ì •ë¦¬ ì‹œì‘: {base_html_dir}")
        total_deleted_html = 0

        # ëª¨ë“  í•˜ìœ„ ë””ë ‰í† ë¦¬ì—ì„œ HTML íŒŒì¼ ì‚­ì œ
        for root, dirs, files in os.walk(base_html_dir, topdown=False):
            for file in files:
                if file.endswith('.html'):
                    file_path = os.path.join(root, file)
                    try:
                        os.remove(file_path)
                        total_deleted_html += 1
                    except Exception as e:
                        print(f"     âŒ ì‚­ì œ ì‹¤íŒ¨: {file} - {e}")

            # ë¹ˆ ë””ë ‰í† ë¦¬ ì‚­ì œ (root ë””ë ‰í† ë¦¬ ì œì™¸)
            if root != base_html_dir:
                try:
                    if not os.listdir(root):
                        os.rmdir(root)
                        print(f"     âœ… ë¹ˆ ë””ë ‰í† ë¦¬ ì‚­ì œ: {root}")
                except Exception as e:
                    print(f"     âš ï¸ ë””ë ‰í† ë¦¬ ì‚­ì œ ì‹¤íŒ¨: {root} - {e}")

        # ì „ì²´ HTML ë””ë ‰í† ë¦¬ê°€ ë¹„ì–´ìˆìœ¼ë©´ ì‚­ì œ
        try:
            if not os.listdir(base_html_dir):
                os.rmdir(base_html_dir)
                print(f"   âœ… ì „ì²´ HTML ë””ë ‰í† ë¦¬ ì‚­ì œ: {base_html_dir}")
        except Exception as e:
            print(f"   âš ï¸ ì „ì²´ ë””ë ‰í† ë¦¬ ì‚­ì œ ì‹¤íŒ¨: {base_html_dir} - {e}")

        print(f"ğŸ“Š HTML íŒŒì¼ ì •ë¦¬ ì™„ë£Œ: {total_deleted_html}ê°œ íŒŒì¼ ì‚­ì œ")

    # 2) ë³€í™˜ëœ CSV íŒŒì¼ ì •ë¦¬ (ì œí’ˆ/ë¦¬ë·°)
    total_deleted_csv = 0
    for csv_path, label in [
        (products_csv_path, 'ì œí’ˆ CSV'),
        (reviews_csv_path, 'ë¦¬ë·° CSV'),
    ]:
        if csv_path and os.path.exists(csv_path):
            try:
                os.remove(csv_path)
                total_deleted_csv += 1
                print(f"âœ… ì‚­ì œ ì™„ë£Œ: {label} - {csv_path}")
            except Exception as e:
                print(f"âŒ ì‚­ì œ ì‹¤íŒ¨: {label} - {csv_path} | {e}")

    if total_deleted_csv == 0:
        print("â„¹ï¸ ì‚­ì œí•  ë¡œì»¬ CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print(f"ğŸ“Š CSV íŒŒì¼ ì •ë¦¬ ì™„ë£Œ: {total_deleted_csv}ê°œ íŒŒì¼ ì‚­ì œ")

    return True

# íƒœìŠ¤í¬ ì •ì˜
start_task = DummyOperator(
    task_id='start_html_to_csv_conversion',
    dag=dag
)

find_html_task = PythonOperator(
    task_id='find_html_files',
    python_callable=find_html_files,
    op_kwargs={
        'base_html_dir': '/home/tjddml/w/html'  # HTML í¬ë¡¤ëŸ¬ DAGì˜ ì¶œë ¥ ê²½ë¡œ
    },
    dag=dag
)

convert_task = PythonOperator(
    task_id='convert_html_to_csv',
    python_callable=convert_html_to_csv,
    op_kwargs={
        'base_html_dir': '/home/tjddml/w/html'  # HTML í¬ë¡¤ëŸ¬ DAGì˜ ì¶œë ¥ ê²½ë¡œ
    },
    dag=dag
)

summary_task = PythonOperator(
    task_id='conversion_summary',
    python_callable=conversion_summary,
    dag=dag
)

# S3ì— ì—…ë¡œë“œ
upload_task = PythonOperator(
    task_id='upload_to_s3',
    python_callable=upload_to_s3_task,
    dag=dag
)

# HTML íŒŒì¼ ì •ë¦¬
cleanup_task = PythonOperator(
    task_id='cleanup_html_files',
    python_callable=cleanup_html_files_task,
    dag=dag
)

# ë‹¤ìŒ DAG íŠ¸ë¦¬ê±° (29cm step1)
trigger_step1 = TriggerDagRunOperator(
    task_id="trigger_step1_link_collector",
    trigger_dag_id="step1_link_collector",
    wait_for_completion=False,
    conf={"triggered_by": "wconcept_html_to_csv_converter"},
)

# íƒœìŠ¤í¬ ì˜ì¡´ì„± ì„¤ì • (S3 ì—…ë¡œë“œ ë° HTML ì •ë¦¬ í¬í•¨)
start_task >> find_html_task >> convert_task >> upload_task >> cleanup_task >> summary_task >> trigger_step1
