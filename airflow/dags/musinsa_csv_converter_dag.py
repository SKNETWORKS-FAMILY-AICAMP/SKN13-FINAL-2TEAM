from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.trigger_dagrun import TriggerDagRunOperator
from airflow.models import Variable
from datetime import datetime, timedelta
import os
import json
import re
import pandas as pd
from bs4 import BeautifulSoup
import boto3
from botocore.exceptions import ClientError
import re

# DAG ê¸°ë³¸ ì„¤ì •
default_args = {
    'owner': 'airflow',
    'start_date': datetime(2025, 8, 5),
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    'email_on_failure': False,
    'email_on_retry': False,
}

class S3Uploader:
    """S3ì— CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ëŠ” í´ëž˜ìŠ¤"""
    
    def __init__(self):
        # Airflow Variablesì—ì„œ AWS í‚¤ ê°€ì ¸ì˜¤ê¸°
        try:
            self.aws_access_key_id = Variable.get("AWS_ACCESS_KEY_ID", default_var=None)
            self.aws_secret_access_key = Variable.get("AWS_SECRET_ACCESS_KEY", default_var=None)
            self.aws_region = Variable.get("AWS_REGION", default_var="ap-northeast-2")
            self.bucket_name = Variable.get("S3_BUCKET_NAME", default_var="ivle-malle")
        except Exception as e:
            print(f"âŒ Airflow Variables ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.aws_access_key_id = None
            self.aws_secret_access_key = None
            self.aws_region = "ap-northeast-2"
            self.bucket_name = "ivle-malle"
        
        # AWS í‚¤ ì¡´ìž¬ ì—¬ë¶€ í™•ì¸
        print(f"ðŸ” AWS ì„¤ì • í™•ì¸:")
        print(f"   AWS_ACCESS_KEY_ID: {'ì„¤ì •ë¨' if self.aws_access_key_id else 'ì„¤ì •ë˜ì§€ ì•ŠìŒ'}")
        print(f"   AWS_SECRET_ACCESS_KEY: {'ì„¤ì •ë¨' if self.aws_secret_access_key else 'ì„¤ì •ë˜ì§€ ì•ŠìŒ'}")
        print(f"   AWS_REGION: {self.aws_region}")
        print(f"   S3_BUCKET_NAME: {self.bucket_name}")
        
        # AWS í‚¤ê°€ ì—†ìœ¼ë©´ S3 í´ë¼ì´ì–¸íŠ¸ë¥¼ Noneìœ¼ë¡œ ì„¤ì •
        if not self.aws_access_key_id or not self.aws_secret_access_key:
            print("âŒ AWS í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
            print("   Airflow UI â†’ Admin â†’ Variablesì—ì„œ ë‹¤ìŒ ë³€ìˆ˜ë“¤ì„ ì„¤ì •í•´ì£¼ì„¸ìš”:")
            print("   - AWS_ACCESS_KEY_ID")
            print("   - AWS_SECRET_ACCESS_KEY")
            print("   - AWS_REGION (ì„ íƒì‚¬í•­)")
            print("   - S3_BUCKET_NAME (ì„ íƒì‚¬í•­)")
            self.s3_client = None
            return
        
        # S3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        try:
            self.s3_client = boto3.client(
                's3',
                aws_access_key_id=self.aws_access_key_id,
                aws_secret_access_key=self.aws_secret_access_key,
                region_name=self.aws_region
            )
            print(f"âœ… S3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ: {self.bucket_name}")
        except Exception as e:
            print(f"âŒ S3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.s3_client = None

def normalize_musinsa_image_url(url):
    """
    ë¬´ì‹ ì‚¬ ì´ë¯¸ì§€ URLì„ ì˜¬ë°”ë¥¸ í˜•íƒœë¡œ ì •ê·œí™”í•©ë‹ˆë‹¤.
    https:/images/goods_img/... â†’ https://image.msscdn.net/thumbnails/images/goods_img/..._big.jpg
    """
    if not url:
        return None
    
    url = str(url).strip()
    
    # ì´ë¯¸ ì˜¬ë°”ë¥¸ í˜•íƒœë¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
    if url.startswith('https://image.msscdn.net/thumbnails/images/goods_img/'):
        return url
    
    # https:/images/goods_img/ í˜•íƒœë¥¼ ì •ê·œí™”
    if url.startswith('https:/images/goods_img/'):
        url = url.replace('https:/images/goods_img/', 'https://image.msscdn.net/thumbnails/images/goods_img/')
    
    # https://images/goods_img/ í˜•íƒœë¥¼ ì •ê·œí™”
    elif url.startswith('https://images/goods_img/'):
        url = url.replace('https://images/goods_img/', 'https://image.msscdn.net/thumbnails/images/goods_img/')
    
    # ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš°
    elif url.startswith('/images/goods_img/'):
        url = 'https://image.msscdn.net/thumbnails/images/goods_img/' + url[18:]
    
    # íŒŒì¼ëª… ì ‘ë¯¸ì‚¬ í†µì¼ (_500, _1000, _org ë“± â†’ _big)
    if '_500.' in url or '_1000.' in url or '_org.' in url:
        url = re.sub(r'_(500|1000|org)\.(jpg|jpeg|png)$', r'_big.\2', url, flags=re.IGNORECASE)
    
    # _big ì ‘ë¯¸ì‚¬ê°€ ì—†ìœ¼ë©´ ì¶”ê°€
    if not re.search(r'_big\.(jpg|jpeg|png)$', url, flags=re.IGNORECASE):
        url = re.sub(r'\.(jpg|jpeg|png)$', r'_big.\1', url, flags=re.IGNORECASE)
    
    return url

def extract_notice_info(html_content):
    """ìƒí’ˆ ê³ ì‹œ ì •ë³´ ì¶”ì¶œ (ì œí’ˆì†Œìž¬, ìƒ‰ìƒ)"""
    soup = BeautifulSoup(html_content, 'html.parser')
    info = {"ì œí’ˆì†Œìž¬": None, "ìƒ‰ìƒ": None}
    
    # ë°©ë²• 1: h3 íƒœê·¸ì—ì„œ "ìƒí’ˆ ê³ ì‹œ" ì°¾ê¸°
    dl = None
    for h3 in soup.find_all("h3"):
        if "ìƒí’ˆ ê³ ì‹œ" in h3.get_text():
            dl = h3.find_next("dl")
            break
    
    # ë°©ë²• 2: dl íƒœê·¸ ì§ì ‘ ì°¾ê¸°
    if not dl:
        dl = soup.find("dl")
    
    if dl:
        items = list(dl.find_all(["dt", "dd"]))
        for i in range(0, len(items)-1, 2):
            key = items[i].get_text(strip=True)
            val = items[i+1].get_text(strip=True)
            if "ì†Œìž¬" in key and not info["ì œí’ˆì†Œìž¬"]:
                info["ì œí’ˆì†Œìž¬"] = val
            if "ìƒ‰ìƒ" in key and not info["ìƒ‰ìƒ"]:
                info["ìƒ‰ìƒ"] = val
    else:
        # ë°©ë²• 3: tableì—ì„œ ì°¾ê¸°
        table = None
        for th in soup.find_all(['th', 'td']):
            txt = th.get_text(strip=True)
            if "ìƒí’ˆ ê³ ì‹œ" in txt or "ìƒí’ˆì •ë³´ì œê³µê³ ì‹œ" in txt:
                table = th.find_parent('table')
                break
        
        if table:
            for row in table.find_all('tr'):
                tds = row.find_all(['th', 'td'])
                if len(tds) >= 2:
                    key = tds[0].get_text(strip=True)
                    val = tds[1].get_text(strip=True)
                    if "ì†Œìž¬" in key and not info["ì œí’ˆì†Œìž¬"]:
                        info["ì œí’ˆì†Œìž¬"] = val
                    if "ìƒ‰ìƒ" in key and not info["ìƒ‰ìƒ"]:
                        info["ìƒ‰ìƒ"] = val
    
    return info

def extract_likes_from_html(soup):
    """ì¢‹ì•„ìš” ìˆ˜ ì¶”ì¶œ"""
    try:
        like_span = soup.select_one("div.sc-16dm5t2-0.jnCysi > div > span")
        if like_span:
            return like_span.get_text(strip=True)
    except:
        pass
    return None

def extract_product_reviews(html_content, product_no, product_info=None):
    """HTMLì—ì„œ ë¦¬ë·° ë°ì´í„° ì¶”ì¶œ"""
    soup = BeautifulSoup(html_content, 'html.parser')
    reviews = []
    
    # ìƒí’ˆ ì •ë³´ì—ì„œ ë¦¬ë·° ê°¯ìˆ˜ì™€ í‰ê·  í‰ì  ê°€ì ¸ì˜¤ê¸°
    total_review_count = product_info.get('ì´ ë¦¬ë·°ê°¯ìˆ˜') if product_info else None
    avg_rating = product_info.get('í‰ê·  í‰ì ') if product_info else None
    
    review_blocks = soup.select("div.review-list-item__Container-sc-13zantg-0")
    for block in review_blocks:
        nickname_tag = block.select_one("span.UserProfileSection__Nickname-sc-12rrbzu-4")
        nickname = nickname_tag.get_text(strip=True) if nickname_tag else None
        
        date_tag = block.select_one("span.UserProfileSection__PurchaseDate-sc-12rrbzu-5")
        date = date_tag.get_text(strip=True) if date_tag else None
        
        rating_tag = block.select_one("div.StarsScore__Container-sc-1qjm0n5-0 span.text-body_13px_semi")
        rating = rating_tag.get_text(strip=True) if rating_tag else None

        user_info_tag = block.select_one("div.UserInfoGoodsOptionSection__UserInfoGoodsOption-sc-1p4ukac-1 span")
        user_info = user_info_tag.get_text(strip=True) if user_info_tag else None

        purchase_option_tag = block.select_one("div.UserInfoGoodsOptionSection__UserInfoGoodsOption-sc-1p4ukac-1 span:nth-child(3)")
        purchase_option = purchase_option_tag.get_text(strip=True) if purchase_option_tag else None

        content_tag = block.select_one("div.ExpandableContent__ContentContainer-sc-gj5b23-1 span.text-body_13px_reg.text-black")
        content = content_tag.get_text(strip=True) if content_tag else None

        img_tag = block.select_one("div.ExpandableImageGroup__Container-sc-uvnh0q-0 img")
        img_url = normalize_musinsa_image_url(img_tag['src']) if img_tag else None

        # ë¦¬ë·° ë‚´ìš© ì •ë¦¬ (ì¤„ë°”ê¿ˆ ì œê±°)
        if content:
            content = content.replace('\n', ' ').replace('\r', ' ').strip()

        reviews.append({
            'ì œí’ˆë²ˆí˜¸': product_no,
            'ìž‘ì„±ìžëª…': nickname,
            'ë¦¬ë·°ê°¯ìˆ˜': total_review_count,
            'í‰ê· í‰ì ': avg_rating,
            'ìž‘ì„±ë‚ ì§œ': date,
            'êµ¬ë§¤ì˜µì…˜': purchase_option,
            'ë¦¬ë·°ë‚´ìš©': content,
            'ë¦¬ë·°ì´ë¯¸ì§€URL': img_url,
            'ìž‘ì„±ìžì •ë³´': user_info,
        })

    return reviews

def extract_data_from_html(html_content):
    """HTMLì—ì„œ ìƒí’ˆ ë°ì´í„° ì¶”ì¶œ"""
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # JSON ë°ì´í„° ì¶”ì¶œ
    script_tag = soup.find('script', id='pdp-data')
    if not script_tag:
        return None
    
    script_text = script_tag.string
    match = re.search(r'window\.__MSS__\.product\.state\s*=\s*(\{.*?\});', script_text, re.S)
    if not match:
        return None
    
    try:
        data = json.loads(match.group(1))
    except json.JSONDecodeError as e:
        print(f"JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        return None
    
    # ìƒí’ˆ ì •ë³´ ì¶”ì¶œ
    product_info = {
        'ì œí’ˆë²ˆí˜¸': data.get('goodsNo'),
        'ì œí’ˆì´ë¦„': data.get('goodsNm'),
        'ì œí’ˆëŒ€ë¶„ë¥˜': data.get('category', {}).get('categoryDepth1Name'),
        'ì œí’ˆì†Œë¶„ë¥˜': data.get('category', {}).get('categoryDepth2Name'),
        'ì›ê°€': data.get('goodsPrice', {}).get('normalPrice'),
        'í• ì¸ê°€': data.get('goodsPrice', {}).get('salePrice'),
        'ì‚¬ì§„': normalize_musinsa_image_url(data.get('thumbnailImageUrl', '')) if data.get('thumbnailImageUrl') else '',
        'ë¸Œëžœë“œ': data.get('brandInfo', {}).get('brandName'),
        'ì„±ë³„': data.get('sex'),
        'ì´ ë¦¬ë·°ê°¯ìˆ˜': data.get('goodsReview', {}).get('totalCount'),
        'í‰ê·  í‰ì ': data.get('goodsReview', {}).get('satisfactionScore'),
        'ì¢‹ì•„ìš”': data.get('wishCount'),
    }

    # ì¢‹ì•„ìš” ìˆ˜ê°€ ì—†ìœ¼ë©´ HTMLì—ì„œ ì§ì ‘ ì¶”ì¶œ
    if not product_info['ì¢‹ì•„ìš”']:
        product_info['ì¢‹ì•„ìš”'] = extract_likes_from_html(soup)

    # ìƒí’ˆ ê³ ì‹œ ì •ë³´ ì¶”ê°€
    notice_info = extract_notice_info(html_content)
    product_info.update(notice_info)
    
    return product_info

def process_html_files(folder_path):
    """í´ë” ë‚´ HTML íŒŒì¼ë“¤ì„ ì²˜ë¦¬í•˜ì—¬ ìƒí’ˆ ì •ë³´ì™€ ë¦¬ë·° ì •ë³´ ë°˜í™˜"""
    if not os.path.exists(folder_path):
        print(f"âŒ í´ë”ê°€ ì¡´ìž¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {folder_path}")
        return [], []
    
    html_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.html')]
    
    if not html_files:
        print(f"âš ï¸ {folder_path}ì— HTML íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return [], []
    
    print(f"ðŸ”„ {folder_path}ì—ì„œ {len(html_files)}ê°œ HTML íŒŒì¼ ì²˜ë¦¬ ì¤‘...")
    
    all_product_info = []
    all_reviews = []
    
    for html_file in html_files:
        try:
            with open(html_file, 'r', encoding='utf-8') as file:
                html_content = file.read()
            
            # ìƒí’ˆ ì •ë³´ ì¶”ì¶œ
            product_data = extract_data_from_html(html_content)
            if product_data:
                all_product_info.append(product_data)
                
                # ë¦¬ë·° ì •ë³´ ì¶”ì¶œ
                if product_data.get('ì œí’ˆë²ˆí˜¸'):
                    reviews = extract_product_reviews(html_content, product_data['ì œí’ˆë²ˆí˜¸'], product_data)
                    all_reviews.extend(reviews)
                    print(f"âœ… {os.path.basename(html_file)} ì²˜ë¦¬ ì™„ë£Œ (ìƒí’ˆ: 1ê°œ, ë¦¬ë·°: {len(reviews)}ê°œ)")
                else:
                    print(f"âœ… {os.path.basename(html_file)} ì²˜ë¦¬ ì™„ë£Œ (ìƒí’ˆ: 1ê°œ, ë¦¬ë·°: 0ê°œ)")
            else:
                print(f"âš ï¸ {os.path.basename(html_file)} ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨")
                
        except Exception as e:
            print(f"âŒ {os.path.basename(html_file)} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

    return all_product_info, all_reviews

def convert_html_to_csv_task(**context):
    """HTML íŒŒì¼ë“¤ì„ CSVë¡œ ë³€í™˜í•˜ëŠ” ë©”ì¸ íƒœìŠ¤í¬"""
    today = datetime.now().strftime('%Y-%m-%d')
    
    # ìž‘ì—… ë””ë ‰í† ë¦¬ ì„¤ì •
    work_dir = f"/home/tjddml/musinsa_csv_converter/{today}"
    os.makedirs(work_dir, exist_ok=True)
    
    # HTML íŒŒì¼ ë””ë ‰í† ë¦¬
    html_base_dir = f"/home/tjddml/musinsa_htmls/{today}"
    
    if not os.path.exists(html_base_dir):
        print(f"âŒ HTML ë””ë ‰í† ë¦¬ê°€ ì¡´ìž¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {html_base_dir}")
        print("   í¬ë¡¤ë§ DAGê°€ ì •ìƒì ìœ¼ë¡œ ì‹¤í–‰ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return
    
    # ì¹´í…Œê³ ë¦¬ë³„ ì²˜ë¦¬
    categories = ['top', 'bottom']
    all_product_data = []
    all_review_data = []
    
    for category in categories:
        print(f"\n{'='*30}")
        print(f"ðŸ”„ {category} ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬ ì‹œìž‘")
        print(f"{'='*30}")
        
        category_html_dir = os.path.join(html_base_dir, category)
        
        if os.path.exists(category_html_dir):
            # HTML íŒŒì¼ ì²˜ë¦¬
            product_info, reviews = process_html_files(category_html_dir)
            
            # ì¹´í…Œê³ ë¦¬ ì •ë³´ ì¶”ê°€
            for product in product_info:
                product['ì¹´í…Œê³ ë¦¬'] = category
            for review in reviews:
                review['ì¹´í…Œê³ ë¦¬'] = category
            
            all_product_data.extend(product_info)
            all_review_data.extend(reviews)
            
            print(f"âœ… {category} ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬ ì™„ë£Œ: ìƒí’ˆ {len(product_info)}ê°œ, ë¦¬ë·° {len(reviews)}ê°œ")
        else:
            print(f"âš ï¸ {category} ì¹´í…Œê³ ë¦¬ HTML ë””ë ‰í† ë¦¬ê°€ ì¡´ìž¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {category_html_dir}")
    
    # ìƒí’ˆ ì •ë³´ CSV ì €ìž¥
    final_product_csv = None
    if all_product_data:
        df_products = pd.DataFrame(all_product_data)
        
        # ìƒí’ˆë§í¬ ì»¬ëŸ¼ ì¶”ê°€
        df_products['ìƒí’ˆë§í¬'] = 'https://www.musinsa.com/products/' + df_products['ì œí’ˆë²ˆí˜¸'].astype(str)
        
        # ì»¬ëŸ¼ëª… ë³€ê²½ ë° ìˆœì„œ ì¡°ì •
        column_mapping = {
            'ì œí’ˆë²ˆí˜¸': 'ìƒí’ˆì½”ë“œ',
            'ì œí’ˆì´ë¦„': 'ìƒí’ˆëª…',
            'ë¸Œëžœë“œ': 'ë¸Œëžœë“œëª…',
            'ì œí’ˆëŒ€ë¶„ë¥˜': 'ëŒ€ë¶„ë¥˜',
            'ì œí’ˆì†Œë¶„ë¥˜': 'ì†Œë¶„ë¥˜',
            'ì›ê°€': 'ì›ê°€',
            'í• ì¸ê°€': 'í• ì¸ê°€',
            'ì„±ë³„': 'ì„±ë³„',
            'ì‚¬ì§„': 'ì´ë¯¸ì§€url',
            'ì œí’ˆì†Œìž¬': 'ì†Œìž¬',
            'ìƒ‰ìƒ': 'ìƒ‰ìƒ',
            'ì¢‹ì•„ìš”': 'ì¢‹ì•„ìš” ìˆ˜',
            'ìƒí’ˆë§í¬': 'ìƒí’ˆë§í¬',
        }
        
        # ì›í•˜ëŠ” ìˆœì„œë¡œ ì¹¼ëŸ¼ ìž¬ë°°ì—´
        desired_columns = [
            'ìƒí’ˆì½”ë“œ', 'ìƒí’ˆëª…', 'ë¸Œëžœë“œëª…', 'ëŒ€ë¶„ë¥˜', 'ì†Œë¶„ë¥˜',
            'ì›ê°€', 'í• ì¸ê°€', 'ì„±ë³„', 'ì´ë¯¸ì§€url', 'ì†Œìž¬',
            'ìƒ‰ìƒ', 'ì¢‹ì•„ìš” ìˆ˜', 'ìƒí’ˆë§í¬'
        ]
        
        # ì¹¼ëŸ¼ëª… ë³€ê²½
        df_products = df_products.rename(columns=column_mapping)
        
        # ì›í•˜ëŠ” ìˆœì„œë¡œ ì¹¼ëŸ¼ ì„ íƒ
        available_columns = [col for col in desired_columns if col in df_products.columns]
        df_products = df_products[available_columns]
        
        # CSV ì €ìž¥ (ì‹œê°„ë³„ êµ¬ë¶„)
        current_time = datetime.now().strftime('%H%M')
        final_product_csv = os.path.join(work_dir, f'product_info_{current_time}.csv')
        df_products.to_csv(final_product_csv, index=False, encoding='utf-8-sig')
        
        print(f"âœ… ìƒí’ˆ ì •ë³´ CSV ì €ìž¥ ì™„ë£Œ: {len(df_products)}ê°œ ì œí’ˆ -> {final_product_csv}")
    
    # ë¦¬ë·° ì •ë³´ CSV ì €ìž¥
    final_review_csv = None
    if all_review_data:
        df_reviews = pd.DataFrame(all_review_data)
        
        # ë¦¬ë·° ì¹¼ëŸ¼ ìˆœì„œ ì¡°ì •
        desired_review_columns = [
            'ì œí’ˆë²ˆí˜¸', 'ìž‘ì„±ìžëª…', 'ë¦¬ë·°ê°¯ìˆ˜', 'í‰ê· í‰ì ', 'ìž‘ì„±ë‚ ì§œ', 
            'êµ¬ë§¤ì˜µì…˜', 'ë¦¬ë·°ë‚´ìš©', 'ë¦¬ë·°ì´ë¯¸ì§€URL', 'ìž‘ì„±ìžì •ë³´'
        ]
        
        # ì¡´ìž¬í•˜ëŠ” ì¹¼ëŸ¼ë§Œ ì„ íƒí•˜ì—¬ ìˆœì„œ ì¡°ì •
        available_review_columns = [col for col in desired_review_columns if col in df_reviews.columns]
        df_reviews = df_reviews[available_review_columns]
        
        # CSV ì €ìž¥ (ì‹œê°„ë³„ êµ¬ë¶„)
        current_time = datetime.now().strftime('%H%M')
        final_review_csv = os.path.join(work_dir, f'reviews_{current_time}.csv')
        df_reviews.to_csv(final_review_csv, index=False, encoding='utf-8-sig')
        
        print(f"âœ… ë¦¬ë·° ì •ë³´ CSV ì €ìž¥ ì™„ë£Œ: {len(df_reviews)}ê°œ ë¦¬ë·° -> {final_review_csv}")
    
    # XComì— ê²°ê³¼ ì €ìž¥
    context['task_instance'].xcom_push(key='final_product_csv', value=final_product_csv)
    context['task_instance'].xcom_push(key='final_review_csv', value=final_review_csv)
    context['task_instance'].xcom_push(key='total_products', value=len(all_product_data))
    context['task_instance'].xcom_push(key='total_reviews', value=len(all_review_data))
    context['task_instance'].xcom_push(key='work_dir', value=work_dir)
    context['task_instance'].xcom_push(key='html_base_dir', value=html_base_dir)

def upload_to_s3_task(**context):
    """CSV íŒŒì¼ë“¤ì„ S3ì— ì—…ë¡œë“œ"""
    final_product_csv = context['task_instance'].xcom_pull(key='final_product_csv')
    final_review_csv = context['task_instance'].xcom_pull(key='final_review_csv')
    total_products = context['task_instance'].xcom_pull(key='total_products')
    total_reviews = context['task_instance'].xcom_pull(key='total_reviews')
    
    if not final_product_csv and not final_review_csv:
        print("âŒ ì—…ë¡œë“œí•  CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return False
    
    # S3 ì—…ë¡œë” ì´ˆê¸°í™”
    s3_uploader = S3Uploader()
    
    if not s3_uploader.s3_client:
        print("âŒ S3 í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return False
    
    today = datetime.now().strftime('%Y-%m-%d')
    upload_results = {}
    
    try:
        # ìƒí’ˆ ì •ë³´ CSV ì—…ë¡œë“œ
        if final_product_csv and os.path.exists(final_product_csv):
            filename = os.path.basename(final_product_csv)
            s3_key = f"musinsa_csvs/{today}/{filename}"
            
            print(f"ðŸ”„ ìƒí’ˆ ì •ë³´ CSV ì—…ë¡œë“œ ì¤‘: {final_product_csv} â†’ s3://{s3_uploader.bucket_name}/{s3_key}")
            
            s3_uploader.s3_client.upload_file(
                final_product_csv,
                s3_uploader.bucket_name,
                s3_key,
                ExtraArgs={'ContentType': 'text/csv'}
            )
            
            upload_results['product_csv'] = f"s3://{s3_uploader.bucket_name}/{s3_key}"
            print(f"âœ… ìƒí’ˆ ì •ë³´ CSV S3 ì—…ë¡œë“œ ì™„ë£Œ: {total_products}ê°œ ì œí’ˆ")
        
        # ë¦¬ë·° ì •ë³´ CSV ì—…ë¡œë“œ
        if final_review_csv and os.path.exists(final_review_csv):
            filename = os.path.basename(final_review_csv)
            s3_key = f"musinsa_reviews/{today}/{filename}"
            
            print(f"ðŸ”„ ë¦¬ë·° ì •ë³´ CSV ì—…ë¡œë“œ ì¤‘: {final_review_csv} â†’ s3://{s3_uploader.bucket_name}/{s3_key}")
            
            s3_uploader.s3_client.upload_file(
                final_review_csv,
                s3_uploader.bucket_name,
                s3_key,
                ExtraArgs={'ContentType': 'text/csv'}
            )
            
            upload_results['review_csv'] = f"s3://{s3_uploader.bucket_name}/{s3_key}"
            print(f"âœ… ë¦¬ë·° ì •ë³´ CSV S3 ì—…ë¡œë“œ ì™„ë£Œ: {total_reviews}ê°œ ë¦¬ë·°")
        
        # ì—…ë¡œë“œ ê²°ê³¼ë¥¼ XComì— ì €ìž¥
        context['task_instance'].xcom_push(key='s3_upload_results', value=upload_results)
        return True
        
    except Exception as e:
        print(f"âŒ S3 ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False

def cleanup_html_files_task(**context):
    """S3 ì—…ë¡œë“œ ì„±ê³µ í›„ HTML íŒŒì¼ë“¤ ì •ë¦¬"""
    html_base_dir = context['task_instance'].xcom_pull(key='html_base_dir')
    s3_upload_results = context['task_instance'].xcom_pull(key='s3_upload_results')
    
    # S3 ì—…ë¡œë“œê°€ ì„±ê³µí•œ ê²½ìš°ì—ë§Œ ì •ë¦¬
    if not s3_upload_results:
        print("âŒ S3 ì—…ë¡œë“œê°€ ì„±ê³µí•˜ì§€ ì•Šì•„ HTML íŒŒì¼ì„ ì •ë¦¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return False
    
    if not html_base_dir or not os.path.exists(html_base_dir):
        print(f"âš ï¸ HTML ë””ë ‰í† ë¦¬ê°€ ì¡´ìž¬í•˜ì§€ ì•ŠìŒ: {html_base_dir}")
        return True
    
    print(f"\nðŸ—‘ï¸ HTML íŒŒì¼ë“¤ ì •ë¦¬ ì‹œìž‘: {html_base_dir}")
    
    categories = ['top', 'bottom']
    total_deleted = 0
    
    for category in categories:
        category_dir = os.path.join(html_base_dir, category)
        
        if not os.path.exists(category_dir):
            continue
        
        print(f"   ðŸ”„ {category} ì¹´í…Œê³ ë¦¬ HTML íŒŒì¼ ì‚­ì œ ì¤‘...")
        
        html_files = [f for f in os.listdir(category_dir) if f.endswith('.html')]
        deleted_count = 0
        
        for html_file in html_files:
            html_path = os.path.join(category_dir, html_file)
            try:
                os.remove(html_path)
                deleted_count += 1
            except Exception as e:
                print(f"     âŒ ì‚­ì œ ì‹¤íŒ¨: {html_file} - {e}")
        
        print(f"     âœ… {category}: {deleted_count}ê°œ ì‚­ì œ ì™„ë£Œ")
        total_deleted += deleted_count
        
        # ë¹ˆ ë””ë ‰í† ë¦¬ ì‚­ì œ
        try:
            if not os.listdir(category_dir):
                os.rmdir(category_dir)
                print(f"     âœ… ë¹ˆ ë””ë ‰í† ë¦¬ ì‚­ì œ: {category}")
        except Exception as e:
            print(f"     âš ï¸ ë””ë ‰í† ë¦¬ ì‚­ì œ ì‹¤íŒ¨: {category} - {e}")
    
    # ì „ì²´ HTML ë””ë ‰í† ë¦¬ê°€ ë¹„ì–´ìžˆìœ¼ë©´ ì‚­ì œ
    try:
        if not os.listdir(html_base_dir):
            os.rmdir(html_base_dir)
            print(f"   âœ… ì „ì²´ HTML ë””ë ‰í† ë¦¬ ì‚­ì œ: {html_base_dir}")
    except Exception as e:
        print(f"   âš ï¸ ì „ì²´ ë””ë ‰í† ë¦¬ ì‚­ì œ ì‹¤íŒ¨: {html_base_dir} - {e}")
    
    print(f"ðŸ“Š HTML íŒŒì¼ ì •ë¦¬ ì™„ë£Œ: {total_deleted}ê°œ íŒŒì¼ ì‚­ì œ")
    return True

def summary_task(**context):
    """ìž‘ì—… ê²°ê³¼ ìš”ì•½"""
    final_product_csv = context['task_instance'].xcom_pull(key='final_product_csv')
    final_review_csv = context['task_instance'].xcom_pull(key='final_review_csv')
    total_products = context['task_instance'].xcom_pull(key='total_products')
    total_reviews = context['task_instance'].xcom_pull(key='total_reviews')
    s3_upload_results = context['task_instance'].xcom_pull(key='s3_upload_results')
    
    print(f"\n{'='*50}")
    print("ðŸ“Š CSV ë³€í™˜ ìž‘ì—… ì™„ë£Œ ìš”ì•½")
    print(f"{'='*50}")
    
    if final_product_csv:
        print(f"ðŸ“ ìƒí’ˆ ì •ë³´ CSV: {final_product_csv}")
        print(f"   ì´ {total_products}ê°œ ì œí’ˆ ì²˜ë¦¬")
    
    if final_review_csv:
        print(f"ðŸ“ ë¦¬ë·° ì •ë³´ CSV: {final_review_csv}")
        print(f"   ì´ {total_reviews}ê°œ ë¦¬ë·° ì²˜ë¦¬")
    
    if s3_upload_results:
        print(f"\nâ˜ï¸ S3 ì—…ë¡œë“œ ê²°ê³¼:")
        for key, path in s3_upload_results.items():
            print(f"   {key}: {path}")
    
    print(f"{'='*50}")

# DAG ì •ì˜
with DAG(
    dag_id='musinsa_csv_converter_dag',
    default_args=default_args,
    description='ë¬´ì‹ ì‚¬ HTML íŒŒì¼ì„ CSVë¡œ ë³€í™˜í•˜ëŠ” DAG',
    schedule_interval=None,
    catchup=False,
    tags=['musinsa', 'csv', 'converter'],
) as dag:

    # HTMLì„ CSVë¡œ ë³€í™˜
    convert_task = PythonOperator(
        task_id='convert_html_to_csv',
        python_callable=convert_html_to_csv_task,
        doc_md="""
        ## HTMLì„ CSVë¡œ ë³€í™˜
        - HTML íŒŒì¼ì—ì„œ ìƒí’ˆ ì •ë³´ì™€ ë¦¬ë·° ì •ë³´ ì¶”ì¶œ
        - ì¹´í…Œê³ ë¦¬ë³„ í†µí•© CSV íŒŒì¼ ìƒì„±
        """
    )

    # S3ì— ì—…ë¡œë“œ
    upload_task = PythonOperator(
        task_id='upload_to_s3',
        python_callable=upload_to_s3_task,
        doc_md="""
        ## S3 ì—…ë¡œë“œ
        - ìƒì„±ëœ CSV íŒŒì¼ë“¤ì„ S3ì— ì—…ë¡œë“œ
        """
    )

    # HTML íŒŒì¼ ì •ë¦¬
    cleanup_task = PythonOperator(
        task_id='cleanup_html_files',
        python_callable=cleanup_html_files_task,
        doc_md="""
        ## HTML íŒŒì¼ ì •ë¦¬
        - S3 ì—…ë¡œë“œ ì„±ê³µ í›„ ì›ë³¸ HTML íŒŒì¼ë“¤ ì‚­ì œ
        """
    )

    # ìž‘ì—… ê²°ê³¼ ìš”ì•½
    summary_task_op = PythonOperator(
        task_id='summary',
        python_callable=summary_task,
        doc_md="""
        ## ìž‘ì—… ê²°ê³¼ ìš”ì•½
        - ì „ì²´ ë³€í™˜ ìž‘ì—… ê²°ê³¼ ì¶œë ¥
        """
    )

    # Wì»¨ì…‰ í¬ë¡¤ë§ DAG íŠ¸ë¦¬ê±° íƒœìŠ¤í¬
    trigger_wconcept_crawler = TriggerDagRunOperator(
        task_id='trigger_wconcept_crawler',
        trigger_dag_id='wconcept_product_crawling',
        wait_for_completion=False,  # ë¹„ë™ê¸° ì‹¤í–‰
        poke_interval=10,  # 10ì´ˆë§ˆë‹¤ ìƒíƒœ í™•ì¸
        allowed_states=['success'],  # ì„±ê³µ ìƒíƒœì—ì„œë§Œ íŠ¸ë¦¬ê±°
        failed_states=['failed'],  # ì‹¤íŒ¨ ìƒíƒœì—ì„œ íŠ¸ë¦¬ê±° ì•ˆí•¨
        dag=dag
    )

    # íƒœìŠ¤í¬ ì˜ì¡´ì„± ì„¤ì • (Wì»¨ì…‰ í¬ë¡¤ë§ DAG íŠ¸ë¦¬ê±° í¬í•¨)
    convert_task >> upload_task >> cleanup_task >> summary_task_op >> trigger_wconcept_crawler