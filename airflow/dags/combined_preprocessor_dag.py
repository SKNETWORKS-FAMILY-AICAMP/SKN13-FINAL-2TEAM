from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.dummy import DummyOperator
from airflow.models import Variable
import pandas as pd
import os
import boto3
from botocore.exceptions import ClientError
import logging
from typing import List, Dict, Optional
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# DAG ê¸°ë³¸ ì„¤ì •
default_args = {
    'owner': 'data_team',
    'depends_on_past': False,
    'start_date': datetime(2023, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=2),
    'catchup': False
}

# DAG ì •ì˜
dag = DAG(
    'combined_preprocessor',
    default_args=default_args,
    description='ê° ì‚¬ì´íŠ¸ë³„ ì „ì²˜ë¦¬ëœ CSVë“¤ì„ í•©ì³ì„œ S3ì— ì—…ë¡œë“œí•˜ëŠ” DAG',
    schedule_interval=None,
    max_active_runs=1,
    tags=['preprocessing', 'data-combination', 's3-upload', 'ecommerce']
)

class S3Uploader:
    """S3 ì—…ë¡œë“œ ìœ í‹¸ë¦¬í‹°"""
    def __init__(self):
        # Removed try-except for Airflow Variables, directly use os.getenv
        self.aws_access_key_id = os.getenv("AWS_ACCESS_KEY_ID")
        self.aws_secret_access_key = os.getenv("AWS_SECRET_ACCESS_KEY")
        self.aws_region = os.getenv("AWS_REGION", "ap-northeast-2")
        self.bucket_name = os.getenv("S3_BUCKET_NAME", "ivle-malle")

        if not self.aws_access_key_id or not self.aws_secret_access_key:
            print("âŒ AWS í‚¤ ì—†ìŒ: S3 ì—…ë¡œë“œ ë¹„í™œì„±í™”")
            self.s3_client = None
            return

        try:
            self.s3_client = boto3.client(
                's3',
                aws_access_key_id=self.aws_access_key_id,
                aws_secret_access_key=self.aws_secret_access_key,
                region_name=self.aws_region
            )
            print(f"âœ… S3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”: {self.bucket_name}")
        except Exception as e:
            print(f"âŒ S3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.s3_client = None

    def upload_file(self, file_path: str, s3_key: str) -> bool:
        """íŒŒì¼ì„ S3ì— ì—…ë¡œë“œ"""
        if not self.s3_client:
            print("âŒ S3 í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            return False

        try:
            self.s3_client.upload_file(file_path, self.bucket_name, s3_key)
            print(f"âœ… S3 ì—…ë¡œë“œ ì™„ë£Œ: {file_path} â†’ s3://{self.bucket_name}/{s3_key}")
            return True
        except ClientError as e:
            print(f"âŒ S3 ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
        except Exception as e:
            print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            return False

class DataCombiner:
    """ë°ì´í„° ê²°í•© í´ë˜ìŠ¤"""
    def __init__(self):
        self.combined_df = None
        self.site_stats = {}
        
    def load_and_validate_csv(self, file_path: str, site_name: str) -> Optional[pd.DataFrame]:
        """CSV íŒŒì¼ ë¡œë“œ ë° ê²€ì¦"""
        try:
            if not os.path.exists(file_path):
                print(f"âš ï¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {file_path}")
                return None
                
            df = pd.read_csv(file_path, encoding='utf-8-sig')
            print(f"âœ… {site_name} ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df)}í–‰, {len(df.columns)}ì»¬ëŸ¼")
            
            # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
            required_columns = ['ìƒí’ˆì½”ë“œ', 'ìƒí’ˆëª…', 'í•œê¸€ë¸Œëœë“œëª…', 'ì˜ì–´ë¸Œëœë“œëª…', 
                              'ëŒ€ë¶„ë¥˜', 'ì†Œë¶„ë¥˜', 'ì›ê°€', 'í• ì¸ê°€', 'ì„±ë³„', 'ì‚¬ì´íŠ¸ëª…']
            
            missing_columns = [col for col in required_columns if col not in df.columns]
            if missing_columns:
                print(f"âš ï¸ {site_name} í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing_columns}")
                return None
                
            return df
            
        except Exception as e:
            print(f"âŒ {site_name} ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def combine_data(self, csv_files: Dict[str, str]) -> pd.DataFrame:
        """ì—¬ëŸ¬ CSV íŒŒì¼ì„ í•˜ë‚˜ë¡œ ê²°í•©"""
        print("ğŸ”„ ë°ì´í„° ê²°í•© ì‹œì‘...")
        
        dataframes = []
        
        for site_name, file_path in csv_files.items():
            df = self.load_and_validate_csv(file_path, site_name)
            if df is not None:
                # ì‚¬ì´íŠ¸ëª… ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì¶”ê°€
                if 'ì‚¬ì´íŠ¸ëª…' not in df.columns:
                    df['ì‚¬ì´íŠ¸ëª…'] = site_name
                
                dataframes.append(df)
                self.site_stats[site_name] = len(df)
                print(f"âœ… {site_name}: {len(df)}í–‰ ì¶”ê°€")
            else:
                print(f"âŒ {site_name}: ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ë¡œ ì œì™¸")
        
        if not dataframes:
            raise ValueError("ë¡œë“œí•  ìˆ˜ ìˆëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
        
        # ë°ì´í„°í”„ë ˆì„ ê²°í•©
        self.combined_df = pd.concat(dataframes, ignore_index=True)
        
        print(f"âœ… ë°ì´í„° ê²°í•© ì™„ë£Œ: ì´ {len(self.combined_df)}í–‰")
        return self.combined_df
    
    def validate_combined_data(self) -> bool:
        """ê²°í•©ëœ ë°ì´í„° ê²€ì¦"""
        if self.combined_df is None:
            print("âŒ ê²°í•©ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return False
        
        print("ğŸ” ê²°í•©ëœ ë°ì´í„° ê²€ì¦ ì¤‘...")
        
        # ê¸°ë³¸ ê²€ì¦
        total_rows = len(self.combined_df)
        total_columns = len(self.combined_df.columns)
        
        print(f"ğŸ“Š ë°ì´í„° í¬ê¸°: {total_rows}í–‰ Ã— {total_columns}ì»¬ëŸ¼")
        
        # ì¤‘ë³µ ìƒí’ˆì½”ë“œ í™•ì¸
        duplicate_codes = self.combined_df['ìƒí’ˆì½”ë“œ'].duplicated().sum()
        if duplicate_codes > 0:
            print(f"âš ï¸ ì¤‘ë³µ ìƒí’ˆì½”ë“œ: {duplicate_codes}ê°œ")
        else:
            print("âœ… ì¤‘ë³µ ìƒí’ˆì½”ë“œ ì—†ìŒ")
        
        # ì‚¬ì´íŠ¸ë³„ í†µê³„
        print("\nğŸ“ˆ ì‚¬ì´íŠ¸ë³„ í†µê³„:")
        site_counts = self.combined_df['ì‚¬ì´íŠ¸ëª…'].value_counts()
        for site, count in site_counts.items():
            print(f"  â€¢ {site}: {count}ê°œ")
        
        # ëŒ€ë¶„ë¥˜ë³„ í†µê³„
        print("\nğŸ“‚ ëŒ€ë¶„ë¥˜ë³„ í†µê³„:")
        category_counts = self.combined_df['ëŒ€ë¶„ë¥˜'].value_counts()
        for category, count in category_counts.items():
            print(f"  â€¢ {category}: {count}ê°œ")
        
        # ë¸Œëœë“œ í†µê³„
        unique_brands = self.combined_df['í•œê¸€ë¸Œëœë“œëª…'].nunique()
        print(f"\nğŸ·ï¸ ê³ ìœ  ë¸Œëœë“œ ìˆ˜: {unique_brands}ê°œ")
        
        return True
    
    def save_combined_data(self, output_path: str) -> str:
        """ê²°í•©ëœ ë°ì´í„° ì €ì¥"""
        if self.combined_df is None:
            raise ValueError("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        # CSV ì €ì¥
        self.combined_df.to_csv(output_path, index=False, encoding='utf-8-sig')
        
        file_size = os.path.getsize(output_path) / (1024 * 1024)  # MB
        print(f"âœ… ê²°í•©ëœ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {output_path}")
        print(f"ğŸ“ íŒŒì¼ í¬ê¸°: {file_size:.2f} MB")
        
        return output_path

# DAG íƒœìŠ¤í¬ í•¨ìˆ˜ë“¤
def load_and_combine_data_task(**context):
    """ë°ì´í„° ë¡œë“œ ë° ê²°í•© íƒœìŠ¤í¬"""
    print("ğŸš€ ë°ì´í„° ë¡œë“œ ë° ê²°í•© ì‹œì‘")

    # Dynamic path for 29cm data
    base_29cm_dir = r"/home/tjddml/airflow/Complete"
    latest_date_29cm_dir = None
    if os.path.exists(base_29cm_dir):
        for entry in os.listdir(base_29cm_dir):
            full_path = os.path.join(base_29cm_dir, entry)
            if os.path.isdir(full_path):
                try:
                    datetime.strptime(entry, '%Y_%m_%d')
                    if not latest_date_29cm_dir or entry > latest_date_29cm_dir:
                        latest_date_29cm_dir = entry
                except ValueError:
                    pass

    # Dynamic path for musinsa data
    base_musinsa_dir = r"/home/tjddml/airflow/musinsa"
    latest_musinsa_file = None
    if os.path.exists(base_musinsa_dir):
        musinsa_files = [f for f in os.listdir(base_musinsa_dir) if f.endswith('.csv')]
        if musinsa_files:
            # Sort files by name (which contains timestamp) to get the latest
            latest_musinsa_file = os.path.join(base_musinsa_dir, max(musinsa_files))

    # ì „ì²˜ë¦¬ëœ CSV íŒŒì¼ ê²½ë¡œë“¤
    csv_files = {
        'wconcept': '/home/tjddml/airflow/dags/wconcept/processed_products.csv',
    }

    if latest_musinsa_file:
        csv_files['musinsa'] = latest_musinsa_file
    else:
        print(f"âš ï¸ Musinsa íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {base_musinsa_dir}")

    if latest_date_29cm_dir:
        csv_files['29cm'] = os.path.join(base_29cm_dir, latest_date_29cm_dir, "29cm_preprocessing_product_info.csv")
    else:
        print(f"âš ï¸ 29cm íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {base_29cm_dir}")

    # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” íŒŒì¼ë§Œ í•„í„°ë§
    existing_files = {}
    for site, file_path in csv_files.items():
        if os.path.exists(file_path):
            existing_files[site] = file_path
        else:
            print(f"âš ï¸ {site} íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {file_path}")
    
    if not existing_files:
        raise ValueError("ë¡œë“œí•  ìˆ˜ ìˆëŠ” ì „ì²˜ë¦¬ëœ CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")

    # ë°ì´í„° ê²°í•©
    combiner = DataCombiner()
    combined_df = combiner.combine_data(existing_files)

    # ê²€ì¦ (ë¡œê·¸ ì¶œë ¥ìš©)
    combiner.validate_combined_data()

    # ê²°í•©ëœ ë°ì´í„°ë¥¼ ì„ì‹œ CSV íŒŒì¼ë¡œ ì €ì¥í•˜ê³  ê²½ë¡œë¥¼ XComìœ¼ë¡œ ì „ë‹¬
    output_dir = "combined"
    os.makedirs(output_dir, exist_ok=True)
    combined_output_path = os.path.join(output_dir, "processed_products_combined.csv")
    combined_df.to_csv(combined_output_path, index=False, encoding="utf-8-sig")
    print(f"âœ… ê²°í•©ëœ ë°ì´í„° ì„ì‹œ ì €ì¥ ì™„ë£Œ: {combined_output_path}")
    
    context['task_instance'].xcom_push(key='combined_csv_path', value=combined_output_path)

    return f"ë°ì´í„° ê²°í•© ì™„ë£Œ: {len(combined_df)}í–‰"

def save_combined_csv_task(**context):
    """ê²°í•©ëœ CSV ì €ì¥ íƒœìŠ¤í¬"""
    print("ğŸ’¾ ê²°í•©ëœ CSV ì €ì¥ ì‹œì‘")
    
    combiner = context['task_instance'].xcom_pull(key='combiner')
    output_path = "combined/processed_products_combined.csv"
    
    saved_path = combiner.save_combined_data(output_path)
    
    context['task_instance'].xcom_push(key='local_csv_path', value=saved_path)
    
    return f"ë¡œì»¬ CSV ì €ì¥ ì™„ë£Œ: {saved_path}"

def upload_to_s3_task(**context):
    """S3 ì—…ë¡œë“œ íƒœìŠ¤í¬"""
    print("â˜ï¸ S3 ì—…ë¡œë“œ ì‹œì‘")

    local_path = context['task_instance'].xcom_pull(key='combined_csv_path')

    if not local_path or not os.path.exists(local_path):
        raise ValueError(f"ì—…ë¡œë“œí•  íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {local_path}")

    # S3 ì—…ë¡œë” ì´ˆê¸°í™”
    uploader = S3Uploader()

    # í˜„ì¬ ë‚ ì§œë¥¼ í¬í•¨í•œ S3 í‚¤ ìƒì„±
    current_date = datetime.now().strftime("%Y%m%d")
    s3_key = f"processed_data/combined_products_{current_date}.csv"

    # S3 ì—…ë¡œë“œ
    success = uploader.upload_file(local_path, s3_key)

    if success:
        context['task_instance'].xcom_push(key='s3_key', value=s3_key)
        return f"S3 ì—…ë¡œë“œ ì™„ë£Œ: s3://{uploader.bucket_name}/{s3_key}"
    else:
        raise Exception("S3 ì—…ë¡œë“œ ì‹¤íŒ¨")

def generate_summary_report_task(**context):
    """ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„± íƒœìŠ¤í¬"""
    print("ğŸ“‹ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±")

    combined_csv_path = context['task_instance'].xcom_pull(key='combined_csv_path')
    s3_key = context['task_instance'].xcom_pull(key='s3_key')

    if not combined_csv_path or not os.path.exists(combined_csv_path):
        print("âš ï¸ ê²°í•©ëœ CSV íŒŒì¼ ê²½ë¡œê°€ ì—†ê±°ë‚˜ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•Šì•„ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return "ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨"
    
    # CSV íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œí•˜ì—¬ combiner ê°ì²´ ì¬ìƒì„±
    combiner = DataCombiner()
    combiner.combined_df = pd.read_csv(combined_csv_path, encoding="utf-8-sig")
    combiner.site_stats = combiner.combined_df['ì‚¬ì´íŠ¸ëª…'].value_counts().to_dict()

    # ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
    report_lines = []
    report_lines.append("=" * 60)
    report_lines.append("ğŸ“Š ì „ì²˜ë¦¬ëœ ë°ì´í„° ê²°í•© ì™„ë£Œ ë¦¬í¬íŠ¸")
    report_lines.append("=" * 60)
    report_lines.append(f"ğŸ“… ì²˜ë¦¬ ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append("")
    
    # ì „ì²´ í†µê³„
    total_rows = len(combiner.combined_df)
    total_columns = len(combiner.combined_df.columns)
    report_lines.append(f"ğŸ“ˆ ì „ì²´ í†µê³„:")
    report_lines.append(f"  â€¢ ì´ ìƒí’ˆ ìˆ˜: {total_rows:,}ê°œ")
    report_lines.append(f"  â€¢ ì´ ì»¬ëŸ¼ ìˆ˜: {total_columns}ê°œ")
    report_lines.append("")
    
    # ì‚¬ì´íŠ¸ë³„ í†µê³„
    report_lines.append("ğŸª ì‚¬ì´íŠ¸ë³„ í†µê³„:")
    site_counts = combiner.combined_df['ì‚¬ì´íŠ¸ëª…'].value_counts()
    for site, count in site_counts.items():
        percentage = (count / total_rows) * 100
        report_lines.append(f"  â€¢ {site}: {count:,}ê°œ ({percentage:.1f}%)")
    report_lines.append("")
    
    # ëŒ€ë¶„ë¥˜ë³„ í†µê³„
    report_lines.append("ğŸ“‚ ëŒ€ë¶„ë¥˜ë³„ í†µê³„:")
    category_counts = combiner.combined_df['ëŒ€ë¶„ë¥˜'].value_counts()
    for category, count in category_counts.items():
        percentage = (count / total_rows) * 100
        report_lines.append(f"  â€¢ {category}: {count:,}ê°œ ({percentage:.1f}%)")
    report_lines.append("")
    
    # ë¸Œëœë“œ í†µê³„
    unique_brands = combiner.combined_df['í•œê¸€ë¸Œëœë“œëª…'].nunique()
    report_lines.append(f"ğŸ·ï¸ ë¸Œëœë“œ í†µê³„:")
    report_lines.append(f"  â€¢ ê³ ìœ  ë¸Œëœë“œ ìˆ˜: {unique_brands:,}ê°œ")
    report_lines.append("")
    
    # S3 ì •ë³´
    if s3_key:
        report_lines.append("â˜ï¸ S3 ì €ì¥ ì •ë³´:")
        report_lines.append(f"  â€¢ ì €ì¥ ìœ„ì¹˜: s3://{S3Uploader().bucket_name}/{s3_key}")
        report_lines.append("")
    
    report_lines.append("âœ… ëª¨ë“  ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    report_lines.append("=" * 60)
    
    # ë¦¬í¬íŠ¸ ì¶œë ¥
    report = "\n".join(report_lines)
    print(report)
    
    # ë¦¬í¬íŠ¸ë¥¼ íŒŒì¼ë¡œ ì €ì¥
    report_path = "combined/processing_report.txt"
    os.makedirs(os.path.dirname(report_path), exist_ok=True)
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"ğŸ“„ ë¦¬í¬íŠ¸ ì €ì¥ ì™„ë£Œ: {report_path}")
    
    return "ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ"

# DAG íƒœìŠ¤í¬ ì •ì˜
start = DummyOperator(task_id='start', dag=dag)

load_and_combine_data = PythonOperator(
    task_id='load_and_combine_data',
    python_callable=load_and_combine_data_task,
    dag=dag
)

# save_combined_csv íƒœìŠ¤í¬ëŠ” load_and_combine_data_taskì— í†µí•©ë˜ë¯€ë¡œ ì œê±°

upload_to_s3 = PythonOperator(
    task_id='upload_to_s3',
    python_callable=upload_to_s3_task,
    dag=dag
)

generate_summary_report = PythonOperator(
    task_id='generate_summary_report',
    python_callable=generate_summary_report_task,
    dag=dag
)

end = DummyOperator(task_id='end', dag=dag)

# DAG ì˜ì¡´ì„± ì„¤ì •
start >> load_and_combine_data >> upload_to_s3 >> generate_summary_report >> end
