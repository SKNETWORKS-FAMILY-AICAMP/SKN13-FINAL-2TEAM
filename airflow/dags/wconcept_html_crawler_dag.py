from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.dummy import DummyOperator
from airflow.operators.trigger_dagrun import TriggerDagRunOperator
from airflow.models import Variable
import os
import csv
import time
import boto3
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
from selenium_stealth import stealth

def convert_wsl_path(path):
    """ê²½ë¡œ ë³€í™˜ í•¨ìˆ˜ (WSLì—ì„œ ì‹¤í–‰ ì‹œ ê·¸ëŒ€ë¡œ ë°˜í™˜)"""
    return path

# DAG ê¸°ë³¸ ì„¤ì •
default_args = {
    'owner': 'data_team',
    'depends_on_past': False,
    'start_date': datetime(2023, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=2),
    'catchup': False
}

# DAG ì •ì˜ - DAG_IDê°€ ìœ ë‹ˆí¬í•œì§€ í™•ì¸
dag = DAG(
    'wconcept_html_crawler',  # ë‹¤ë¥¸ DAGì™€ ì¤‘ë³µë˜ì§€ ì•Šë„ë¡ ë³€ê²½
    default_args=default_args,
    description='Wì»¨ì…‰ CSV íŒŒì¼ë“¤ì˜ HTML í¬ë¡¤ë§ DAG',
    schedule_interval=None,  # ìˆ˜ë™ ì‹¤í–‰ ë˜ëŠ” íŠ¸ë¦¬ê±°ë¡œë§Œ ì‹¤í–‰
    max_active_runs=1,
    tags=['crawling', 'wconcept', 'html', 'ecommerce']
)

class S3Uploader:
    """S3 ì—…ë¡œë“œ ìœ í‹¸ë¦¬í‹°"""
    def __init__(self):
        try:
            self.aws_access_key_id = Variable.get("AWS_ACCESS_KEY_ID", default_var=None)
            self.aws_secret_access_key = Variable.get("AWS_SECRET_ACCESS_KEY", default_var=None)
            self.aws_region = Variable.get("AWS_REGION", default_var="ap-northeast-2")
            self.bucket_name = Variable.get("S3_BUCKET_NAME", default_var="ivle-malle")
        except Exception:
            self.aws_access_key_id = None
            self.aws_secret_access_key = None
            self.aws_region = "ap-northeast-2"
            self.bucket_name = "ivle-malle"

        if not self.aws_access_key_id or not self.aws_secret_access_key:
            print("âŒ AWS í‚¤ ì—†ìŒ: S3 ì—…ë¡œë“œ ë¹„í™œì„±í™”")
            self.s3_client = None
            return

        try:
            self.s3_client = boto3.client(
                's3',
                aws_access_key_id=self.aws_access_key_id,
                aws_secret_access_key=self.aws_secret_access_key,
                region_name=self.aws_region
            )
            print(f"âœ… S3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”: {self.bucket_name}")
        except Exception as e:
            print(f"âŒ S3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.s3_client = None

def setup_chrome_driver():
    """Chrome ë“œë¼ì´ë²„ ì„¤ì •"""
    try:
        options = webdriver.ChromeOptions()
        options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        options.add_argument('--window-size=1920,1080')
        options.add_argument('--disable-blink-features=AutomationControlled')
        options.add_argument('--disable-extensions')
        options.add_argument('--disable-plugins')
        options.add_experimental_option('excludeSwitches', ['enable-logging'])
        options.add_experimental_option('useAutomationExtension', False)
        
        # íƒ€ì„ì•„ì›ƒ ì„¤ì •
        options.add_argument('--page-load-strategy=eager')
        
        driver = webdriver.Chrome(options=options)
        driver.set_page_load_timeout(30)
        
        # ë´‡ ê°ì§€ ìš°íšŒ ì„¤ì •
        stealth(driver,
                languages=["ko-KR", "ko"],
                vendor="Google Inc.",
                platform="Win32",
                webgl_vendor="Intel Inc.",
                renderer="Intel Iris OpenGL Engine",
                fix_hairline=True,
                )
        
        return driver
    except Exception as e:
        print(f"âŒ Chrome ë“œë¼ì´ë²„ ì„¤ì • ì‹¤íŒ¨: {e}")
        raise

def get_latest_csv_files(base_path='/home/tjddml/w/link', **context):
    """`link/ìµœì‹ ë‚ ì§œí´ë”(YYYYMMDD)/` ì•ˆì˜ CSV íŒŒì¼ë“¤ë§Œ ì„ íƒ."""
    print(f"\n{'='*50}")
    print(f"ğŸ” ìµœì‹  CSV íŒŒì¼ ê²€ìƒ‰ ì‹œì‘: {base_path}")
    print(f"{'='*50}")

    try:
        print(f"ğŸ” ê²€ìƒ‰ ê²½ë¡œ: {base_path}")

        if not os.path.exists(base_path):
            print(f"âŒ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {base_path}")
            context['task_instance'].xcom_push(key='csv_files', value=[])
            context['task_instance'].xcom_push(key='latest_date', value=None)
            return []

        # 1) ë‚ ì§œ í´ë”(YYYYMMDD) ëª©ë¡ ìˆ˜ì§‘
        date_dirs = []
        for name in os.listdir(base_path):
            dir_path = os.path.join(base_path, name)
            if os.path.isdir(dir_path) and len(name) == 8 and name.isdigit():
                date_dirs.append(name)

        csv_files = []
        latest_date = None

        if date_dirs:
            # 2) ê°€ì¥ ìµœì‹  ë‚ ì§œ í´ë” ì„ íƒ
            latest_date = max(date_dirs)
            latest_dir_path = os.path.join(base_path, latest_date)
            print(f"ğŸ“… ìµœì‹  ë‚ ì§œ í´ë” ì„ íƒ: {latest_dir_path}")

            # 3) í•´ë‹¹ í´ë”ì˜ CSV íŒŒì¼ë§Œ ìˆ˜ì§‘
            for file in os.listdir(latest_dir_path):
                if file.lower().endswith('.csv'):
                    file_path = os.path.join(latest_dir_path, file)
                    csv_files.append(file_path)
                    print(f"âœ… ë°œê²¬: {file} (ê²½ë¡œ: {file_path})")

            print(f"\nğŸ“Š ìµœì‹  ë‚ ì§œ {latest_date} í´ë”ì˜ CSV íŒŒì¼ {len(csv_files)}ê°œ ë°œê²¬")
        else:
            # ë‚ ì§œ í´ë”ê°€ ì—†ëŠ” ê²½ìš°: í•˜ìœ„ í˜¸í™˜ - base_path ë°”ë¡œ ì•„ë˜ì˜ CSV íŒŒì¼ ì‚¬ìš©
            print("âš ï¸ ë‚ ì§œ í´ë”(YYYYMMDD)ë¥¼ ì°¾ì§€ ëª»í•´ ê¸°ë³¸ í´ë” ë‚´ CSVë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
            for file in os.listdir(base_path):
                if file.lower().endswith('.csv'):
                    file_path = os.path.join(base_path, file)
                    csv_files.append(file_path)
                    print(f"âœ… ë°œê²¬(í´ë°±): {file} (ê²½ë¡œ: {file_path})")

            print(f"\nğŸ“Š í´ë°±: ì´ {len(csv_files)}ê°œì˜ CSV íŒŒì¼ ë°œê²¬")

        # XCom ì €ì¥
        context['task_instance'].xcom_push(key='csv_files', value=csv_files)
        context['task_instance'].xcom_push(key='latest_date', value=latest_date)

        return csv_files

    except Exception as e:
        print(f"âŒ CSV íŒŒì¼ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
        context['task_instance'].xcom_push(key='csv_files', value=[])
        context['task_instance'].xcom_push(key='latest_date', value=None)
        return []

def scrape_and_save_html_from_csv(csv_filename, start_index=0, max_retries=3, restart_interval=50, max_links=None, **context):
    """
    í•˜ë‚˜ì˜ CSV íŒŒì¼ì„ ì½ì–´, ê·¸ ì•ˆì˜ ëª¨ë“  ë§í¬ì— ì ‘ì†í•˜ì—¬
    ê°ê°ì˜ HTML ì†ŒìŠ¤ë¥¼ ë³„ë„ì˜ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    """
    driver = None
    
    try:
        # CSV íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not os.path.exists(csv_filename):
            print(f"â—ï¸ [ì˜¤ë¥˜] '{csv_filename}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False
            
        # HTML ì €ì¥ì„ ìœ„í•œ ê¸°ë³¸ í´ë” ìƒì„±
        base_html_dir = '/home/tjddml/w/html'
        os.makedirs(base_html_dir, exist_ok=True)
        
        # CSV íŒŒì¼ ì´ë¦„ì— ë§ì¶° ê²°ê³¼ë¥¼ ì €ì¥í•  í´ë” ìƒì„±
        output_dir = f"{os.path.splitext(os.path.basename(csv_filename))[0]}_html"
        full_output_dir = os.path.join(base_html_dir, output_dir)
        os.makedirs(full_output_dir, exist_ok=True)
        
        print(f"\n{'='*50}")
        print(f"â–¶ '{os.path.basename(csv_filename)}' íŒŒì¼ ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
        print(f"â–¶ HTML íŒŒì¼ì€ '{full_output_dir}' í´ë”ì— ì €ì¥ë©ë‹ˆë‹¤.")
        print(f"{'='*50}")

        # Selenium WebDriver ì„¤ì •
        driver = setup_chrome_driver()

        # CSV íŒŒì¼ ì½ê¸°
        all_links = []
        with open(csv_filename, mode='r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            for row in reader:
                if 'ë§í¬' in row and row['ë§í¬']:
                    all_links.append(row['ë§í¬'])

        total_links = len(all_links)
        if total_links == 0:
            print(f"ğŸŸ¡ '{csv_filename}' íŒŒì¼ì— ì²˜ë¦¬í•  ë§í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ë§í¬ ìˆ˜ ì œí•œ (ì„ íƒì‚¬í•­)
        if max_links and max_links < total_links:
            all_links = all_links[:max_links]
            total_links = len(all_links)
            print(f"ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ: {max_links}ê°œ ë§í¬ë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
            
        links_to_scrape = all_links[start_index:]
        success_count = 0
        error_count = 0
        
        # ê° ë§í¬ ìˆœíšŒ ë° HTML ì €ì¥
        for i, url in enumerate(links_to_scrape, start=start_index + 1):
            # ë©”ëª¨ë¦¬ ìµœì í™”: ì¼ì • ê°„ê²©ìœ¼ë¡œ ë“œë¼ì´ë²„ ì¬ì‹œì‘
            if i > 1 and (i - 1) % restart_interval == 0:
                print(f"ğŸ”„ ë©”ëª¨ë¦¬ ìµœì í™”ë¥¼ ìœ„í•´ ë“œë¼ì´ë²„ ì¬ì‹œì‘ ì¤‘... ({i-1}ê°œ ì™„ë£Œ)")
                if driver:
                    driver.quit()
                driver = setup_chrome_driver()
                time.sleep(2)  # ì¬ì‹œì‘ í›„ ì ì‹œ ëŒ€ê¸°
                
            # ì§„í–‰ë¥  í‘œì‹œ (10ê°œë§ˆë‹¤)
            if i % 10 == 0 or i == len(links_to_scrape):
                progress = (i / len(links_to_scrape)) * 100
                print(f"ğŸ“Š ì§„í–‰ë¥ : {progress:.1f}% ({i}/{len(links_to_scrape)})")
            
            retry_count = 0
            
            while retry_count < max_retries:
                try:
                    product_id = url.split('/')[-1].split('?')[0]
                    html_filepath = os.path.join(full_output_dir, f"{product_id}.html")
                    
                    if os.path.exists(html_filepath):
                        print(f"ğŸŸ¡ ({i}/{total_links}) [{product_id}.html] ì´ë¯¸ ì¡´ì¬í•˜ë¯€ë¡œ ê±´ë„ˆëœë‹ˆë‹¤.")
                        success_count += 1  # ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ê²ƒë„ ì„±ê³µìœ¼ë¡œ ì¹´ìš´íŠ¸
                        break

                    driver.get(url)
                    WebDriverWait(driver, 20).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, "div.pdt_info"))
                    )
                    time.sleep(1)

                    html_source = driver.page_source
                    with open(html_filepath, "w", encoding='utf-8') as file:
                        file.write(html_source)
                    
                    success_count += 1
                    print(f"âœ… ({i}/{total_links}) [{product_id}.html] ì €ì¥ ì™„ë£Œ")
                    break  # ì„±ê³µí•˜ë©´ ë£¨í”„ ì¢…ë£Œ

                except TimeoutException:
                    retry_count += 1
                    if retry_count < max_retries:
                        print(f"âš ï¸ ({i}/{total_links}) URL ë¡œë”© ì‹œê°„ ì´ˆê³¼, ì¬ì‹œë„ {retry_count}/{max_retries}: {url}")
                        time.sleep(2)  # ì¬ì‹œë„ ì „ ëŒ€ê¸°
                    else:
                        error_count += 1
                        print(f"â—ï¸ ({i}/{total_links}) URL ë¡œë”© ì‹œê°„ ì´ˆê³¼ (ìµœì¢…): {url}")
                except Exception as e:
                    retry_count += 1
                    if retry_count < max_retries:
                        print(f"âš ï¸ ({i}/{total_links}) URL ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜, ì¬ì‹œë„ {retry_count}/{max_retries}: {url} | ì˜¤ë¥˜: {e}")
                        time.sleep(2)  # ì¬ì‹œë„ ì „ ëŒ€ê¸°
                    else:
                        error_count += 1
                        print(f"â—ï¸ ({i}/{total_links}) URL ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ìµœì¢…): {url} | ì˜¤ë¥˜: {e}")
        
        print(f"\nğŸ‰ '{os.path.basename(csv_filename)}' íŒŒì¼ì˜ ëª¨ë“  ë§í¬ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"ğŸ“Š ì„±ê³µ: {success_count}ê°œ, ì‹¤íŒ¨: {error_count}ê°œ")
        
        # ì„±ê³µë¥  ê³„ì‚°
        print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: {success_count}/{total_links} ë§í¬")
        
        # ê²°ê³¼ë¥¼ XComì— ì €ì¥
        result = {
            'csv_file': csv_filename,
            'total_links': total_links,
            'success_count': success_count,
            'error_count': error_count,
            'output_dir': full_output_dir
        }
        
        context['task_instance'].xcom_push(
            key=f'html_result_{os.path.basename(csv_filename)}', 
            value=result
        )
        
        return True
        
    except Exception as e:
        print(f"âŒ CSV íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        return False
        
    finally:
        if driver:
            try:
                driver.quit()
            except:
                pass

def process_all_csv_files(max_retries=3, restart_interval=50, wait_time=1, timeout=20, max_links=None, **context):
    """ë°œê²¬ëœ ëª¨ë“  CSV íŒŒì¼ì„ ì²˜ë¦¬í•˜ëŠ” í†µí•© íƒœìŠ¤í¬"""
    try:
        csv_files = context['task_instance'].xcom_pull(task_ids='find_latest_csv_files', key='csv_files')
        
        if not csv_files:
            print("âŒ ì²˜ë¦¬í•  CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return True  # ì‹¤íŒ¨ë¡œ ì²˜ë¦¬í•˜ì§€ ì•Šê³  ì •ìƒ ì™„ë£Œë¡œ ì²˜ë¦¬
        
        print(f"ğŸ”„ ì´ {len(csv_files)}ê°œì˜ CSV íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
        
        results = []
        
        for csv_file in csv_files:
            try:
                print(f"\nì²˜ë¦¬ ì¤‘: {csv_file}")
                result = scrape_and_save_html_from_csv(
                    csv_file, 
                    start_index=0, 
                    max_retries=max_retries, 
                    restart_interval=restart_interval,
                    max_links=max_links,
                    **context
                )
                results.append({
                    'file': csv_file,
                    'success': result
                })
            except Exception as e:
                print(f"âŒ {csv_file} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                results.append({
                    'file': csv_file,
                    'success': False,
                    'error': str(e)
                })
        
        # ê²°ê³¼ ìš”ì•½
        successful_files = [r for r in results if r['success']]
        failed_files = [r for r in results if not r['success']]
        
        print(f"\nğŸ‰ ëª¨ë“  CSV íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"ğŸ“Š ì„±ê³µ: {len(successful_files)}ê°œ, ì‹¤íŒ¨: {len(failed_files)}ê°œ")
        
        # XComì— ê²°ê³¼ ì €ì¥
        context['task_instance'].xcom_push(key='processing_results', value=results)
        
        return True
        
    except Exception as e:
        print(f"âŒ process_all_csv_files ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        raise

def html_crawling_summary(**context):
    """HTML í¬ë¡¤ë§ ì‘ì—… ê²°ê³¼ ìš”ì•½"""
    try:
        print("\n" + "="*50)
        print("ğŸ“Š HTML í¬ë¡¤ë§ ì‘ì—… ì™„ë£Œ ìš”ì•½")
        print("="*50)
        
        csv_files = context['task_instance'].xcom_pull(task_ids='find_latest_csv_files', key='csv_files')
        latest_date = context['task_instance'].xcom_pull(task_ids='find_latest_csv_files', key='latest_date')
        
        if not csv_files:
            print("âŒ ì²˜ë¦¬ëœ CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return True
        
        if latest_date:
            print(f"ğŸ“… ì²˜ë¦¬ëœ ë‚ ì§œ: {latest_date}")
        
        total_success = 0
        total_errors = 0
        processed_files = 0
        
        for csv_file in csv_files:
            csv_filename = os.path.basename(csv_file)
            result = context['task_instance'].xcom_pull(
                task_ids='process_all_csv_files',  # íƒœìŠ¤í¬ ID ëª…ì‹œ
                key=f'html_result_{csv_filename}'
            )
            
            if result:
                processed_files += 1
                print(f"âœ… {csv_filename}:")
                print(f"   ğŸ“Š ì´ ë§í¬: {result['total_links']}ê°œ")
                print(f"   âœ… ì„±ê³µ: {result['success_count']}ê°œ")
                print(f"   âŒ ì‹¤íŒ¨: {result['error_count']}ê°œ")
                print(f"   ğŸ“ ì €ì¥ ê²½ë¡œ: {result['output_dir']}")
                total_success += result['success_count']
                total_errors += result['error_count']
            else:
                print(f"âŒ {csv_filename}: ì²˜ë¦¬ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì „ì²´ ì„±ê³µë¥  ê³„ì‚°
        total_links = total_success + total_errors
        print(f"\nğŸ‰ ì „ì²´ HTML í¬ë¡¤ë§ ì™„ë£Œ!")
        print(f"ğŸ“Š ì²˜ë¦¬ëœ íŒŒì¼: {processed_files}ê°œ")
        print(f"ğŸ“Š ì´ ì„±ê³µ: {total_success}ê°œ, ì´ ì‹¤íŒ¨: {total_errors}ê°œ")
        print("="*50)
        
        return True
        
    except Exception as e:
        print(f"âŒ ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        return True  # ìš”ì•½ ì‹¤íŒ¨í•´ë„ DAG ì „ì²´ë¥¼ ì‹¤íŒ¨ë¡œ ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ

# íƒœìŠ¤í¬ ì •ì˜
start_task = DummyOperator(
    task_id='start_html_crawling',
    dag=dag
)

find_csv_task = PythonOperator(
    task_id='find_latest_csv_files',
    python_callable=get_latest_csv_files,
    dag=dag
)

# ìš´ì˜ìš©: ì „ì²´ ë§í¬ ì²˜ë¦¬
html_crawling_task = PythonOperator(
    task_id='process_all_csv_files',
    python_callable=process_all_csv_files,
    op_kwargs={
        'max_retries': 3,        # ì¬ì‹œë„ íšŸìˆ˜
        'restart_interval': 50,  # ë“œë¼ì´ë²„ ì¬ì‹œì‘ ê°„ê²©
        'wait_time': 1,          # í˜ì´ì§€ ë¡œë”© í›„ ëŒ€ê¸° ì‹œê°„
        'timeout': 20,           # í˜ì´ì§€ ë¡œë”© íƒ€ì„ì•„ì›ƒ
        'max_links': None        # Noneì´ë©´ ì „ì²´ ë§í¬ ì²˜ë¦¬, ìˆ«ìë©´ í•´ë‹¹ ê°œìˆ˜ë§Œ ì²˜ë¦¬ (í…ŒìŠ¤íŠ¸: 5, ìš´ì˜: None)
    },
    dag=dag
)

# í…ŒìŠ¤íŠ¸ìš©: 5ê°œ ë§í¬ë§Œ ì²˜ë¦¬
test_html_crawling_task = PythonOperator(
    task_id='test_process_all_csv_files',
    python_callable=process_all_csv_files,
    op_kwargs={
        'max_retries': 3,        # ì¬ì‹œë„ íšŸìˆ˜
        'restart_interval': 50,  # ë“œë¼ì´ë²„ ì¬ì‹œì‘ ê°„ê²©
        'wait_time': 1,          # í˜ì´ì§€ ë¡œë”© í›„ ëŒ€ê¸° ì‹œê°„
        'timeout': 20,           # í˜ì´ì§€ ë¡œë”© íƒ€ì„ì•„ì›ƒ
        'max_links': 5           # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ 5ê°œ ë§í¬ë§Œ ì²˜ë¦¬
    },
    dag=dag
)

summary_task = PythonOperator(
    task_id='html_crawling_summary',
    python_callable=html_crawling_summary,
    dag=dag
)

# íƒœìŠ¤í¬ ì˜ì¡´ì„± ì„¤ì •
# ìš´ì˜ìš©: ì „ì²´ ë§í¬ ì²˜ë¦¬
start_task >> find_csv_task >> html_crawling_task >> summary_task

# HTML to CSV ë³€í™˜ DAG ìë™ ì‹¤í–‰ íƒœìŠ¤í¬
trigger_html_to_csv_converter = TriggerDagRunOperator(
    task_id='trigger_html_to_csv_converter',
    trigger_dag_id='wconcept_html_to_csv_converter',
    wait_for_completion=False,  # ë¹„ë™ê¸° ì‹¤í–‰
    poke_interval=10,  # 10ì´ˆë§ˆë‹¤ ìƒíƒœ í™•ì¸
    allowed_states=['success'],  # ì„±ê³µ ìƒíƒœì—ì„œë§Œ íŠ¸ë¦¬ê±°
    failed_states=['failed'],  # ì‹¤íŒ¨ ìƒíƒœì—ì„œ íŠ¸ë¦¬ê±° ì•ˆí•¨
    dag=dag
)

# í…ŒìŠ¤íŠ¸ìš©ì„ ì‚¬ìš©í•˜ë ¤ë©´ ì•„ë˜ ì£¼ì„ì„ í•´ì œí•˜ê³  ìœ„ì˜ ìš´ì˜ìš©ì„ ì£¼ì„ ì²˜ë¦¬
# start_task >> find_csv_task >> test_html_crawling_task >> summary_task

# íƒœìŠ¤í¬ ì˜ì¡´ì„± ì„¤ì • (HTML to CSV ë³€í™˜ DAG íŠ¸ë¦¬ê±° í¬í•¨)
start_task >> find_csv_task >> html_crawling_task >> summary_task >> trigger_html_to_csv_converter